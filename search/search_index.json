{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RCC User Guide Welcome! This user guide provides information on accessing and making use of the Research Computing Center's High Performance Computing (HPC) resources. How to use this guide Here are a few things to keep in mind as you navigate the user guide: The guide is organized by system ; be sure you're in the right section! You will see in the top-right of grey code blocks, which will allow you to copy the contents of the block to your clipboard. Try the search bar in the top right to quickly find what you're looking for (e.g., search: \"GPU\"). If you come across any content that you think should be changed or improved (typo, out-of-date, etc.), please feel free to do any of the following to help make the guide better: Create an Issue on GitHub Edit the guide's markdown source directly and submit a pull request Email help@rcc.uchicago.edu The RCC Workflow This flowchart illustrates the workflow of a typical researcher using the RCC's HPC resources. Overview of RCC's HPC Systems The following table provides a high-level summary of the various high-performance computing (HPC) systems that the RCC houses. While our flagship system for scientific computing is Midway3, offering state-of-the art CPU and GPU nodes, we also offer several other specialized HPC systems to meet users' needs. System Description Use Midway3 RCC's flagship HPC cluster for multi-purpose scientific computing General Midway2 Primary HPC cluster from 2016-2021; users transitioning to Midway3 General MidwayR RCC's HPC cluster for sensitive data, housed within the Secure Data Enclave Specialized Beagle3 A GPU-focused HPC cluster for life sciences research Specialized DaLI Data Lifecycle Instrument, a data storage and software platform Specialized MidwaySSD HPC cluster dedicated to social sciences research Specialized Skyway Run Midway jobs on cloud computing platforms Specialized Where to start? Researchers interested in using the RCC systems can request an account . For Service Units (computing time) and storage resources, request an allocation . If you would like to chat with an RCC specialist about what services are best for you, please email help@rcc.uchicago.edu","title":"Home"},{"location":"#rcc-user-guide","text":"Welcome! This user guide provides information on accessing and making use of the Research Computing Center's High Performance Computing (HPC) resources.","title":"RCC User Guide"},{"location":"#how-to-use-this-guide","text":"Here are a few things to keep in mind as you navigate the user guide: The guide is organized by system ; be sure you're in the right section! You will see in the top-right of grey code blocks, which will allow you to copy the contents of the block to your clipboard. Try the search bar in the top right to quickly find what you're looking for (e.g., search: \"GPU\"). If you come across any content that you think should be changed or improved (typo, out-of-date, etc.), please feel free to do any of the following to help make the guide better: Create an Issue on GitHub Edit the guide's markdown source directly and submit a pull request Email help@rcc.uchicago.edu","title":"How to use this guide"},{"location":"#the-rcc-workflow","text":"This flowchart illustrates the workflow of a typical researcher using the RCC's HPC resources.","title":"The RCC Workflow"},{"location":"#overview-of-rccs-hpc-systems","text":"The following table provides a high-level summary of the various high-performance computing (HPC) systems that the RCC houses. While our flagship system for scientific computing is Midway3, offering state-of-the art CPU and GPU nodes, we also offer several other specialized HPC systems to meet users' needs. System Description Use Midway3 RCC's flagship HPC cluster for multi-purpose scientific computing General Midway2 Primary HPC cluster from 2016-2021; users transitioning to Midway3 General MidwayR RCC's HPC cluster for sensitive data, housed within the Secure Data Enclave Specialized Beagle3 A GPU-focused HPC cluster for life sciences research Specialized DaLI Data Lifecycle Instrument, a data storage and software platform Specialized MidwaySSD HPC cluster dedicated to social sciences research Specialized Skyway Run Midway jobs on cloud computing platforms Specialized","title":"Overview of RCC's HPC Systems"},{"location":"#where-to-start","text":"Researchers interested in using the RCC systems can request an account . For Service Units (computing time) and storage resources, request an allocation . If you would like to chat with an RCC specialist about what services are best for you, please email help@rcc.uchicago.edu","title":"Where to start?"},{"location":"general_faq/","text":"Frequently Asked Questions General How do I cite RCC in my publications and talks? Citations and acknowledgements help the RCC demonstrate the importance of computational resources and support staff in research at the University of Chicago. We ask that an acknolwedgment be given to the RCC in any presentation or publication of results that were made possible by RCC resources. Please reference the RCC as \u201cThe University of Chicago Research Computing Center\u201d in citations and acknowledgements. Here are a few examples of suggested text: This work was completed in part with resources provided by the University of Chicago Research Computing Center. We are grateful for the support of the University of Chicago Research Computing Center for assistance with the calculations carried out in this work. We acknowledge the University of Chicago Research Computing Center for support of this work. If you cite or acknowledge the RCC in your work, please notify the RCC by sending an email to info@rcc.uchicago.edu. Getting Started How do I become an RCC user? RCC user account requests should be submitted via our online application forms. See RCC Account Request for more information. How do I access RCC systems? There are various ways to access RCC systems. To access Midway2 interactively, use ThinLinc or an SSH client. See Connecting to RCC Resources for details. To access files stored on Midway2, use scp, Globus Online or SAMBA. See Data Transfer for details. How do I request access to a PI\u2019s account if I already have an account on Midway? Please submit a User Account Request and provide your CNetID and the PI Account name (typically pi- followed by the CNetID of the PI). The PI will receive an automated email requesting authorization for this request. What is my RCC username and password? The RCC uses the University of Chicago CNetIDs for user credentials. Once your RCC account is created, your RCC username and password will be the same as your CNetID credentials. Can an external collaborator get a CNetID so they can log in to RCC? The RCC can create CNetIDs for external collaborators. See RCC Account Request for more information. What should I do if I left the university and my CNetID password no longer works? You can use your CNetID for authentication after you have left, but IT Services may expire it when you leave. If you have an RCC account, but you still can\u2019t log in, it is likely that password authentication has been disabled by IT Services. Please contact help@rcc.uchicago.edu to request re-enabling access to your account. How do I change/reset my password? The RCC cannot change or reset your password. Go to the CNet Password Recovery page to change or reset your password. What groups am I a member of? To list the groups you are a member of, type groups on any RCC system. How do I access the data visualization lab in the Zar room of Crerar Library? The Zar room and its visualization equipment can be reserved for events, classes, or visualization work by contacting the RCC at help@rcc.uchicago.edu. More information regarding the RCC\u2019s visualization facilities can be found on the RCC Data Visualization page. What login shells are supported and how do I change my default shell? The RCC supports the following shells: /bin/bash /bin/tcsh /bin/zsh Use this command to change your default shell: $ chsh -s /path/to/shell It may take up to 30 minutes for that change to take effect. Is remote access with Mosh supported? Yes. To use Mosh, first log in to Midway via SSH, and add the command module load mosh to your ~/.bashrc (or ~/.zshenv if you use zsh). Then, you can log in by entering the following command in a terminal window: $ mosh @midway2.rcc.uchicago.edu Is SSH key authentication allowed on RCC machines? No. Why am I getting \u201cssh_exchange_identification: read: Connection reset by peer\u201d when I try to log in via SSH ? You can get this error if you incorrectly enter your password too many times. This is a security measure that is in place to limit the ability for malicious users to use brute force SSH attacks against our systems. After 3 failed password entry attempts, an IP address will be blocked for 4 hours. While you wait for the block to be lifted, you should still be able to access Midway2 using ThinLinc. Why am I getting prompted for YubiKey when I try to log in via SSH ? There are few reasons to get that error message. Please enroll in two factor authentication if you have not done already by visiting https://2fa.rcc.uchicago.edu. Please make sure you run ssh -Y your_cnetID@midway2.rcc.uchicago.edu. Finally, please make sure your Midway account has not been expired. Allocations What is an allocation? An allocation is a specified number of computing and storage resources granted to a PI or education account. An allocation is necessary to run jobs on RCC systems. See RCC Allocations for more details. What is a service unit (SU)? A service unit (SU) is roughly equal to 1 core-hour; for a more precise definition, see RCC Service Units. How do I obtain an allocation? The RCC accepts proposals for large (\u201cResearch II\u201d) allocations bi-annually. Medium-sized allocations, special purpose allocations for time-critical research, and allocations for education and outreach may be submitted at any time. See RCC Allocations for more information. How is compute cluster usage charged to my account? The charge associated with a job on Midway2 is a function of (1) the number of cores allocated to the job, and (2) the elapsed wall-clock time (in hours). How do I check the balance of my allocation? The rcchelp tool is the easiest way to check your account balance. $ rcchelp balance How do I check how my allocation has been used? The rcchelp tool has several options for summarizing allocation usage. For a summary, run $ rcchelp usage To see usage per job, run $ rcchelp usage --byjob If you are the PI, you may use --byuser option to see your group members\u2019 individual usage $ rcchelp usage --byuser Software What software does RCC offer on its compute systems? Software available within the RCC environment is constantly evolving. We regularly install new software, and new versions of existing software. Information about available software and how to use specific software pacakges can be found in the Software section of the User Guide. To view the current list of installed software on Midway2, run $ module avail To view the list of available versions for a specific software, run $ module avail How do I get help with RCC software? Documentation for many program can be viewed with the following command. $ man Many programs also provide documentation through command-line options such as --help or -h. For example, $ module load gcc $ gcc --help RCC also maintains supplementary documentation for software specific to our systems. Consult the Software page for more information. Why is my favorite command not available? Most likely it is because you have not loaded the appropriate software module. Most software packages are only available after first loading the appropriate software module. See Software for more information on how to access pre-installed software on RCC systems. Why do I get an error that says a module cannot be loaded due to a conflict? Occassionally, modules are incompatible with each other and cannot be loaded simultaneously. The module command typically gives you hints about which previously loaded module conflicts with the one you are trying to load. If you see such an error, try unloading a module with this command: $ module unload How do I request installation of a new or updated software package? Please send email to help@rcc.uchicago.edu with the details of your software request, including what software package you need and which version of the software you prefer. Why can\u2019t I run Gaussian? Gaussian\u2019s creators have historically had a strict usage policy, so we have limited its availability on RCC systems. If you need to use Gaussian for your research, please contact help@rcc.uchicago.edu to request access. Cluster Usage How do I submit a job to the queue? RCC systems use Slurm to manage resources and job queues. For advice on how to run specific types of jobs, consult the Running jobs on midway section of the User Guide. Can I login directly to a compute node? You can start up an interactive session on a compute node with the sinteractive command. This command takes the same arguments as sbatch. More information about interactive jobs, see Interactive Jobs. How do I run jobs in parallel? There are many ways to configure parallel jobs\u2014the best approach will depend on your software and resource requirements. For more information on two commonly used approaches, see Parallel batch jobs and Job arrays. Are there any limits to running jobs on Midway2? Run rcchelp qos on Midway to view the current criteria. I am a member of multiple accounts. How do I choose which allocation is charged? If you belong to multiple accounts, jobs will get charged to your default account unless you specify the --account= option when you submit a job with sbatch. Run this command to determine your default account. sacctmgr list user $USER To change your default account, run this command. sacctmgr modify user $USER set defaultaccount= Alternatively, you may request the change by contacting the RCC. Why is my job not starting? This could be due to a variety of factors. Running squeue --user= can will help to find the answer; see in particular the NODELIST(REASON) column in the squeue output. A job that is waiting in the queue may show one of the following labels in this column: (Priority): Other jobs currently have higher priority than your job. (Resources): Your job has enough priority to run, but there aren\u2019t yet enough free resources to run it. (QOSResourceLimit): Your job exceeds the QOS limits. The QOS limits include wall time, number of jobs a user can have running at once, number of nodes a user can use at once, and so on. For example, if you are at or near the limit of number of jobs that can be run at once, your job will become eligible to run as soon as other jobs finish. Please contact RCC support if you believe that your job is not being handled correctly by the Slurm queuing system. Also, note that if you see a large number of jobs that aren\u2019t running when many resources are idle, it is possible that RCC staff have scheduled an upcoming maintenance window. In this case, any jobs requesting a wall time that overlaps with the maintenance window will remain in the queue until after the maintainence period is over. The RCC staff will typically notify users via email prior to performing a maintenance and after a maintenance is completed. Why does my job fail after a few seconds? This is most likely because there is an error in your job submission script, or because the program you are trying to run is producing an error and terminating prematurely. If you need help troubleshooting the issue, please send your job submission script, as well as the error generated by your job submission script, to help@rcc.uchicago.edu. Why does my job fail with message \u201cexceeded memory limit, being killed\u201d? On the main midway2 partition, broadwl, Slurm allocates 2 GB of memory per allocated CPU by default. If your computations require more than the default amount, you should adjust the memory allocated to your job with the --mem or --mem-per-cpu flags. For example, to request 10 cores and 40 GB of memory on a broadwl node, include these options when running sbatch or sinteractive: --ntasks=1 --cpus-per-task=10 --mem=40G. Why does my sinteractive job fail with \u201cConnection to closed.\u201d? There are two likely explanations for this error. One possibility is that you are over the time limit. The default walltime for sinteractive is 2 hours. This can be increased by including the --time flag to your sinteractive call. Another possiblity is that your job exceeded the memory limit. You can resolve this by requesting additional memory using --mem or --mem-per-cpu. How do I run jobs that need to run longer than the maximum wall time? The RCC queuing system is designed to provide fair resource allocation to all RCC users. The maximum wall time is intended to prevent individual users from using more than their fair share of cluster resources. If you have specific computing tasks that cannot be solved with the current constraints, please submit a special request for resources to help@rcc.uchicago.edu. Can I create a cron job? The RCC does not support users creating cron jobs. However, it is possible to use Slurm to submit \u201ccron-like\u201d jobs. See Cron-like jobs for more information. Performance and Coding What compilers does the RCC support? The RCC supports the GNU, Intel, PGI and NVidia\u2019s CUDA compilers. Which versions of MPI does RCC support? The RCC maintains OpenMPI, IntelMPI, and MVAPICH2 compilers. See Message Passing Interface (MPI) for more information and instructions for using these MPI frameworks. Can the RCC help me parallelize and optimize my code? The RCC support staff are available to consult with you or your research team to help parallelize and optimize your code for use on RCC systems. Contact the RCC staff at help@rcc.uchicago.edu to set up a consultation. Does RCC provide GPU computing resources? Yes. The RCC high-performance systems provide GPU-equipped compute nodes. For instructions on using the GPU nodes, see GPU jobs. File I/O, Storage, and Transfers How much storage space do I have? Use the quota command to get a summary of your current file system usage and available storage space. How do I get my storage quota increased? Additional storage space can be purchased through the Cluster Partnership Program. You may also request additional storage as part of a Research II Allocation or Special Allocation. How do I share files with others? The recommended way to share files with members of your group is to store them in the /project or /project2 directory for your group. Project directories are created for all PI and project accounts. File and directory permissions can be customized to allow access to users within the group, as well as RCC users that do not belong to your group. I accidentally deleted or lost a file. How do I restore it? The best way to recover a recently deleted, corrupted or lost file is from a snapshot. See Data Recovery and Backups for more information. How do I request a restore of my files from tape backup? The RCC maintains tape backups of all home and project directories. These tape backups are intended for disaster recovery purposes only. There is no long-term history of files on tape. In most cases, you should use file system snapshots to retrieve recover files. See Data Recovery and Backups for more information.","title":"Frequently Asked Questions"},{"location":"general_faq/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"general_faq/#general","text":"","title":"General"},{"location":"general_faq/#how-do-i-cite-rcc-in-my-publications-and-talks","text":"Citations and acknowledgements help the RCC demonstrate the importance of computational resources and support staff in research at the University of Chicago. We ask that an acknolwedgment be given to the RCC in any presentation or publication of results that were made possible by RCC resources. Please reference the RCC as \u201cThe University of Chicago Research Computing Center\u201d in citations and acknowledgements. Here are a few examples of suggested text: This work was completed in part with resources provided by the University of Chicago Research Computing Center. We are grateful for the support of the University of Chicago Research Computing Center for assistance with the calculations carried out in this work. We acknowledge the University of Chicago Research Computing Center for support of this work. If you cite or acknowledge the RCC in your work, please notify the RCC by sending an email to info@rcc.uchicago.edu.","title":"How do I cite RCC in my publications and talks?"},{"location":"general_faq/#getting-started","text":"","title":"Getting Started"},{"location":"general_faq/#how-do-i-become-an-rcc-user","text":"RCC user account requests should be submitted via our online application forms. See RCC Account Request for more information.","title":"How do I become an RCC user?"},{"location":"general_faq/#how-do-i-access-rcc-systems","text":"There are various ways to access RCC systems. To access Midway2 interactively, use ThinLinc or an SSH client. See Connecting to RCC Resources for details. To access files stored on Midway2, use scp, Globus Online or SAMBA. See Data Transfer for details.","title":"How do I access RCC systems?"},{"location":"general_faq/#how-do-i-request-access-to-a-pis-account-if-i-already-have-an-account-on-midway","text":"Please submit a User Account Request and provide your CNetID and the PI Account name (typically pi- followed by the CNetID of the PI). The PI will receive an automated email requesting authorization for this request.","title":"How do I request access to a PI\u2019s account if I already have an account on Midway?"},{"location":"general_faq/#what-is-my-rcc-username-and-password","text":"The RCC uses the University of Chicago CNetIDs for user credentials. Once your RCC account is created, your RCC username and password will be the same as your CNetID credentials.","title":"What is my RCC username and password?"},{"location":"general_faq/#can-an-external-collaborator-get-a-cnetid-so-they-can-log-in-to-rcc","text":"The RCC can create CNetIDs for external collaborators. See RCC Account Request for more information.","title":"Can an external collaborator get a CNetID so they can log in to RCC?"},{"location":"general_faq/#what-should-i-do-if-i-left-the-university-and-my-cnetid-password-no-longer-works","text":"You can use your CNetID for authentication after you have left, but IT Services may expire it when you leave. If you have an RCC account, but you still can\u2019t log in, it is likely that password authentication has been disabled by IT Services. Please contact help@rcc.uchicago.edu to request re-enabling access to your account.","title":"What should I do if I left the university and my CNetID password no longer works?"},{"location":"general_faq/#how-do-i-changereset-my-password","text":"The RCC cannot change or reset your password. Go to the CNet Password Recovery page to change or reset your password.","title":"How do I change/reset my password?"},{"location":"general_faq/#what-groups-am-i-a-member-of","text":"To list the groups you are a member of, type groups on any RCC system.","title":"What groups am I a member of?"},{"location":"general_faq/#how-do-i-access-the-data-visualization-lab-in-the-zar-room-of-crerar-library","text":"The Zar room and its visualization equipment can be reserved for events, classes, or visualization work by contacting the RCC at help@rcc.uchicago.edu. More information regarding the RCC\u2019s visualization facilities can be found on the RCC Data Visualization page.","title":"How do I access the data visualization lab in the Zar room of Crerar Library?"},{"location":"general_faq/#what-login-shells-are-supported-and-how-do-i-change-my-default-shell","text":"The RCC supports the following shells: /bin/bash /bin/tcsh /bin/zsh Use this command to change your default shell: $ chsh -s /path/to/shell It may take up to 30 minutes for that change to take effect.","title":"What login shells are supported and how do I change my default shell?"},{"location":"general_faq/#is-remote-access-with-mosh-supported","text":"Yes. To use Mosh, first log in to Midway via SSH, and add the command module load mosh to your ~/.bashrc (or ~/.zshenv if you use zsh). Then, you can log in by entering the following command in a terminal window: $ mosh @midway2.rcc.uchicago.edu","title":"Is remote access with Mosh supported?"},{"location":"general_faq/#is-ssh-key-authentication-allowed-on-rcc-machines","text":"No.","title":"Is SSH key authentication allowed on RCC machines?"},{"location":"general_faq/#why-am-i-getting-ssh_exchange_identification-read-connection-reset-by-peer-when-i-try-to-log-in-via-ssh","text":"You can get this error if you incorrectly enter your password too many times. This is a security measure that is in place to limit the ability for malicious users to use brute force SSH attacks against our systems. After 3 failed password entry attempts, an IP address will be blocked for 4 hours. While you wait for the block to be lifted, you should still be able to access Midway2 using ThinLinc.","title":"Why am I getting \u201cssh_exchange_identification: read: Connection reset by peer\u201d when I try to log in via SSH ?"},{"location":"general_faq/#why-am-i-getting-prompted-for-yubikey-when-i-try-to-log-in-via-ssh","text":"There are few reasons to get that error message. Please enroll in two factor authentication if you have not done already by visiting https://2fa.rcc.uchicago.edu. Please make sure you run ssh -Y your_cnetID@midway2.rcc.uchicago.edu. Finally, please make sure your Midway account has not been expired.","title":"Why am I getting prompted for YubiKey when I try to log in via SSH ?"},{"location":"general_faq/#allocations","text":"","title":"Allocations"},{"location":"general_faq/#what-is-an-allocation","text":"An allocation is a specified number of computing and storage resources granted to a PI or education account. An allocation is necessary to run jobs on RCC systems. See RCC Allocations for more details.","title":"What is an allocation?"},{"location":"general_faq/#what-is-a-service-unit-su","text":"A service unit (SU) is roughly equal to 1 core-hour; for a more precise definition, see RCC Service Units.","title":"What is a service unit (SU)?"},{"location":"general_faq/#how-do-i-obtain-an-allocation","text":"The RCC accepts proposals for large (\u201cResearch II\u201d) allocations bi-annually. Medium-sized allocations, special purpose allocations for time-critical research, and allocations for education and outreach may be submitted at any time. See RCC Allocations for more information.","title":"How do I obtain an allocation?"},{"location":"general_faq/#how-is-compute-cluster-usage-charged-to-my-account","text":"The charge associated with a job on Midway2 is a function of (1) the number of cores allocated to the job, and (2) the elapsed wall-clock time (in hours). How do I check the balance of my allocation? The rcchelp tool is the easiest way to check your account balance. $ rcchelp balance How do I check how my allocation has been used? The rcchelp tool has several options for summarizing allocation usage. For a summary, run $ rcchelp usage To see usage per job, run $ rcchelp usage --byjob If you are the PI, you may use --byuser option to see your group members\u2019 individual usage $ rcchelp usage --byuser Software","title":"How is compute cluster usage charged to my account?"},{"location":"general_faq/#what-software-does-rcc-offer-on-its-compute-systems","text":"Software available within the RCC environment is constantly evolving. We regularly install new software, and new versions of existing software. Information about available software and how to use specific software pacakges can be found in the Software section of the User Guide. To view the current list of installed software on Midway2, run $ module avail To view the list of available versions for a specific software, run $ module avail How do I get help with RCC software? Documentation for many program can be viewed with the following command. $ man Many programs also provide documentation through command-line options such as --help or -h. For example, $ module load gcc $ gcc --help RCC also maintains supplementary documentation for software specific to our systems. Consult the Software page for more information.","title":"What software does RCC offer on its compute systems?"},{"location":"general_faq/#why-is-my-favorite-command-not-available","text":"Most likely it is because you have not loaded the appropriate software module. Most software packages are only available after first loading the appropriate software module. See Software for more information on how to access pre-installed software on RCC systems.","title":"Why is my favorite command not available?"},{"location":"general_faq/#why-do-i-get-an-error-that-says-a-module-cannot-be-loaded-due-to-a-conflict","text":"Occassionally, modules are incompatible with each other and cannot be loaded simultaneously. The module command typically gives you hints about which previously loaded module conflicts with the one you are trying to load. If you see such an error, try unloading a module with this command: $ module unload How do I request installation of a new or updated software package? Please send email to help@rcc.uchicago.edu with the details of your software request, including what software package you need and which version of the software you prefer.","title":"Why do I get an error that says a module cannot be loaded due to a conflict?"},{"location":"general_faq/#why-cant-i-run-gaussian","text":"Gaussian\u2019s creators have historically had a strict usage policy, so we have limited its availability on RCC systems. If you need to use Gaussian for your research, please contact help@rcc.uchicago.edu to request access.","title":"Why can\u2019t I run Gaussian?"},{"location":"general_faq/#cluster-usage","text":"How do I submit a job to the queue? RCC systems use Slurm to manage resources and job queues. For advice on how to run specific types of jobs, consult the Running jobs on midway section of the User Guide.","title":"Cluster Usage"},{"location":"general_faq/#can-i-login-directly-to-a-compute-node","text":"You can start up an interactive session on a compute node with the sinteractive command. This command takes the same arguments as sbatch. More information about interactive jobs, see Interactive Jobs.","title":"Can I login directly to a compute node?"},{"location":"general_faq/#how-do-i-run-jobs-in-parallel","text":"There are many ways to configure parallel jobs\u2014the best approach will depend on your software and resource requirements. For more information on two commonly used approaches, see Parallel batch jobs and Job arrays.","title":"How do I run jobs in parallel?"},{"location":"general_faq/#are-there-any-limits-to-running-jobs-on-midway2","text":"Run rcchelp qos on Midway to view the current criteria.","title":"Are there any limits to running jobs on Midway2?"},{"location":"general_faq/#i-am-a-member-of-multiple-accounts-how-do-i-choose-which-allocation-is-charged","text":"If you belong to multiple accounts, jobs will get charged to your default account unless you specify the --account= option when you submit a job with sbatch. Run this command to determine your default account. sacctmgr list user $USER To change your default account, run this command. sacctmgr modify user $USER set defaultaccount= Alternatively, you may request the change by contacting the RCC.","title":"I am a member of multiple accounts. How do I choose which allocation is charged?"},{"location":"general_faq/#why-is-my-job-not-starting","text":"This could be due to a variety of factors. Running squeue --user= can will help to find the answer; see in particular the NODELIST(REASON) column in the squeue output. A job that is waiting in the queue may show one of the following labels in this column: (Priority): Other jobs currently have higher priority than your job. (Resources): Your job has enough priority to run, but there aren\u2019t yet enough free resources to run it. (QOSResourceLimit): Your job exceeds the QOS limits. The QOS limits include wall time, number of jobs a user can have running at once, number of nodes a user can use at once, and so on. For example, if you are at or near the limit of number of jobs that can be run at once, your job will become eligible to run as soon as other jobs finish. Please contact RCC support if you believe that your job is not being handled correctly by the Slurm queuing system. Also, note that if you see a large number of jobs that aren\u2019t running when many resources are idle, it is possible that RCC staff have scheduled an upcoming maintenance window. In this case, any jobs requesting a wall time that overlaps with the maintenance window will remain in the queue until after the maintainence period is over. The RCC staff will typically notify users via email prior to performing a maintenance and after a maintenance is completed.","title":"Why is my job not starting?"},{"location":"general_faq/#why-does-my-job-fail-after-a-few-seconds","text":"This is most likely because there is an error in your job submission script, or because the program you are trying to run is producing an error and terminating prematurely. If you need help troubleshooting the issue, please send your job submission script, as well as the error generated by your job submission script, to help@rcc.uchicago.edu.","title":"Why does my job fail after a few seconds?"},{"location":"general_faq/#why-does-my-job-fail-with-message-exceeded-memory-limit-being-killed","text":"On the main midway2 partition, broadwl, Slurm allocates 2 GB of memory per allocated CPU by default. If your computations require more than the default amount, you should adjust the memory allocated to your job with the --mem or --mem-per-cpu flags. For example, to request 10 cores and 40 GB of memory on a broadwl node, include these options when running sbatch or sinteractive: --ntasks=1 --cpus-per-task=10 --mem=40G.","title":"Why does my job fail with message \u201cexceeded memory limit, being killed\u201d?"},{"location":"general_faq/#why-does-my-sinteractive-job-fail-with-connection-to-closed","text":"There are two likely explanations for this error. One possibility is that you are over the time limit. The default walltime for sinteractive is 2 hours. This can be increased by including the --time flag to your sinteractive call. Another possiblity is that your job exceeded the memory limit. You can resolve this by requesting additional memory using --mem or --mem-per-cpu.","title":"Why does my sinteractive job fail with \u201cConnection to closed.\u201d?"},{"location":"general_faq/#how-do-i-run-jobs-that-need-to-run-longer-than-the-maximum-wall-time","text":"The RCC queuing system is designed to provide fair resource allocation to all RCC users. The maximum wall time is intended to prevent individual users from using more than their fair share of cluster resources. If you have specific computing tasks that cannot be solved with the current constraints, please submit a special request for resources to help@rcc.uchicago.edu.","title":"How do I run jobs that need to run longer than the maximum wall time?"},{"location":"general_faq/#can-i-create-a-cron-job","text":"The RCC does not support users creating cron jobs. However, it is possible to use Slurm to submit \u201ccron-like\u201d jobs. See Cron-like jobs for more information.","title":"Can I create a cron job?"},{"location":"general_faq/#performance-and-coding","text":"","title":"Performance and Coding"},{"location":"general_faq/#what-compilers-does-the-rcc-support","text":"The RCC supports the GNU, Intel, PGI and NVidia\u2019s CUDA compilers.","title":"What compilers does the RCC support?"},{"location":"general_faq/#which-versions-of-mpi-does-rcc-support","text":"The RCC maintains OpenMPI, IntelMPI, and MVAPICH2 compilers. See Message Passing Interface (MPI) for more information and instructions for using these MPI frameworks.","title":"Which versions of MPI does RCC support?"},{"location":"general_faq/#can-the-rcc-help-me-parallelize-and-optimize-my-code","text":"The RCC support staff are available to consult with you or your research team to help parallelize and optimize your code for use on RCC systems. Contact the RCC staff at help@rcc.uchicago.edu to set up a consultation.","title":"Can the RCC help me parallelize and optimize my code?"},{"location":"general_faq/#does-rcc-provide-gpu-computing-resources","text":"Yes. The RCC high-performance systems provide GPU-equipped compute nodes. For instructions on using the GPU nodes, see GPU jobs.","title":"Does RCC provide GPU computing resources?"},{"location":"general_faq/#file-io-storage-and-transfers","text":"How much storage space do I have? Use the quota command to get a summary of your current file system usage and available storage space.","title":"File I/O, Storage, and Transfers"},{"location":"general_faq/#how-do-i-get-my-storage-quota-increased","text":"Additional storage space can be purchased through the Cluster Partnership Program. You may also request additional storage as part of a Research II Allocation or Special Allocation.","title":"How do I get my storage quota increased?"},{"location":"general_faq/#how-do-i-share-files-with-others","text":"The recommended way to share files with members of your group is to store them in the /project or /project2 directory for your group. Project directories are created for all PI and project accounts. File and directory permissions can be customized to allow access to users within the group, as well as RCC users that do not belong to your group.","title":"How do I share files with others?"},{"location":"general_faq/#i-accidentally-deleted-or-lost-a-file-how-do-i-restore-it","text":"The best way to recover a recently deleted, corrupted or lost file is from a snapshot. See Data Recovery and Backups for more information.","title":"I accidentally deleted or lost a file. How do I restore it?"},{"location":"general_faq/#how-do-i-request-a-restore-of-my-files-from-tape-backup","text":"The RCC maintains tape backups of all home and project directories. These tape backups are intended for disaster recovery purposes only. There is no long-term history of files on tape. In most cases, you should use file system snapshots to retrieve recover files. See Data Recovery and Backups for more information.","title":"How do I request a restore of my files from tape backup?"},{"location":"general_troubleshooting/","text":"General Troubleshooting and Frequently Asked Questions Errors in submitting jobs Scheduler is busy Sometimes when the cluster is being very heavily utilized, the Slurm controller will be busy, and unable to accept your job submission request. If this is happening, you will notice that your job submission commands will hang; you can receive an error message like: Unable to allocate resources: Socket timed out on send/recv operation If this happens, wait a few minutes and try submitting your job again. Error message: Unable to contact Slurm controller sbatch: error: Batch job submission failed: Unable to contact Slurm controller (connect failure) There may be an issue with Slurm. If the error is still seen after a few minutes, report to RCC. Jobs that never start Requested resources not available If your jobs requested many cores or a large amount of memory, they may not start running very quickly. You can run squeue -u -t PD (substitute with your CNETID) to see the REASON why your jobs are not running. If the REASON seen in squeue is Resources, then the resources you requested in the job submission are not yet available. You may also see the ReqNodeNotAvail job reason if you requested that your job is run on a node that is not available (other jobs are running there, or the node is offline). Jobs that start running and then exit Jobs can exit for a number of reasons, such as an error in the code, exceeding a time or resource limit you specified, or due to a problem that the node was running on. To look up information for a completed job: sacct -j <jobid> Exceeded run time If your job runs beyond the \"wall clock\" time limit you requested with -t in your job submission command, then it will be killed. Exceeded requested memory If your job uses more memory than you requested using the --mem-per-cpu or --mem parameters in your job submission, it will be killed. You may see errors like this: slurmstepd: error: Exceeded job memory limit or slurmstepd: error: Exceeded step memory limit at some point. Slurm allows you to have \"job steps\", which are tasks that are part of a job (See the official Slurm Quick Start Guide for more information). By default, there will be one job step per job. Depending on which memory limit your job exceeds (job limit or step limit), you will see one of the above messages. Slurm Job States Your job will report different states before, during, and after execution. The most common ones are seen below, but this is not an exhaustive list. Look at Job State Codes in the squeue manual or this section in the sacct manual for more detail. Job State Long form Job State Short form Meaning CANCELLED CA Job was killed, either by the user who submitted it, a system administrator, or for using more resources than requested. COMPLETED CD Job has ended in a zero exit status, and all processes from the job are no longer running. COMPLETING CG This status differs from COMPLETED because some processes may still be running from this job. FAILED F Job did not complete successfully, and ended in a non-zero exit status. NODE_FAIL N The node or nodes that the job was running on had a problem. OUT_OF_MEMORY OOM The job tried to use more memory than was requested through the scheduler. PENDING PD Job is queued, so it is not yet running. Look at the Jobs that never start section for details on why jobs can remain in pending state. RUNNING R Job has been allocated resources, and is currently running. TIMEOUT TO Job exited because it reached its walltime limit. # Slurm Job Reasons If your job is pending, the squeue command will show a \"reason\" why it is unable to run. Some of these reasons are detailed below, but please reference Job Reason Codes in the squeue manual for more detail. Job Reason Meaning AssocMaxCpuPerJobLimit Job violates accounting/QOS policy (job submit limit, user's size and/or time limits). The job cannot run because the is no allocation in the account. Dependency The job can't start until a job dependency finishes. JobHeldAdmin The job will stay pending, as it has been held by an administrator. JobHeldUser The job will stay pending, as it has been held by the user. NodeDown A node that the job requires is in \"down\" state, meaning that the node can't be currently used. Priority Your job has lower priority than others in the queue. The jobs with higher priority must dispatch first. ReqNodeNotAvail A node that the job requests using cannot currently accept jobs. ReqNodeNotAvail is a generic reason; in the simplest case, it means that the node is fully in use and is unable to run any more jobs. In other scenarios, this reason can indicate that there is a problem with the node. Resources The required resources for running this job are not yet available.","title":"General Troubleshooting and Frequently Asked Questions"},{"location":"general_troubleshooting/#general-troubleshooting-and-frequently-asked-questions","text":"","title":"General Troubleshooting and Frequently Asked Questions"},{"location":"general_troubleshooting/#errors-in-submitting-jobs","text":"","title":"Errors in submitting jobs"},{"location":"general_troubleshooting/#scheduler-is-busy","text":"Sometimes when the cluster is being very heavily utilized, the Slurm controller will be busy, and unable to accept your job submission request. If this is happening, you will notice that your job submission commands will hang; you can receive an error message like: Unable to allocate resources: Socket timed out on send/recv operation If this happens, wait a few minutes and try submitting your job again.","title":"Scheduler is busy"},{"location":"general_troubleshooting/#error-message-unable-to-contact-slurm-controller","text":"sbatch: error: Batch job submission failed: Unable to contact Slurm controller (connect failure) There may be an issue with Slurm. If the error is still seen after a few minutes, report to RCC.","title":"Error message: Unable to contact Slurm controller"},{"location":"general_troubleshooting/#jobs-that-never-start","text":"","title":"Jobs that never start"},{"location":"general_troubleshooting/#requested-resources-not-available","text":"If your jobs requested many cores or a large amount of memory, they may not start running very quickly. You can run squeue -u -t PD (substitute with your CNETID) to see the REASON why your jobs are not running. If the REASON seen in squeue is Resources, then the resources you requested in the job submission are not yet available. You may also see the ReqNodeNotAvail job reason if you requested that your job is run on a node that is not available (other jobs are running there, or the node is offline).","title":"Requested resources not available"},{"location":"general_troubleshooting/#jobs-that-start-running-and-then-exit","text":"Jobs can exit for a number of reasons, such as an error in the code, exceeding a time or resource limit you specified, or due to a problem that the node was running on. To look up information for a completed job: sacct -j <jobid>","title":"Jobs that start running and then exit"},{"location":"general_troubleshooting/#exceeded-run-time","text":"If your job runs beyond the \"wall clock\" time limit you requested with -t in your job submission command, then it will be killed.","title":"Exceeded run time"},{"location":"general_troubleshooting/#exceeded-requested-memory","text":"If your job uses more memory than you requested using the --mem-per-cpu or --mem parameters in your job submission, it will be killed. You may see errors like this: slurmstepd: error: Exceeded job memory limit or slurmstepd: error: Exceeded step memory limit at some point. Slurm allows you to have \"job steps\", which are tasks that are part of a job (See the official Slurm Quick Start Guide for more information). By default, there will be one job step per job. Depending on which memory limit your job exceeds (job limit or step limit), you will see one of the above messages.","title":"Exceeded requested memory"},{"location":"general_troubleshooting/#slurm-job-states","text":"Your job will report different states before, during, and after execution. The most common ones are seen below, but this is not an exhaustive list. Look at Job State Codes in the squeue manual or this section in the sacct manual for more detail. Job State Long form Job State Short form Meaning CANCELLED CA Job was killed, either by the user who submitted it, a system administrator, or for using more resources than requested. COMPLETED CD Job has ended in a zero exit status, and all processes from the job are no longer running. COMPLETING CG This status differs from COMPLETED because some processes may still be running from this job. FAILED F Job did not complete successfully, and ended in a non-zero exit status. NODE_FAIL N The node or nodes that the job was running on had a problem. OUT_OF_MEMORY OOM The job tried to use more memory than was requested through the scheduler. PENDING PD Job is queued, so it is not yet running. Look at the Jobs that never start section for details on why jobs can remain in pending state. RUNNING R Job has been allocated resources, and is currently running. TIMEOUT TO Job exited because it reached its walltime limit. # Slurm Job Reasons If your job is pending, the squeue command will show a \"reason\" why it is unable to run. Some of these reasons are detailed below, but please reference Job Reason Codes in the squeue manual for more detail. Job Reason Meaning AssocMaxCpuPerJobLimit Job violates accounting/QOS policy (job submit limit, user's size and/or time limits). The job cannot run because the is no allocation in the account. Dependency The job can't start until a job dependency finishes. JobHeldAdmin The job will stay pending, as it has been held by an administrator. JobHeldUser The job will stay pending, as it has been held by the user. NodeDown A node that the job requires is in \"down\" state, meaning that the node can't be currently used. Priority Your job has lower priority than others in the queue. The jobs with higher priority must dispatch first. ReqNodeNotAvail A node that the job requests using cannot currently accept jobs. ReqNodeNotAvail is a generic reason; in the simplest case, it means that the node is fully in use and is unable to run any more jobs. In other scenarios, this reason can indicate that there is a problem with the node. Resources The required resources for running this job are not yet available.","title":"Slurm Job States"},{"location":"glossary/","text":"Term Definition Batch job A job is the Slurm\u2019s computing unit by which resources are allocated and shared. Users create job submission scripts to ask Slurm for resources such as cores, memory, walltime, etc. Slurm puts the requests in a queue and allocates requested resources based on jobs\u2019 priority. Compute cluster A group of independent computers connected via a fast network interconnect, managed by a resource manage, and act as a large parallel computer. Each node in a cluster can be a shared memory parallel computer. Compute node A compute node is a stand-alone computer connected to other compute nodes via a fast network interconnect. A compute node is where a batch job runs and is not usually accessible directly by the users. Core Smallest computation unit that can run a program (used to be called a processor, still is, also called a CPU \u2013 Central Processing Unit) Distributed memory architecture Distributed memory architecture refers to a way to create a parallel computer. In this architecture, stand-alone compute nodes are connected using a fast interconnect such as Infiniband and exchange messages over the network. FLOPS FLoating point Operation Per Second (FLOPS) is a measure of computing performance in terms of number of floating operations that a CPU can perfomr per second. Modern CPUs are capable of doing Tera FLOPS (10^12 floating point operations per second) GPU Graphics Processing Unit (GPU) is a specialized device initially used to generate computer output. GPUs have their own memory but should be hosted in a node. Each compute node can host one or more GPUs. Modern GPUs have many simple compute cores and have been used for parallel processing. HPC High Performance Computing (HPC) refers to the practice of aggregating computing power to achieve higher performance that would not possible by using a typical computer. Infiniband A computer network standard featuring high bandwidth and low latency. The current Infiniband devices are capable of transferring data at up to 100Gbits/sec with less than a microsecond latency. As of this writing, the popular Infiniband versions are FDR (Fourteen Data Rate) with 56Gbits/sec and EDR (Enhanced Data Rate) with 100Gbits/sec. Login node Login nodes (a.k.a. head nodes) are point of access to a parallel computer. Users usually connect to login nodes via SSH to compile and debug their code, review their results, do some simple tests, and submit their batch jobs to the parallel computer. Modules An open source software management tool used in most HPC facilities. Using modules enable users to selectively pick the software that they want and add them to their environment. Node A stand-alone computer system that contains one or more sockets, memory, storage, etc. connected to other nodes via a fast network interconnect OpenMP Open Multi Processing (OpenMP) is a parallel programming model designed for shared memory architecture. In this programming mode, programmers insert compiler directives in their code and the compiler generates a code that can run on more than one core. Partition A subset of a compute cluster with a common feature. For example, compute nodes with GPU could form a partition. Shared memory architecture Shared memory architecture is a way to create a parallel computer. In this architecture, a large memory is shared among many cores and communication between cores is done via the shared memory.By introducing the multi-core CPUs, each computer is a shared-memory parallel computer. Slurm Simple Linux Utility for Resource Management (SLURM) is a software that manages high performance computing resources. SLURM coordinates running of many programs on a shared facility and makes sure that resources are used in a fair share manner. Socket A computational unit, packaged as one and usually made of a single chip often called processor. Modern sockets carry many cores (2, 4 on most laptops, 8 to 16 on most servers) SSH Secure Shell is a protocol to securely access remote computers. Based on the client-server model, multiple users with an SSH client can access to a remote computer. Some operating systems such as Linux and Mac OS have a built-in SSH client and others can use one of many publicly available clients. Tightly-coupled nodes A set of compute nodes connected via fast Infiniband interconnect. These nodes can exchange data in a fast rate and are used to solve big problems that cannot fit in a single computer. Walltime The time that requires a program to finish it execution.","title":"Glossary"},{"location":"user_policy/","text":"RCC User Policy By collaborating with the Research Computing Center (RCC), you may have access to various RCC computer systems, networks, applications, data, and/or other technology assets. By using any RCC resource, you are agreeing to abide by the usage policy below and to use RCC resources in a responsible and ethical manner. Please read the information below carefully. Violations of the RCC usage policy can result in the removal of access to RCC resources and the reporting of suspicious activities to the appropriate authorities. General usage policy The following principles govern the usage of all RCC resources. You are required to abide by these principles. You are responsible for the proper use of the tools each computer system provides and for the confidentiality of any sensitive information entrusted to you. You must not use RCC resources for illegal or malicious purposes such as harassment, disrupting communications or services, or unauthorized monitoring of communications. You must refrain from the unethical use of RCC resources, including unauthorized use of computer accounts and resources assigned to others, use of computing facilities for private business or political purposes or private gain, academic or scientific dishonesty, or violation of software license agreements. You will respect the confidentiality and privacy of individuals whose records you may have access to in accordance with RCC policy, University of Chicago ethical standards, and state and federal laws. You should immediately report any suspected breach of security, policy violation, or suspicious activity to help@rcc.uchicago.edu and your supervisor, principal investigator, or sponsor. You will not attempt to subvert or circumvent any system security features. You will not attempt to subvert or circumvent any system that allocates resources. You should be aware that all computer activity and files on these systems may be monitored by RCC system administrators and appropriate authorities. Your account may be terminated by RCC staff for failure to follow these principles. The RCC provides computational and data storage resources for the sole purpose of supporting legitimate research activities at the University of Chicago. A user must refrain from using RCC resources for any task other than that which was described on their account application form. RCC users are also required to familiarize themselves with the University of Chicago\u2019s Eligibility and Acceptable Use Policy for Information Technology. User accounts and passwords Your account credentials are assigned to you alone and should not be shared with anyone, including co-workers, trainers, or computer support staff. If you believe your account password has been compromised, report the incident to RCC support staff immediately and change your password through the University of Chicago\u2019s CNetID password service. Make sure you are the only person who has access to your 2FA device. Many mechanisms exist for sharing data and working collaboratively with your colleagues, and the RCC will assist you with these tools as necessary. There is never a reason to share account and/or password information. Data policy and security The RCC maintains an open research network that provides shared computing and data management resources to a large number of University of Chicago researchers and their collaborators. Therefore, the RCC cannot guarantee the complete security of data stored on any RCC system. While information security is a primary concern of the RCC, neither the RCC nor the University of Chicago shall be held liable for any breach of security resulting in the compromise of sensitive or confidential data. It is the sole responsibility of the user to ensure proper measures have been taken to protect sensitive or confidential information. RCC system administrators and support staff that have super-user privileges are able to view any and all data on RCC systems unless it is encrypted. RCC system administrators will only look at user data to provide assistance to the owner, or if there is sufficient reason to believe a serious problem or security threat exists that could disrupt the work of others. Protected Health Information Currently, the RCC provides MidwayR, a secure data enclave, compute resource that is compliant with the Health Insurance Portability and Accountability Act of 1996 (HIPAA) regulations for the storage or processing of Protected Health Information (PHI). If your project necessitates the use of PHI data, please inform RCC staff of your requirements and we will work to determine if your data/project can be accommodated on MidwayR. Data Encryption The RCC provides encrypted file systems on MidwayR. Any data that is sensitive needs to be stored on MidwayR. Data Integrity and Retention Policy The RCC strives to maintain robust filesystems and archival storage facilities with the highest levels of performance and reliability. However, neither the RCC nor the University of Chicago shall be held liable for any loss of data. The RCC provides an archival tape backup system in a separate location from the main disk array for use in recovering data in the event of corruption, deletion, or machine failure. Note: Not all filesystems and machines are backed up. Due to limited space in the archival storage system, only /home and /project directories are backed up to archival tape. If your project has special data retention requirements, please alert RCC support staff so appropriate actions can be taken to help ensure the protection and recoverability of your data.","title":"RCC User Policy"},{"location":"user_policy/#rcc-user-policy","text":"By collaborating with the Research Computing Center (RCC), you may have access to various RCC computer systems, networks, applications, data, and/or other technology assets. By using any RCC resource, you are agreeing to abide by the usage policy below and to use RCC resources in a responsible and ethical manner. Please read the information below carefully. Violations of the RCC usage policy can result in the removal of access to RCC resources and the reporting of suspicious activities to the appropriate authorities.","title":"RCC User Policy"},{"location":"user_policy/#general-usage-policy","text":"The following principles govern the usage of all RCC resources. You are required to abide by these principles. You are responsible for the proper use of the tools each computer system provides and for the confidentiality of any sensitive information entrusted to you. You must not use RCC resources for illegal or malicious purposes such as harassment, disrupting communications or services, or unauthorized monitoring of communications. You must refrain from the unethical use of RCC resources, including unauthorized use of computer accounts and resources assigned to others, use of computing facilities for private business or political purposes or private gain, academic or scientific dishonesty, or violation of software license agreements. You will respect the confidentiality and privacy of individuals whose records you may have access to in accordance with RCC policy, University of Chicago ethical standards, and state and federal laws. You should immediately report any suspected breach of security, policy violation, or suspicious activity to help@rcc.uchicago.edu and your supervisor, principal investigator, or sponsor. You will not attempt to subvert or circumvent any system security features. You will not attempt to subvert or circumvent any system that allocates resources. You should be aware that all computer activity and files on these systems may be monitored by RCC system administrators and appropriate authorities. Your account may be terminated by RCC staff for failure to follow these principles. The RCC provides computational and data storage resources for the sole purpose of supporting legitimate research activities at the University of Chicago. A user must refrain from using RCC resources for any task other than that which was described on their account application form. RCC users are also required to familiarize themselves with the University of Chicago\u2019s Eligibility and Acceptable Use Policy for Information Technology.","title":"General usage policy"},{"location":"user_policy/#user-accounts-and-passwords","text":"Your account credentials are assigned to you alone and should not be shared with anyone, including co-workers, trainers, or computer support staff. If you believe your account password has been compromised, report the incident to RCC support staff immediately and change your password through the University of Chicago\u2019s CNetID password service. Make sure you are the only person who has access to your 2FA device. Many mechanisms exist for sharing data and working collaboratively with your colleagues, and the RCC will assist you with these tools as necessary. There is never a reason to share account and/or password information.","title":"User accounts and passwords"},{"location":"user_policy/#data-policy-and-security","text":"The RCC maintains an open research network that provides shared computing and data management resources to a large number of University of Chicago researchers and their collaborators. Therefore, the RCC cannot guarantee the complete security of data stored on any RCC system. While information security is a primary concern of the RCC, neither the RCC nor the University of Chicago shall be held liable for any breach of security resulting in the compromise of sensitive or confidential data. It is the sole responsibility of the user to ensure proper measures have been taken to protect sensitive or confidential information. RCC system administrators and support staff that have super-user privileges are able to view any and all data on RCC systems unless it is encrypted. RCC system administrators will only look at user data to provide assistance to the owner, or if there is sufficient reason to believe a serious problem or security threat exists that could disrupt the work of others.","title":"Data policy and security"},{"location":"user_policy/#protected-health-information","text":"Currently, the RCC provides MidwayR, a secure data enclave, compute resource that is compliant with the Health Insurance Portability and Accountability Act of 1996 (HIPAA) regulations for the storage or processing of Protected Health Information (PHI). If your project necessitates the use of PHI data, please inform RCC staff of your requirements and we will work to determine if your data/project can be accommodated on MidwayR.","title":"Protected Health Information"},{"location":"user_policy/#data-encryption","text":"The RCC provides encrypted file systems on MidwayR. Any data that is sensitive needs to be stored on MidwayR.","title":"Data Encryption"},{"location":"user_policy/#data-integrity-and-retention-policy","text":"The RCC strives to maintain robust filesystems and archival storage facilities with the highest levels of performance and reliability. However, neither the RCC nor the University of Chicago shall be held liable for any loss of data. The RCC provides an archival tape backup system in a separate location from the main disk array for use in recovering data in the event of corruption, deletion, or machine failure. Note: Not all filesystems and machines are backed up. Due to limited space in the archival storage system, only /home and /project directories are backed up to archival tape. If your project has special data retention requirements, please alert RCC support staff so appropriate actions can be taken to help ensure the protection and recoverability of your data.","title":"Data Integrity and Retention Policy"},{"location":"beagle3/beagle3_overview/","text":"Beagle3","title":"Beagle3"},{"location":"beagle3/beagle3_overview/#beagle3","text":"","title":"Beagle3"},{"location":"dali/","text":"DaLi User Guide The user guide will be built using MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs. Instructions Once you have installed MkDocs, it is easy to render the webpages and view them: cd dali-docs/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"DaLi User Guide"},{"location":"dali/#dali-user-guide","text":"The user guide will be built using MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs.","title":"DaLi User Guide"},{"location":"dali/#instructions","text":"Once you have installed MkDocs, it is easy to render the webpages and view them: cd dali-docs/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"Instructions"},{"location":"dali/dali_overview/","text":"DaLI","title":"DaLI"},{"location":"dali/dali_overview/#dali","text":"","title":"DaLI"},{"location":"dali/docs/","text":"Welcome to DaLi user guide The Data Lifecycle Instrument (DaLI) enables management and sharing of data from instruments and observations, allowing researchers to: acquire, transfer, process, and store data from experiments and observations in a unified workflow. manage data collections over their entire lifecycle. share and publish data. enhance outreach and education opportunities. If you have any questions about DaLi, please send an email to rt@rcc.uchicago.edu.","title":"Welcome to DaLi user guide"},{"location":"dali/docs/#welcome-to-dali-user-guide","text":"The Data Lifecycle Instrument (DaLI) enables management and sharing of data from instruments and observations, allowing researchers to: acquire, transfer, process, and store data from experiments and observations in a unified workflow. manage data collections over their entire lifecycle. share and publish data. enhance outreach and education opportunities. If you have any questions about DaLi, please send an email to rt@rcc.uchicago.edu.","title":"Welcome to DaLi user guide"},{"location":"dali/docs/connecting/","text":"Connecting to DaLi The information here describes how users can access RCC resources. All users of RCC resources are responsible for knowing and abiding by the RCC User Policy . To use DaLi resources, you will need to have a Midway user account and will need to have an approval to access to your PI directory on DaLi. If you do not have a Midway user account, please see the Getting Started page for how to apply for an account. If you don't have a permission to access to your PI directory on DaLi, please submit a ticket to rt@rcc.uchicago.edu. Account Credentials Connecting with SSH Macintosh/Linux User SSH Access Windows User SSH Access Account Credentials In order to make use of resources provided by the Research Computing Center you must first obtain a RCC user account. If you do not already have a RCC acount, see the Getting Started page for more information on obtaining a RCC account. Your RCC account uses your UChicago CNetID for the username and the corresponding CNetID password for the password: Username: CNetID Password: CNet password Please also note that you must have enabled Two Factor Authentication for your CNetID before connecting to DaLi. Connecting with an SSH client Secure Shell (SSH) is a protocol that provides secure command-line access to remote resources such as DaLi or Midway2. By using SSH, you can remotely log in to your DaLi account and interact with the DaLi high-performance compute cluster. Macintosh/Linux User SSH Access Most Unix-like operating systems (Mac OS X, Linux, etc) provide an ssh utility by default that can be accessed by typing the command ssh in a terminal window. To log in to DaLi from a Linux or Mac computer, open a terminal and at the command line enter: ssh CNetID@dali-login.rcc.uchicago.edu Provide your CNetID password when prompted for a password. A Duo two-factor autentication window will then pop up requesting you select from the available 2FA options to authenticate to DaLi: ![Screenshot showing PuTTY SSH keyalert](images/duo-2fa.png) **Note** SSH key-based authentication is no longer supported. The SSH password-based authentication is currently the only supported method for authentication. Choose from the available two-factor authentication options and finish the authentication process. To enable X11 forwarding when connecting to DaLi with ssh, the -Y flag should be included. ssh -Y CNetID@dali-login.rcc.uchicago.edu **Note** XQuartz is required to enable trusted X11 forwarding on a Mac. Windows User SSH Access Windows users running Windows 10\u2019s April 2018 release will have ssh enabled from the Powershell by default. All other Windows users will first need to download an ssh client to interact with the remote Unix command line. We recommend the MobaXterm , client, although other options are available. Once the MobaXterm client is installed on your local machine, open the MobaXterm client and click on the Sessions icon at the upper left hand corner of the client. Then perform the following numbered steps, illustrated in the figure below, to establish a connection to DaLi. 1. Click the SSH tab to expand the SSH login options. 2. In the Remote host field input: dali-login.rcc.uchicago.edu (to connect to DaLi cluster) 1. Select the Specify username button and input your CNetID 2. Proceed to log in by clicking the OK button. ![Screenshot showing MobaXterm Login](images/mobaxterm.png) Provide your CNetID password when prompted for a password. A Duo two-factor autentication window will then pop up requesting you select from the available 2FA options to authenticate to DaLi: ![Screenshot showing PuTTY SSH keyalert](images/duo-2fa.png) Choose from the available two-factor authentication options and finish the authentication process.","title":"Index"},{"location":"dali/docs/data-storage/","text":"Data Storage on DaLI The information here describes how users can access RCC resources. All users of RCC resources are responsible for knowing and abiding by the RCC User Policy . RCC provides a high-performance GPFS shared file system on DaLI. All DaLI project directories are accessible on Midway2 via the directory /dali . Quotas The amount of data that can be stored on DaLI is controlled by quota. RCC enforces hard and soft limits on quotas. A soft quota can be exceeded for a short period of time called a grace period. The hard quota cannot be exceeded under any circumstances. Additional storage is available through the Cluster Partnership Program , - Research I Allocation , Research II Allocation or, in certain circumstances, - Special Allocation . Checking available space To check your current quotas use the quota command. Typical output may look like this The output could have up to three sections. The top section displays information about the home directory and the scratch space (scratch2). The middle section displays information about the project2 space. The bottom section displays information about the DaLi space. Depending on how many groups you are part of, you may see multiple lines in the middle and the bottom sections. Descriptions of the fields: fileset : This is the file set or file system where this quota is valid. type : This is the type of quota. This can be blocks for the amount of consumed disk space or files for the number of files in a directory. Either of blocks or files quotas can be set at the user or group level. The quota on the home directory and the scratch space is set as per user basis and the quota on the scratch space is set as per group basis. used : This is the amount of disk space consumed or the number of files in the specified location. quota : This is the soft quota (storage space or file count) associated with the specified location. It is possible for usage to exceed the soft quota for the grace period or up to the hard limit. limit : This is the hard quota (storage space or file count) associated with the specified location. When your usage exceeds this limit, you will NOT be able to write to that filesystem. grace : This is the grace period, or the amount of time remaining that the soft quota can be exceeded. The value none means that the quota is not exceeded. After a soft quota has been exceeded for longer than the grace period, it will no longer be possible to create new files. Persistent Space Persistent space is appropriate for long term storage. The location for persistent space on DaLI is the directory /dali . These directories have both file system :ref: snapshots and :ref: tape backup for data protection. Home Directories RCC users' home directories are accessible from DaLI and are generally used for storing frequently used items such as source code, binaries, and scripts. By default, a home directory is only accessible by its owner (mode 0700 ) and is suitable for storing files which do not need to be shared with others. Project Directories On DaLI, RCC PI Groups are allocated a Project Directory located at :file: /dali/<PI CNetID> where is the CNetID of your RCC PI account holder. These directories are accessible by all members of the PI Group and are generally used for storing files which need to be shared by members of the group. Additional storage in project directories is available through the Cluster Partnership Program , - Research I Allocation , Research II Allocation or, in certain circumstances, - Special Allocation . The default permissions for files and directories created in a project directory allow group read/write with the group sticky bit set (mode 2770 ). The group ownership is set to the PI group. Scratch Space Shared Scratch Space High performance shared scratch space can be accessed on DaLI using the SCRATCH environment variable. This scratch space is intended to be used for reading or writing data required by jobs running on the cluster. If a user is over quota, s/he can use scratch space as a temporary location to hold files (and/or compress them for archival purposes) but as scratch space is neither snapshotted nor backed up, it should always be viewed as temporary. {>>Note: It is the responsibility of the user to ensure any important data in scratch space is moved to persistent storage. Scratch space is meant to be used for temporary, short-term storage only.\\<<} The default permissions for scratch space allow access only by its owner (mode 0700 ). The standard quota for the high performance scratch directory is 5 TB with a 100GB soft limit. The grace period that the soft limit may be exceeded is 30 days for shared scratch space. File System Permissions Let's summarize the default file system permissions: Directory Permissions $HOME 0700 -- Accessible only to the owner $SCRATCH 0700 -- Accessible only to the owner /dali/<PI CNetID> 2770 -- Read/write for the project group /project2/<PI CNetID> 2770 -- Read/write for the project group The default umask is 002 . When new files or directories are created, the umask influences the default permissions of those files and directories. With the umask set to 002 all files and directories will be group readable and writable by default. In your home directory, the group ownership will be set to your personal group, which is the same as your CNetID, so you will still be the only user that can access your files and directories. In the project directories, the group sticky bit causes the group ownership to be the same as the directory. This means files created in a project directory will be readable and writable by the project group, which is typically what is wanted in those directories. Contact RCC help if you need assistance with setting filesystem permissions. Data Recovery and Backups Snapshots Automated snapshots of project directories on DaLI are available in case of accidental file deletion or other problems. Currently snapshots are available for these time periods: Directory Permissions Snapshot path $HOME 7 daily and 2 weekly /snapshots/home/SNAPSHOT/home/CNetID dali/<any_folder> 7 daily and 4 weekly /gpfs3/cap/.snapshots/<any_folder> The snapshots for the dali directories are available from the DaLI login nodes. The {SNAPSHOT} refers to the time of the backup, e.g. daily-YYYY-MM-DD.05h30 or weekly-YYYY-MM-DD.05h30. To view the available snapshots, for example, from the DaLI login node use the command ls /gpfs3/cap/.snapshots To restore a file from a snapshot, simply copy the file to where you want it with either cp or rsync . Advanced Access Control via ACL General Instructions This section discusses a more flexible mechanism to administer data permissions. By default, only Linux-based permissions are set for folders and files, as described in File System Permissions . However, this only supports the permissions at the owner/group/others level. A second mechanism is called \u201cAccess Control Lists\u201d (ACL), which provides precise control over any data (files or directories) customizable for individual users or groups. Before applying ACL to your data, please read and understand the following caveats. By default no ACL is set for user data. ACL provides a highly flexible permission control, however, it also brings increased complexity to user access and management. PIs will normally want to share an entire project folder to all group members, and for this, the Linux-based permissions are enough. We suggest that users implement ACL controls only when necessary. One example is to protect confidential data in the project space by allowing only certain users to access confidential directories or files. After ACL is set, both Linux-based and ACL permissions will work together as a dual-guard system. The final effective access to data is granted only if permitted by both mechanisms. For example, if a folder is group-accessible to a user by Linux-based permission but restricted by ACL, the user cannot access this folder. Be sure you have enough knowledge setting up access via Linux-based permissions and ACL, i.e. you understand what \u201cusers\u201d, \u201cgroups\u201d and each attribute in \u201crwx\u201d mean and how to use them. Otherwise, please ask help@rcc.uchicago.edu for assistance managing your data access. We are here and happy to help you set up the permissions to keep your data safe and accessible as required. Example Suppose there is a folder tree as below, and you want to allow the folder my_folder to be accessible by the user jim only, and jim is already a member of your group rcctemp1 : /dali/rcctemp1 |- my_folder |- other_stuff Before using ACL, you need to confirm that this folder is permitted by all members in the group rcctemp1 : $ cd /dali $ chgrp -R rcctemp1 my_folder $ chmod -R 770 rcctemp1 $ cd rcctemp1 At this moment, the folder rcctemp1 becomes readable and writable by all members of group rcctemp1 . Then, you can use the setfacl command to control the individual users access precisely. First, you need to remove the default group access by ACL:: $ setfacl -m g::--- my_folder Although the command ls -l will still display group rwx access for the my_folder folder in the Linux-based permissions, users cannot access it anymore due to the permission set by ACL. Then, you can grant the user jim access to the folder:: $ setfacl -m u:jim:rwx my_folder At this step, the user jim has both read and write permissions to the folder my_folder . You can set up permissions for each user the way you want. Tip The format of the command is setfacl -m [level]:[id]:[permissions] [folder_or_file_name] , where the level bit is either u or g as for an individual user or a group, the id is the username or group name to be set, and the permissions are the three bits r , w , x or - (means off) with the same meanings as Linux-based attributes. To view the list of configured accesses on the folder my_folder , run:: $ getfacl my_folder # file: my_folder # owner: root # group: rcctemp1 user::rwx user:jim:rwx group::--- mask::rwx other::--- To revoke the permissions of the user jim to the folder:: $ setfacl -x u:jim my_folder To clean up (remove) all ACL controls to the folder:: $ setfacl -b my_folder To change the ACL configuraitons recursively in all subfolders, you can add -R option to the commands above. For more information, please visit the ACL manual .","title":"Index"},{"location":"dali/docs/data-storage/#data-storage-on-dali","text":"The information here describes how users can access RCC resources. All users of RCC resources are responsible for knowing and abiding by the RCC User Policy . RCC provides a high-performance GPFS shared file system on DaLI. All DaLI project directories are accessible on Midway2 via the directory /dali .","title":"Data Storage on DaLI"},{"location":"dali/docs/data-storage/#quotas","text":"The amount of data that can be stored on DaLI is controlled by quota. RCC enforces hard and soft limits on quotas. A soft quota can be exceeded for a short period of time called a grace period. The hard quota cannot be exceeded under any circumstances. Additional storage is available through the Cluster Partnership Program , - Research I Allocation , Research II Allocation or, in certain circumstances, - Special Allocation .","title":"Quotas"},{"location":"dali/docs/data-storage/#checking-available-space","text":"To check your current quotas use the quota command. Typical output may look like this The output could have up to three sections. The top section displays information about the home directory and the scratch space (scratch2). The middle section displays information about the project2 space. The bottom section displays information about the DaLi space. Depending on how many groups you are part of, you may see multiple lines in the middle and the bottom sections. Descriptions of the fields: fileset : This is the file set or file system where this quota is valid. type : This is the type of quota. This can be blocks for the amount of consumed disk space or files for the number of files in a directory. Either of blocks or files quotas can be set at the user or group level. The quota on the home directory and the scratch space is set as per user basis and the quota on the scratch space is set as per group basis. used : This is the amount of disk space consumed or the number of files in the specified location. quota : This is the soft quota (storage space or file count) associated with the specified location. It is possible for usage to exceed the soft quota for the grace period or up to the hard limit. limit : This is the hard quota (storage space or file count) associated with the specified location. When your usage exceeds this limit, you will NOT be able to write to that filesystem. grace : This is the grace period, or the amount of time remaining that the soft quota can be exceeded. The value none means that the quota is not exceeded. After a soft quota has been exceeded for longer than the grace period, it will no longer be possible to create new files.","title":"Checking available space"},{"location":"dali/docs/data-storage/#persistent-space","text":"Persistent space is appropriate for long term storage. The location for persistent space on DaLI is the directory /dali . These directories have both file system :ref: snapshots and :ref: tape backup for data protection.","title":"Persistent Space"},{"location":"dali/docs/data-storage/#home-directories","text":"RCC users' home directories are accessible from DaLI and are generally used for storing frequently used items such as source code, binaries, and scripts. By default, a home directory is only accessible by its owner (mode 0700 ) and is suitable for storing files which do not need to be shared with others.","title":"Home Directories"},{"location":"dali/docs/data-storage/#project-directories","text":"On DaLI, RCC PI Groups are allocated a Project Directory located at :file: /dali/<PI CNetID> where is the CNetID of your RCC PI account holder. These directories are accessible by all members of the PI Group and are generally used for storing files which need to be shared by members of the group. Additional storage in project directories is available through the Cluster Partnership Program , - Research I Allocation , Research II Allocation or, in certain circumstances, - Special Allocation . The default permissions for files and directories created in a project directory allow group read/write with the group sticky bit set (mode 2770 ). The group ownership is set to the PI group.","title":"Project Directories"},{"location":"dali/docs/data-storage/#scratch-space","text":"","title":"Scratch Space"},{"location":"dali/docs/data-storage/#shared-scratch-space","text":"High performance shared scratch space can be accessed on DaLI using the SCRATCH environment variable. This scratch space is intended to be used for reading or writing data required by jobs running on the cluster. If a user is over quota, s/he can use scratch space as a temporary location to hold files (and/or compress them for archival purposes) but as scratch space is neither snapshotted nor backed up, it should always be viewed as temporary. {>>Note: It is the responsibility of the user to ensure any important data in scratch space is moved to persistent storage. Scratch space is meant to be used for temporary, short-term storage only.\\<<} The default permissions for scratch space allow access only by its owner (mode 0700 ). The standard quota for the high performance scratch directory is 5 TB with a 100GB soft limit. The grace period that the soft limit may be exceeded is 30 days for shared scratch space.","title":"Shared Scratch Space"},{"location":"dali/docs/data-storage/#data-recovery-and-backups","text":"","title":"Data Recovery and Backups"},{"location":"dali/docs/data-storage/#snapshots","text":"Automated snapshots of project directories on DaLI are available in case of accidental file deletion or other problems. Currently snapshots are available for these time periods: Directory Permissions Snapshot path $HOME 7 daily and 2 weekly /snapshots/home/SNAPSHOT/home/CNetID dali/<any_folder> 7 daily and 4 weekly /gpfs3/cap/.snapshots/<any_folder> The snapshots for the dali directories are available from the DaLI login nodes. The {SNAPSHOT} refers to the time of the backup, e.g. daily-YYYY-MM-DD.05h30 or weekly-YYYY-MM-DD.05h30. To view the available snapshots, for example, from the DaLI login node use the command ls /gpfs3/cap/.snapshots To restore a file from a snapshot, simply copy the file to where you want it with either cp or rsync .","title":"Snapshots"},{"location":"dali/docs/data-storage/#advanced-access-control-via-acl","text":"","title":"Advanced Access Control via ACL"},{"location":"dali/docs/data-storage/#general-instructions","text":"This section discusses a more flexible mechanism to administer data permissions. By default, only Linux-based permissions are set for folders and files, as described in File System Permissions . However, this only supports the permissions at the owner/group/others level. A second mechanism is called \u201cAccess Control Lists\u201d (ACL), which provides precise control over any data (files or directories) customizable for individual users or groups. Before applying ACL to your data, please read and understand the following caveats. By default no ACL is set for user data. ACL provides a highly flexible permission control, however, it also brings increased complexity to user access and management. PIs will normally want to share an entire project folder to all group members, and for this, the Linux-based permissions are enough. We suggest that users implement ACL controls only when necessary. One example is to protect confidential data in the project space by allowing only certain users to access confidential directories or files. After ACL is set, both Linux-based and ACL permissions will work together as a dual-guard system. The final effective access to data is granted only if permitted by both mechanisms. For example, if a folder is group-accessible to a user by Linux-based permission but restricted by ACL, the user cannot access this folder. Be sure you have enough knowledge setting up access via Linux-based permissions and ACL, i.e. you understand what \u201cusers\u201d, \u201cgroups\u201d and each attribute in \u201crwx\u201d mean and how to use them. Otherwise, please ask help@rcc.uchicago.edu for assistance managing your data access. We are here and happy to help you set up the permissions to keep your data safe and accessible as required.","title":"General Instructions"},{"location":"dali/docs/data-storage/#example","text":"Suppose there is a folder tree as below, and you want to allow the folder my_folder to be accessible by the user jim only, and jim is already a member of your group rcctemp1 : /dali/rcctemp1 |- my_folder |- other_stuff Before using ACL, you need to confirm that this folder is permitted by all members in the group rcctemp1 : $ cd /dali $ chgrp -R rcctemp1 my_folder $ chmod -R 770 rcctemp1 $ cd rcctemp1 At this moment, the folder rcctemp1 becomes readable and writable by all members of group rcctemp1 . Then, you can use the setfacl command to control the individual users access precisely. First, you need to remove the default group access by ACL:: $ setfacl -m g::--- my_folder Although the command ls -l will still display group rwx access for the my_folder folder in the Linux-based permissions, users cannot access it anymore due to the permission set by ACL. Then, you can grant the user jim access to the folder:: $ setfacl -m u:jim:rwx my_folder At this step, the user jim has both read and write permissions to the folder my_folder . You can set up permissions for each user the way you want.","title":"Example"},{"location":"dali/docs/data-storage/#tip","text":"The format of the command is setfacl -m [level]:[id]:[permissions] [folder_or_file_name] , where the level bit is either u or g as for an individual user or a group, the id is the username or group name to be set, and the permissions are the three bits r , w , x or - (means off) with the same meanings as Linux-based attributes. To view the list of configured accesses on the folder my_folder , run:: $ getfacl my_folder # file: my_folder # owner: root # group: rcctemp1 user::rwx user:jim:rwx group::--- mask::rwx other::--- To revoke the permissions of the user jim to the folder:: $ setfacl -x u:jim my_folder To clean up (remove) all ACL controls to the folder:: $ setfacl -b my_folder To change the ACL configuraitons recursively in all subfolders, you can add -R option to the commands above. For more information, please visit the ACL manual .","title":"Tip"},{"location":"dali/docs/data-transfer/","text":"Data Transfer to DaLI RCC provides a number of methods for accessing and transferring data in/out of DaLi. We recommend the :command: scp protocol for transferring files to/from RCC systems. RCC hosts a managed Globus Online endpoint that can be used for moving very large amounts of data. SCP Secure copy or SCP is a means of securely transferring computer files between a local host and a remote host. It is based on the Secure Shell (SSH) and Secure File Transfer (SFTP) protocols. Command-Line Operation Most UNIX-like operating systems (Mac OS X, Linux, etc) provide a:command: scp command which can be accessed from the command line. To transfer files from your local computer to your home directory on DaLI, open a terminal window and issue the command:: Single files: $ scp <some file> <CNetID>@dali-login.rcc.uchicago.edu: Directories: $ scp -r <some dir> <CNetID>@dali-login.rcc.uchicago.edu: When prompted, enter your CNet password. WinSCP GUI for Windows Clients WinSCP is a scp client software that can be used to move files to and from DaLI and a Windows machine. WinSCP can be obtained from http://www.winscp.net. When setting up your connection to DaLI in WinSCP, use the following information:: Hostname: dali-login.rcc.uchicago.edu Port: 22 Username: CNetID Password: CNet password After connecting, if you are prompted to accept the server's host key, select \"yes.\" Upon successfully connecting, the main WinSCP window allows you to move files from your local machine (left side) to DaLI (right side). Globus Online Globus Online is a robust tool for transferring large data files to/from DaLI. The RCC has a customized Globus Online login site at https://globus.rcc.uchicago.edu and uses Single Sign On capabilities of CILogon. If you have already signed up, here is the connection information:: URL: https://globus.rcc.uchicago.edu End Point: UChicago RCC DaLI Follow these instructions to get started: Go to https://globus.rcc.uchicago.edu and hit Proceed Select \"University of Chicago\" for the Identity Provider Enter your CNetID and password when prompted You will need to link your University of Chicago credentials to a Globus Online account. Either create a new Globus Online account or sign in to your existing account if you have one. Once you are signed in, enter UChicago RCC DaLI as the Endpoint and hit the Go button If you want to transfer files from your local computer, click the More Options link on the File Manager page and follow the instructions in Install Globus Connect Personal . There is extensive documentation on the Globus Online site as to how to transfer files in different modes. Please refer to their documentation for more details or contact us with any RCC specific issues.","title":"Index"},{"location":"dali/docs/data-transfer/#data-transfer-to-dali","text":"RCC provides a number of methods for accessing and transferring data in/out of DaLi. We recommend the :command: scp protocol for transferring files to/from RCC systems. RCC hosts a managed Globus Online endpoint that can be used for moving very large amounts of data.","title":"Data Transfer to DaLI"},{"location":"dali/docs/data-transfer/#scp","text":"Secure copy or SCP is a means of securely transferring computer files between a local host and a remote host. It is based on the Secure Shell (SSH) and Secure File Transfer (SFTP) protocols.","title":"SCP"},{"location":"dali/docs/data-transfer/#command-line-operation","text":"Most UNIX-like operating systems (Mac OS X, Linux, etc) provide a:command: scp command which can be accessed from the command line. To transfer files from your local computer to your home directory on DaLI, open a terminal window and issue the command:: Single files: $ scp <some file> <CNetID>@dali-login.rcc.uchicago.edu: Directories: $ scp -r <some dir> <CNetID>@dali-login.rcc.uchicago.edu: When prompted, enter your CNet password.","title":"Command-Line Operation"},{"location":"dali/docs/data-transfer/#winscp-gui-for-windows-clients","text":"WinSCP is a scp client software that can be used to move files to and from DaLI and a Windows machine. WinSCP can be obtained from http://www.winscp.net. When setting up your connection to DaLI in WinSCP, use the following information:: Hostname: dali-login.rcc.uchicago.edu Port: 22 Username: CNetID Password: CNet password After connecting, if you are prompted to accept the server's host key, select \"yes.\" Upon successfully connecting, the main WinSCP window allows you to move files from your local machine (left side) to DaLI (right side).","title":"WinSCP GUI for Windows Clients"},{"location":"dali/docs/data-transfer/#globus-online","text":"Globus Online is a robust tool for transferring large data files to/from DaLI. The RCC has a customized Globus Online login site at https://globus.rcc.uchicago.edu and uses Single Sign On capabilities of CILogon. If you have already signed up, here is the connection information:: URL: https://globus.rcc.uchicago.edu End Point: UChicago RCC DaLI Follow these instructions to get started: Go to https://globus.rcc.uchicago.edu and hit Proceed Select \"University of Chicago\" for the Identity Provider Enter your CNetID and password when prompted You will need to link your University of Chicago credentials to a Globus Online account. Either create a new Globus Online account or sign in to your existing account if you have one. Once you are signed in, enter UChicago RCC DaLI as the Endpoint and hit the Go button If you want to transfer files from your local computer, click the More Options link on the File Manager page and follow the instructions in Install Globus Connect Personal . There is extensive documentation on the Globus Online site as to how to transfer files in different modes. Please refer to their documentation for more details or contact us with any RCC specific issues.","title":"Globus Online"},{"location":"dali/docs/running-jobs/","text":"Running jobs on DaLi This section of DaLi user guide is about running computations on the DaLi compute nodes. All jobs running on compute nodes consume Service Units (SUs); see RCC Service Units for more information. Login nodes vs. compute nodes Once you have connected to DaLi, you may work on one of the login nodes. Login nodes may be used for compiling and debugging code, installing software, editing and managing files, submitting jobs, or any other work that is not long-running or computationally intensive. Login nodes should not be used for computionally intensive work . The Midway compute cluster, including DaLi, uses a scheduler to manage requests for access to compute resources. These requests are called jobs. In particular, we use the Slurm resource manager to schedule jobs as well as interactive access to compute nodes. Here, we give the essential information you need to know to start computing on Midway. For more detailed information on running specialized compute jobs, see Running jobs on Midway . Service Units and Allocations Service Units (SUs) are a measure of the amount of computing resources consumed on a compute cluster. Computing resources in a compute cluster include processing units (also called CPUs or cores), memory, and Graphical Processing Units (GPUs). The aim of the Service Unit (SU) is to provide a \u201cfair\u201d account of computing resources. For more information, please refer to the RCC Service Units webpage. An \u201callocation\u201d is a quantity of computing time (SUs) and storage resources that are granted to a group of users, usually a lab managed by a principal investigator (PI). Without an allocation, you cannot schedule and run jobs on the RCC compute cluster. For more information about SU allocations, see RCC Allocations . Checking your account balance The rcchelp tool can be used to check account balances. After logging into Midway, simply type: $ rcchelp balance If you are a member of multiple groups, this will display the allocations and usage for all your groups. The rcchelp balance command has a number of options for summarizing allocation usage. For information on these options, type $ rcchelp balance --help To see an overall summary of your usage, simply enter: $ rcchelp usage You can also get a more detailed breakdown of your usage by job using the --byjob option: $ rcchelp usage --byjob For more options available in the rcchelp tool, type $ rcchelp --help Types of Compute Nodes The Midway compute cluster, including DaLi, is made up of compute nodes with a variety architectures and configurations. Currently, DaLi has 30 nodes with the following specifications: Cluster Partition Compute cores (CPUs) per node Memory Other configuraiton details Dali dali 2 SkyLake processors (40 CPUs) 64 GB DDR4 EDR and FDR Infiniband interconnect Interactive Jobs After submitting an \u201cinteractive job\u201d on Midway, the Slurm job scheduler will connect you to a compute node, and will load up an interactive shell environment for you to use on that compute node. This interactive session will persist until you disconnect from the compute node, or until you reach the maximum requested time. The default requested time is 2 hours. sinteractive The command sinteractive is the recommended Slurm command for requesting an interactive session. As soon as the requested resources become available, sinteractive will do the following: Log in to the node. Change into the directory you were working in. Set up X11 forwarding for displaying graphics. Transfer your current shell environment, including any modules you have previously loaded. To get started (with the default interactive settings), simply enter sinteractive in the command line: $ sinteractive By default, an interactive session times out after 2 hours. If you would like more than 2 hours, be sure to include a -- time=HH:MM:SS flag to specify the necessary amount of time. For example, to request an interactive session for 6 hours, run the following command: $ sinteractive --time=06:00:00 There are many additional options for the sinteractive command, including options to select the number of nodes, the number of cores per node, the amount of memory, and so $ sinteractive --exclusive --partition=dali --nodes=2 --time=08:00:00 For more details about these and other useful options, read below about the sbatch command, and see Running jobs on midway. Note that all options available in the sbatch command are also available for the sinteractive command. srun An alternative to the sinteractive command is the srun command: $ srun --pty bash Unlike sinteractive , this command does not set up X11 forwarding, which means you cannot display graphics using srun . Both the srun and sinteractive commands have the same command options. Batch Jobs\u00b6 The sbatch command is the command most commonly used by RCC users to request computing resources on the Midway cluster including DaLi. Rather than specify all the options in the command line, users typically write an sbatch script that contains all the commands and parameters necessary to run the program on the cluster. In an sbatch script, all Slurm parameters are declared with #SBATCH , followed by additional definitions. Here is an example of an sbatch script: #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:05:00 #SBATCH --partition=dali #SBATCH --nodes=4 #SBATCH --ntasks-per-node=20 #SBATCH --mem-per-cpu=3000 #SBATCH --account=pi-rcc module load openmpi mpirun ./hello-mpi Here is an explanation of what each of these options does: Option Description #SBATCH --job-name=example_sbatch Assigns label example_sbatch to the job. #SBATCH --output=example_sbatch.out Writes console output to file example_sbatch.out. #SBATCH --error=example_sbatch.err Writes an error messages to file example_sbatch.err. #SBATCH --time=01:30:00 Reserves the computing resources for 1 hour and 30 minutes (or less if program completes before 1.5 hours). #SBATCH --partition=dali Requests compute nodes from the DaLi partition on the Midway cluster. #SBATCH --nodes=4 Requests 4 compute nodes. #SBATCH --ntasks-per-node=20 Requests 20 cores (CPUs) per node, for a total of 20 * 4 = 80 cores. #SBATCH --mem-per-cpu=3000 Requests 3000 MB (3 GB) of memory (RAM) per core, for a total of 3 * 20 = 60 GB per node. #SBATCH --account=pi-rcc Use SU from the pi-rcc account allocation. In this example, we have requested 4 compute nodes with 20 CPUs each. Therefore, we have requested a total of 60 CPUs for running our program. The last two lines of the script load the OpenMPI module and launch the MPI-based executable that we have called hello-mpi (see MPI jobs ). Continuing the example above, suppose that this script is saved in the current directory into a file called example.sbatch . This script is submitted to the cluster using the following command: $ sbatch ./example.sbatch Many other options are available for submitting jobs using the sbatch command. For more specialized computational needs, see Running jobs on midway . Additionally, for a complete list of the available options, see the Official SBATCH Documentation . Managing Jobs The Slurm job scheduler provides several command-line tools for checking on the status of your jobs and for managing them. For a complete list of Slurm commands, see the Slurm man pages . Here are a few commands that you may find particularly useful: squeue : finds out the status of jobs submitted by you and other users. sacct : retrieves job history and statistics about past jobs. scancel : cancels jobs you have submitted. In the next couple sections we explain how to use squeue to find out the status of your submitted jobs, and scancel to cancel jobs in the queue. Checking your jobs Use the squeue command to check on the status of your jobs, and other jobs running on Midway. The simplest invocation lists all jobs that are currently running or waiting in the job queue (\u201cpending\u201d), along with details about each job such as the job id and the number of nodes requested: $ squeue Any job with 0:00 under the TIME column is a job that is still waiting in the queue. To view only the jobs that you have submitted, use the --user flag $ squeue --user=$USER This command has many other useful options for querying the status of the queue and getting information about individual jobs. For example, to get information about all jobs that are waiting to run on DaLi partition, enter: $ squeue --state=PENDING --partition=dali Alternatively, to get information about all your jobs that are running on the DaLi partition, type: $ squeue --state=RUNNING --partition=dali --user=$USER The last column of the output tells us which nodes are allocated for each job. For example, if it shows dali-012 for one of the jobs under your name, you may type ssh DaLI-012 to log in to that compute node and inspect the progress of your computation locally. For more information, consult the command-line help by typing squeue --help , or visit the official online documentation. Canceling your jobs To cancel a job you have submitted, use the scancel command. This requires you to specify the id of the job you wish to cancel. For example, to cancel a job with id 8885128 , do the following: $ scancel 8885128 If you are unsure what is the id of the job you would like to cancel, see the JOBID column from running squeue --user=$USER . To cancel all jobs you have submitted that are either running or waiting in the queue, enter the following: $ scancel --user=$USER Job Limits To distribute computational resources fairly to all Midway users even among DaLi users, the RCC sets limits on the amount of computing resources that may be requested by a single user at any given time. On DaLi, the maximum run-time for an individual job is 36 hours. The maximum number of jobs per user is 200 and the maximum number of jobs that each user can submit is 500. These apply to all batch and interactive jobs submitted to DaLi compute nodes. Temporary File Storage Many applications generate temporary or intermediate files that are written to /tmp . (These applications may write files to /tmp even without you being aware that this is happening.) This folder is typically on a local drive or the RAM disk that virtualized in the system memory. Contents in /tmp left by a user\u2019s job won\u2019t be automatically purged before rebooting the corresponding node, and therefore may affect other jobs later running on the same node. Therefore, RCC enforces a data purge policy for files written to /tmp on compute nodes: For each running job, a special \u201cjob-protected\u201d folder /tmp/jobs/${SLURM_JOB_ID} is created on each allocated node. Its contents are safely purged only upon termination of the job (when it is sucessfully completed, canceled or killed). For any running jobs, environment variables SLURM_TMPDIR and TMPDIR are set to /tmp/jobs/${SLURM_JOB_ID} . Whenever possible, users should write to the paths specified by these environment variables rather than using /tmp explicitly. (Most applications should already be using these environment variables by default, so in many cases this will not require any change to your code.) In addition to using $TMPDIR , users should also verify that no additional files are being written to /tmp . Note that upon termination of a job, any folders or files directly under /tmp that belong to the submitter of this job will be purged. The contents of /tmp do not persist after jobs terminate. The RCC is not responsible for retrieving or recovering data stored there. For critical outputs, please save them to the persisent file storage systems. Note: Folders or files created by users in /tmp outside $TMPDIR are NOT job-protected. For example, consider the case when user has two running jobs (A and B) on the same node, and job B is directly writing files in /tmp . If job A terminates before job B, the contents of /tmp will be also purged, and in some cases may cause job B to fail. To avoid failure, users should therefore write any temporary data to the job-protected folder, SLURM_TMPDIR or TMPDIR .","title":"Index"},{"location":"dali/docs/running-jobs/#running-jobs-on-dali","text":"This section of DaLi user guide is about running computations on the DaLi compute nodes. All jobs running on compute nodes consume Service Units (SUs); see RCC Service Units for more information.","title":"Running jobs on DaLi"},{"location":"dali/docs/running-jobs/#login-nodes-vs-compute-nodes","text":"Once you have connected to DaLi, you may work on one of the login nodes. Login nodes may be used for compiling and debugging code, installing software, editing and managing files, submitting jobs, or any other work that is not long-running or computationally intensive. Login nodes should not be used for computionally intensive work . The Midway compute cluster, including DaLi, uses a scheduler to manage requests for access to compute resources. These requests are called jobs. In particular, we use the Slurm resource manager to schedule jobs as well as interactive access to compute nodes. Here, we give the essential information you need to know to start computing on Midway. For more detailed information on running specialized compute jobs, see Running jobs on Midway .","title":"Login nodes vs. compute nodes"},{"location":"dali/docs/running-jobs/#service-units-and-allocations","text":"Service Units (SUs) are a measure of the amount of computing resources consumed on a compute cluster. Computing resources in a compute cluster include processing units (also called CPUs or cores), memory, and Graphical Processing Units (GPUs). The aim of the Service Unit (SU) is to provide a \u201cfair\u201d account of computing resources. For more information, please refer to the RCC Service Units webpage. An \u201callocation\u201d is a quantity of computing time (SUs) and storage resources that are granted to a group of users, usually a lab managed by a principal investigator (PI). Without an allocation, you cannot schedule and run jobs on the RCC compute cluster. For more information about SU allocations, see RCC Allocations .","title":"Service Units and Allocations"},{"location":"dali/docs/running-jobs/#checking-your-account-balance","text":"The rcchelp tool can be used to check account balances. After logging into Midway, simply type: $ rcchelp balance If you are a member of multiple groups, this will display the allocations and usage for all your groups. The rcchelp balance command has a number of options for summarizing allocation usage. For information on these options, type $ rcchelp balance --help To see an overall summary of your usage, simply enter: $ rcchelp usage You can also get a more detailed breakdown of your usage by job using the --byjob option: $ rcchelp usage --byjob For more options available in the rcchelp tool, type $ rcchelp --help","title":"Checking your account balance"},{"location":"dali/docs/running-jobs/#types-of-compute-nodes","text":"The Midway compute cluster, including DaLi, is made up of compute nodes with a variety architectures and configurations. Currently, DaLi has 30 nodes with the following specifications: Cluster Partition Compute cores (CPUs) per node Memory Other configuraiton details Dali dali 2 SkyLake processors (40 CPUs) 64 GB DDR4 EDR and FDR Infiniband interconnect","title":"Types of Compute Nodes"},{"location":"dali/docs/running-jobs/#interactive-jobs","text":"After submitting an \u201cinteractive job\u201d on Midway, the Slurm job scheduler will connect you to a compute node, and will load up an interactive shell environment for you to use on that compute node. This interactive session will persist until you disconnect from the compute node, or until you reach the maximum requested time. The default requested time is 2 hours.","title":"Interactive Jobs"},{"location":"dali/docs/running-jobs/#sinteractive","text":"The command sinteractive is the recommended Slurm command for requesting an interactive session. As soon as the requested resources become available, sinteractive will do the following: Log in to the node. Change into the directory you were working in. Set up X11 forwarding for displaying graphics. Transfer your current shell environment, including any modules you have previously loaded. To get started (with the default interactive settings), simply enter sinteractive in the command line: $ sinteractive By default, an interactive session times out after 2 hours. If you would like more than 2 hours, be sure to include a -- time=HH:MM:SS flag to specify the necessary amount of time. For example, to request an interactive session for 6 hours, run the following command: $ sinteractive --time=06:00:00 There are many additional options for the sinteractive command, including options to select the number of nodes, the number of cores per node, the amount of memory, and so $ sinteractive --exclusive --partition=dali --nodes=2 --time=08:00:00 For more details about these and other useful options, read below about the sbatch command, and see Running jobs on midway. Note that all options available in the sbatch command are also available for the sinteractive command.","title":"sinteractive"},{"location":"dali/docs/running-jobs/#srun","text":"An alternative to the sinteractive command is the srun command: $ srun --pty bash Unlike sinteractive , this command does not set up X11 forwarding, which means you cannot display graphics using srun . Both the srun and sinteractive commands have the same command options.","title":"srun"},{"location":"dali/docs/running-jobs/#batch-jobs","text":"The sbatch command is the command most commonly used by RCC users to request computing resources on the Midway cluster including DaLi. Rather than specify all the options in the command line, users typically write an sbatch script that contains all the commands and parameters necessary to run the program on the cluster. In an sbatch script, all Slurm parameters are declared with #SBATCH , followed by additional definitions. Here is an example of an sbatch script: #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:05:00 #SBATCH --partition=dali #SBATCH --nodes=4 #SBATCH --ntasks-per-node=20 #SBATCH --mem-per-cpu=3000 #SBATCH --account=pi-rcc module load openmpi mpirun ./hello-mpi Here is an explanation of what each of these options does: Option Description #SBATCH --job-name=example_sbatch Assigns label example_sbatch to the job. #SBATCH --output=example_sbatch.out Writes console output to file example_sbatch.out. #SBATCH --error=example_sbatch.err Writes an error messages to file example_sbatch.err. #SBATCH --time=01:30:00 Reserves the computing resources for 1 hour and 30 minutes (or less if program completes before 1.5 hours). #SBATCH --partition=dali Requests compute nodes from the DaLi partition on the Midway cluster. #SBATCH --nodes=4 Requests 4 compute nodes. #SBATCH --ntasks-per-node=20 Requests 20 cores (CPUs) per node, for a total of 20 * 4 = 80 cores. #SBATCH --mem-per-cpu=3000 Requests 3000 MB (3 GB) of memory (RAM) per core, for a total of 3 * 20 = 60 GB per node. #SBATCH --account=pi-rcc Use SU from the pi-rcc account allocation. In this example, we have requested 4 compute nodes with 20 CPUs each. Therefore, we have requested a total of 60 CPUs for running our program. The last two lines of the script load the OpenMPI module and launch the MPI-based executable that we have called hello-mpi (see MPI jobs ). Continuing the example above, suppose that this script is saved in the current directory into a file called example.sbatch . This script is submitted to the cluster using the following command: $ sbatch ./example.sbatch Many other options are available for submitting jobs using the sbatch command. For more specialized computational needs, see Running jobs on midway . Additionally, for a complete list of the available options, see the Official SBATCH Documentation .","title":"Batch Jobs\u00b6"},{"location":"dali/docs/running-jobs/#managing-jobs","text":"The Slurm job scheduler provides several command-line tools for checking on the status of your jobs and for managing them. For a complete list of Slurm commands, see the Slurm man pages . Here are a few commands that you may find particularly useful: squeue : finds out the status of jobs submitted by you and other users. sacct : retrieves job history and statistics about past jobs. scancel : cancels jobs you have submitted. In the next couple sections we explain how to use squeue to find out the status of your submitted jobs, and scancel to cancel jobs in the queue.","title":"Managing Jobs"},{"location":"dali/docs/running-jobs/#checking-your-jobs","text":"Use the squeue command to check on the status of your jobs, and other jobs running on Midway. The simplest invocation lists all jobs that are currently running or waiting in the job queue (\u201cpending\u201d), along with details about each job such as the job id and the number of nodes requested: $ squeue Any job with 0:00 under the TIME column is a job that is still waiting in the queue. To view only the jobs that you have submitted, use the --user flag $ squeue --user=$USER This command has many other useful options for querying the status of the queue and getting information about individual jobs. For example, to get information about all jobs that are waiting to run on DaLi partition, enter: $ squeue --state=PENDING --partition=dali Alternatively, to get information about all your jobs that are running on the DaLi partition, type: $ squeue --state=RUNNING --partition=dali --user=$USER The last column of the output tells us which nodes are allocated for each job. For example, if it shows dali-012 for one of the jobs under your name, you may type ssh DaLI-012 to log in to that compute node and inspect the progress of your computation locally. For more information, consult the command-line help by typing squeue --help , or visit the official online documentation.","title":"Checking your jobs"},{"location":"dali/docs/running-jobs/#canceling-your-jobs","text":"To cancel a job you have submitted, use the scancel command. This requires you to specify the id of the job you wish to cancel. For example, to cancel a job with id 8885128 , do the following: $ scancel 8885128 If you are unsure what is the id of the job you would like to cancel, see the JOBID column from running squeue --user=$USER . To cancel all jobs you have submitted that are either running or waiting in the queue, enter the following: $ scancel --user=$USER","title":"Canceling your jobs"},{"location":"dali/docs/running-jobs/#job-limits","text":"To distribute computational resources fairly to all Midway users even among DaLi users, the RCC sets limits on the amount of computing resources that may be requested by a single user at any given time. On DaLi, the maximum run-time for an individual job is 36 hours. The maximum number of jobs per user is 200 and the maximum number of jobs that each user can submit is 500. These apply to all batch and interactive jobs submitted to DaLi compute nodes.","title":"Job Limits"},{"location":"dali/docs/running-jobs/#temporary-file-storage","text":"Many applications generate temporary or intermediate files that are written to /tmp . (These applications may write files to /tmp even without you being aware that this is happening.) This folder is typically on a local drive or the RAM disk that virtualized in the system memory. Contents in /tmp left by a user\u2019s job won\u2019t be automatically purged before rebooting the corresponding node, and therefore may affect other jobs later running on the same node. Therefore, RCC enforces a data purge policy for files written to /tmp on compute nodes: For each running job, a special \u201cjob-protected\u201d folder /tmp/jobs/${SLURM_JOB_ID} is created on each allocated node. Its contents are safely purged only upon termination of the job (when it is sucessfully completed, canceled or killed). For any running jobs, environment variables SLURM_TMPDIR and TMPDIR are set to /tmp/jobs/${SLURM_JOB_ID} . Whenever possible, users should write to the paths specified by these environment variables rather than using /tmp explicitly. (Most applications should already be using these environment variables by default, so in many cases this will not require any change to your code.) In addition to using $TMPDIR , users should also verify that no additional files are being written to /tmp . Note that upon termination of a job, any folders or files directly under /tmp that belong to the submitter of this job will be purged. The contents of /tmp do not persist after jobs terminate. The RCC is not responsible for retrieving or recovering data stored there. For critical outputs, please save them to the persisent file storage systems. Note: Folders or files created by users in /tmp outside $TMPDIR are NOT job-protected. For example, consider the case when user has two running jobs (A and B) on the same node, and job B is directly writing files in /tmp . If job A terminates before job B, the contents of /tmp will be also purged, and in some cases may cause job B to fail. To avoid failure, users should therefore write any temporary data to the job-protected folder, SLURM_TMPDIR or TMPDIR .","title":"Temporary File Storage"},{"location":"midway23/midway_connecting/","text":"Connecting to Midway The information here describes how users can connect to Midway to access RCC resources. All users are responsible for knowing and abiding by the RCC User Policy . Account Credentials To connect to Midway you must have a RCC user account ( request an account ). Your RCC account uses your UChicago CNetID for the username and the corresponding CNetID password for the password: Username: CNetID Password: CNet password Login Nodes When we say \"connect to Midway,\" what we're really saying is connect to one of Midway's login nodes . The login nodes are physical parts of the Midway cluster that are connected to the internet and serve as the \"foyer\" to the system. You connect to the login nodes to manage data, download and install packages, and submit jobs to the compute nodes, as the diagram below depicts. Upon logging in to Midway, you will automatically be connected to either one of two login nodes on the respective system: Midway2: midway2-login1.rcc.uchicago.edu or midway2-login2.rcc.uchicago.edu Midway3: midway3-login1.rcc.uchicago.edu or midway3-login2.rcc.uchicago.edu NOTE : The login nodes are NOT for computionally intensive work. For running computationally intensive programs, see Running jobs on Midway . Summary of Connection Methods There are two main ways to connect to Midway, detailed below. This table provides a high level summary of the two: Connection Method Description Secure Shell (SSH) Command-line (Terminal or Powershell) access to the cluster. Good for users with command-line programming experince, and is typically the most stable. ThinLinc Provides a GUI (graphical user interface), and thus is more 'user friendly' and minimizes need for command-line interaction. Typically less stable than SSH. Connecting with SSH Secure Shell (SSH) is a protocol that provides secure command-line access to remote resources such as Midway.* To log in to Midway from a Linux or Mac computer, open a terminal. To log in to Midway from a Windows computer, open powershell .** At the command line enter: Midway2 Midway3 ssh <CNetID>@midway2.rcc.uchicago.edu ssh <CNetID>@midway3.rcc.uchicago.edu Provide your CNetID password when prompted. Duo two-factor autentication will request you select from the available 2FA options to authenticate to Midway. Duo two-factor authentication for user Enter a passcode or select one of the following options: 1) receive a push code on your Duo app, 2) Receive authentication through your phone number, and 3) get an SMS code. Passcode or option (1-3): Choose from the available two-factor authentication options and finish the authentication process. X11 Forwarding X11 forwarding is a mechanism that allows you to forward a remote application's display to your local machine. To enable X11 forwarding when connecting to a Midway system with SSH, the -Y flag should be included: Midway2 Midway3 ssh -Y <CNetID>@midway2.rcc.uchicago.edu ssh -Y <CNetID>@midway3.rcc.uchicago.edu NOTE : XQuartz is required to enable trusted X11 forwarding on a Mac. Connecting with ThinLinc ThinLinc is a remote desktop server used to connect to Midway and obtain a remote graphical user interface (GUI). We recommend using ThinLinc to use software that requires a GUI. ThinLinc Web Browser Point your web browser to the following web address: Midway2 Midway3 https://midway2.rcc.uchicago.edu. You will land on this page: https://midway3.rcc.uchicago.edu. You will land on this page: Proceed to log in with your CNetID and password. Duo two-factor autentication will request you select from the available 2FA options to authenticate to Midway. Duo two-factor authentication for user Enter a passcode or select one of the following options: 1) receive a push code on your Duo app, 2) Receive authentication through your phone number, and 3) get an SMS code. Passcode or option (1-3): ThinLinc Desktop Client Download and install the appropriate ThinLinc client here: https://www.cendio.com/thinlinc/download Open the ThinLinc client and use the following information to set up your connection to Midway: Midway2 Midway3 Server: midway2.rcc.uchicago.edu Username: CNetID Password: CNet password Your client should look similar to this: Server: midway3.rcc.uchicago.edu Username: CNetID Password: CNet password Your client should look similar to this: ThinLinc will default to open in a fullscreen window that fills all monitors . To change this use Options from the initial login interface. After clicking the Connect button, Duo two-factor autentication will request you select from the available 2FA options to authenticate to Midway. Duo two-factor authentication for user Enter a passcode or select one of the following options: 1) receive a push code on your Duo app, 2) Receive authentication through your phone number, and 3) get an SMS code. Passcode or option (1-3): The ThinLinc Interface Upon successfully logging in, you will be presented with an IceWM desktop. Select Applications tab in the top left corner to access the terminal, file browser, and other utilities. To copy/paste between Thinlinc webaccess client and your computer, open the side toolbar by clicking the purple handle. Click the Clipboard icon. The text field that just open will be synced with the clipboard on the server, so you can copy and paste to and from this text field. With ThinLinc it is possible to maintain an active session after you have closed your connection to Midway. To disconnect from Midway but maintain an active session, simply close the ThinLinc window. NOTE: You must have \"End existing session\" unchecked for this to occur. To exit ThinLinc and terminate your session completely, simply exit or close the ThinLinc application. Remote Visualization on Midway2 RCC provides a mechanism for accessing a GPU-equipped visualization node, which can be used for running 3D and graphics-intensive visualization software packages. First log into Midway via ThinLinc. Once logged in, open a terminal and in the terminal window, issue the command sviz To exit the Visualization node, simply close the terminal window from which it was launched. You can then log out of Midway by selecting Logout from the Applications menu in ThinLinc, or by simply closing the ThinLinc window. ** Windows users running a version of Windows older than Windows 10\u2019s April 2018 release will have to download an ssh client to connect via ssh. We recommend the MobaXterm, client, although other options are available. * SSH key-based authentication is no longer supported. The SSH password-based authentication is currently the only supported method for authentication.","title":"Connecting to Midway"},{"location":"midway23/midway_connecting/#connecting-to-midway","text":"The information here describes how users can connect to Midway to access RCC resources. All users are responsible for knowing and abiding by the RCC User Policy .","title":"Connecting to Midway"},{"location":"midway23/midway_connecting/#account-credentials","text":"To connect to Midway you must have a RCC user account ( request an account ). Your RCC account uses your UChicago CNetID for the username and the corresponding CNetID password for the password: Username: CNetID Password: CNet password","title":"Account Credentials"},{"location":"midway23/midway_connecting/#login-nodes","text":"When we say \"connect to Midway,\" what we're really saying is connect to one of Midway's login nodes . The login nodes are physical parts of the Midway cluster that are connected to the internet and serve as the \"foyer\" to the system. You connect to the login nodes to manage data, download and install packages, and submit jobs to the compute nodes, as the diagram below depicts. Upon logging in to Midway, you will automatically be connected to either one of two login nodes on the respective system: Midway2: midway2-login1.rcc.uchicago.edu or midway2-login2.rcc.uchicago.edu Midway3: midway3-login1.rcc.uchicago.edu or midway3-login2.rcc.uchicago.edu NOTE : The login nodes are NOT for computionally intensive work. For running computationally intensive programs, see Running jobs on Midway .","title":"Login Nodes"},{"location":"midway23/midway_connecting/#summary-of-connection-methods","text":"There are two main ways to connect to Midway, detailed below. This table provides a high level summary of the two: Connection Method Description Secure Shell (SSH) Command-line (Terminal or Powershell) access to the cluster. Good for users with command-line programming experince, and is typically the most stable. ThinLinc Provides a GUI (graphical user interface), and thus is more 'user friendly' and minimizes need for command-line interaction. Typically less stable than SSH.","title":"Summary of Connection Methods"},{"location":"midway23/midway_connecting/#connecting-with-ssh","text":"Secure Shell (SSH) is a protocol that provides secure command-line access to remote resources such as Midway.* To log in to Midway from a Linux or Mac computer, open a terminal. To log in to Midway from a Windows computer, open powershell .** At the command line enter: Midway2 Midway3 ssh <CNetID>@midway2.rcc.uchicago.edu ssh <CNetID>@midway3.rcc.uchicago.edu Provide your CNetID password when prompted. Duo two-factor autentication will request you select from the available 2FA options to authenticate to Midway. Duo two-factor authentication for user Enter a passcode or select one of the following options: 1) receive a push code on your Duo app, 2) Receive authentication through your phone number, and 3) get an SMS code. Passcode or option (1-3): Choose from the available two-factor authentication options and finish the authentication process.","title":"Connecting with SSH"},{"location":"midway23/midway_connecting/#x11-forwarding","text":"X11 forwarding is a mechanism that allows you to forward a remote application's display to your local machine. To enable X11 forwarding when connecting to a Midway system with SSH, the -Y flag should be included: Midway2 Midway3 ssh -Y <CNetID>@midway2.rcc.uchicago.edu ssh -Y <CNetID>@midway3.rcc.uchicago.edu NOTE : XQuartz is required to enable trusted X11 forwarding on a Mac.","title":"X11 Forwarding"},{"location":"midway23/midway_connecting/#connecting-with-thinlinc","text":"ThinLinc is a remote desktop server used to connect to Midway and obtain a remote graphical user interface (GUI). We recommend using ThinLinc to use software that requires a GUI.","title":"Connecting with ThinLinc"},{"location":"midway23/midway_connecting/#thinlinc-web-browser","text":"Point your web browser to the following web address: Midway2 Midway3 https://midway2.rcc.uchicago.edu. You will land on this page: https://midway3.rcc.uchicago.edu. You will land on this page: Proceed to log in with your CNetID and password. Duo two-factor autentication will request you select from the available 2FA options to authenticate to Midway. Duo two-factor authentication for user Enter a passcode or select one of the following options: 1) receive a push code on your Duo app, 2) Receive authentication through your phone number, and 3) get an SMS code. Passcode or option (1-3):","title":"ThinLinc Web Browser"},{"location":"midway23/midway_connecting/#thinlinc-desktop-client","text":"Download and install the appropriate ThinLinc client here: https://www.cendio.com/thinlinc/download Open the ThinLinc client and use the following information to set up your connection to Midway: Midway2 Midway3 Server: midway2.rcc.uchicago.edu Username: CNetID Password: CNet password Your client should look similar to this: Server: midway3.rcc.uchicago.edu Username: CNetID Password: CNet password Your client should look similar to this: ThinLinc will default to open in a fullscreen window that fills all monitors . To change this use Options from the initial login interface. After clicking the Connect button, Duo two-factor autentication will request you select from the available 2FA options to authenticate to Midway. Duo two-factor authentication for user Enter a passcode or select one of the following options: 1) receive a push code on your Duo app, 2) Receive authentication through your phone number, and 3) get an SMS code. Passcode or option (1-3):","title":"ThinLinc Desktop Client"},{"location":"midway23/midway_connecting/#the-thinlinc-interface","text":"Upon successfully logging in, you will be presented with an IceWM desktop. Select Applications tab in the top left corner to access the terminal, file browser, and other utilities. To copy/paste between Thinlinc webaccess client and your computer, open the side toolbar by clicking the purple handle. Click the Clipboard icon. The text field that just open will be synced with the clipboard on the server, so you can copy and paste to and from this text field. With ThinLinc it is possible to maintain an active session after you have closed your connection to Midway. To disconnect from Midway but maintain an active session, simply close the ThinLinc window. NOTE: You must have \"End existing session\" unchecked for this to occur. To exit ThinLinc and terminate your session completely, simply exit or close the ThinLinc application.","title":"The ThinLinc Interface"},{"location":"midway23/midway_connecting/#remote-visualization-on-midway2","text":"RCC provides a mechanism for accessing a GPU-equipped visualization node, which can be used for running 3D and graphics-intensive visualization software packages. First log into Midway via ThinLinc. Once logged in, open a terminal and in the terminal window, issue the command sviz To exit the Visualization node, simply close the terminal window from which it was launched. You can then log out of Midway by selecting Logout from the Applications menu in ThinLinc, or by simply closing the ThinLinc window. ** Windows users running a version of Windows older than Windows 10\u2019s April 2018 release will have to download an ssh client to connect via ssh. We recommend the MobaXterm, client, although other options are available. * SSH key-based authentication is no longer supported. The SSH password-based authentication is currently the only supported method for authentication.","title":"Remote Visualization on Midway2"},{"location":"midway23/midway_data_storage/","text":"Data Storage RCC provides a high-performance GPFS shared file system that houses users\u2019 home directories, shared project spaces, and high-throughput scratch space. Summary of Data Storage on Midway The following table and chart provides a summary of the multiple data storage locations on Midway. Read on for more details about each location. Midway2 Midway3 Name Location Soft Quota Suitable For Home /home/<CNetID> 30 GB Core personal scripts, files, environments Project2 /project2/<PI CNetID> variable Shared datasets, scripts, environments Scratch /scratch/midway2/<CNetID> 100 GB Output of jobs, intermediate datasets Name Location Soft Quota Suitable For Home /home/<CNetID> 30 GB Core personal scripts, files, environments Project /project/<PI CNetID> variable Shared datasets, scripts, environments Scratch /scratch/midway3/<CNetID> 100 GB Output of jobs, intermediate datasets Quotas The amount of data that can be stored in home directories, project directories, and shared scratch directories is controlled by quota. RCC enforces hard and soft limits on quotas. A soft quota can be exceeded for a short period of time called a grace period. The hard quota cannot be exceeded under any circumstances. Purchasing More Storage Additional storage is available through the Cluster Partnership Program , a Research I Allocation , Research II Allocation or, in certain circumstances, a Special Allocation . Checking available storage To check your current quotas use rcchelp quota . Typical output may look like this --------------------------------------------------------------------------- fileset type used quota limit grace ---------------- ---------------- ---------- ---------- ---------- -------- home blocks (user) 8.77G 30.00G 35.00G none files (user) 157865 300000 1000000 none scratch blocks (user) 16.07G 100.00G 5.00T none files (user) 193028 10000000 20000000 none ---------------- ---------------- ---------- ---------- ---------- -------- >>> Capacity Filesystem: project2 (GPFS) ---------------- ---------------- ---------- ---------- ---------- -------- rcc blocks (group) 259.10T 500.00T 501.00T none files (group) 45825436 384500000 385500000 none ---------------- ---------------- ---------- ---------- ---------- -------- --------------------------------------------------------------------------- The following table describes the fields: Field Meaning fileset File set or file system where this quota is valid. type Type of quota. Blocks are the amount of consumed disk space. Files are the number of files in a directory. Blocks or files quotas can be set at the user or group level. used The amount of disk space consumed or the number of files in the specified location. quota The soft quota (disk space or file count) associated with the specified location. It is possible for usage to exceed the soft quota for the grace period or up to the hard limit. limit The hard quota (disk space or file count) associated with the specified location. When your usage exceeds this limit, you will NOT be able to write to that filesystem. grace The amount of time remaining that the soft quota can be exceeded. None means that the quota is not exceeded. After a soft quota has been exceeded for longer than the grace period, it will no longer be possible to create new files. Persistent Space Persistent spaces are where data go for medium- to long-term storage. The two persistent storage locations on Midway are the home and project directories. Both directories have frequent file system snapshots and tape backup for data protection. Home Directories Every RCC user has a home directory located at /home/<CNetID> . The HOME environment variable points to this location. The home directory is accessible from all RCC compute systems and is generally used for storing frequently used items such as source code, binaries, and scripts. By default, a home directory is only accessible by its owner (mode 0700 ) and is suitable for storing files which do not need to be shared with others. Project Directories All RCC PI Groups are allocated a Project Directory located at /project/<PI CNetID> or /project2/<PI CNetID> where is the CNetID of your RCC PI account holder. These directories are accessible by all members of the PI Group and are generally used for storing files which need to be shared by members of the group. The default permissions for files and directories created in a project directory allow group read/write with the group sticky bit set (mode 2770 ). The group ownership is set to the PI group. Scratch Space Shared Scratch Space High performance shared scratch space can be accessed using the SCRATCH environment variable. This scratch space is intended to be used for reading or writing data required by jobs running on the cluster. If a user is over quota, they can use scratch space as a temporary location to hold files (and/or compress them for archival purposes) but as scratch space is neither snapshotted nor backed up, it should always be viewed as temporary. NOTE : It is the responsibility of the user to ensure any important data in scratch space is moved to persistent storage. Scratch space is meant to be used for temporary, short-term storage only. The default permissions for scratch space allow access only by its owner (mode 0700 ). The standard quota for the high performance scratch directory is 5 TB with a 100GB soft limit. The grace period that the soft limit may be exceeded is 30 days for shared scratch space. Cost-Effective Data Storage In addition to high-performance GPFS file system, RCC also offers Cost-effective Data Storage (CDS) through Cluster Partnership Program for long-term data storage. CDS is only available from login nodes and is meant to be used as a storage for less frequently accessed data. Before performing any computation on the data stored on CDS, it first needs to be copied to the GPFS file system. Data Recovery and Backups Snapshots Automated snapshots of home and project directories are available in case of accidental file deletion or other problems. Currently snapshots are available for these time periods: Directory Snapshot kept Snapshot Path $HOME 7 daily and 2 weekly /snapshots/home/SNAPSHOT/home/CNetID /project/<any_folder> 7 daily and 2 weekly /snapshots/project/SNAPSHOT/project/<any_folder> /project2/<any_folder> 7 daily and 4 weekly /snapshots/project2/SNAPSHOT/project2/<any_folder> The snapshots for the home and project directories are available from the login nodes. The {SNAPSHOT} refers to the time of the backup, e.g. daily-YYYY-MM-DD.05h30 or weekly-YYYY-MM-DD.05h30. To view the available snapshots of the home directory, for example, use the command ls /snapshots/home To restore a file from a snapshot, simply copy the file to where you want it with either cp or rsync .","title":"Data Storage"},{"location":"midway23/midway_data_storage/#data-storage","text":"RCC provides a high-performance GPFS shared file system that houses users\u2019 home directories, shared project spaces, and high-throughput scratch space.","title":"Data Storage"},{"location":"midway23/midway_data_storage/#summary-of-data-storage-on-midway","text":"The following table and chart provides a summary of the multiple data storage locations on Midway. Read on for more details about each location. Midway2 Midway3 Name Location Soft Quota Suitable For Home /home/<CNetID> 30 GB Core personal scripts, files, environments Project2 /project2/<PI CNetID> variable Shared datasets, scripts, environments Scratch /scratch/midway2/<CNetID> 100 GB Output of jobs, intermediate datasets Name Location Soft Quota Suitable For Home /home/<CNetID> 30 GB Core personal scripts, files, environments Project /project/<PI CNetID> variable Shared datasets, scripts, environments Scratch /scratch/midway3/<CNetID> 100 GB Output of jobs, intermediate datasets","title":"Summary of Data Storage on Midway"},{"location":"midway23/midway_data_storage/#quotas","text":"The amount of data that can be stored in home directories, project directories, and shared scratch directories is controlled by quota. RCC enforces hard and soft limits on quotas. A soft quota can be exceeded for a short period of time called a grace period. The hard quota cannot be exceeded under any circumstances.","title":"Quotas"},{"location":"midway23/midway_data_storage/#purchasing-more-storage","text":"Additional storage is available through the Cluster Partnership Program , a Research I Allocation , Research II Allocation or, in certain circumstances, a Special Allocation .","title":"Purchasing More Storage"},{"location":"midway23/midway_data_storage/#checking-available-storage","text":"To check your current quotas use rcchelp quota . Typical output may look like this --------------------------------------------------------------------------- fileset type used quota limit grace ---------------- ---------------- ---------- ---------- ---------- -------- home blocks (user) 8.77G 30.00G 35.00G none files (user) 157865 300000 1000000 none scratch blocks (user) 16.07G 100.00G 5.00T none files (user) 193028 10000000 20000000 none ---------------- ---------------- ---------- ---------- ---------- -------- >>> Capacity Filesystem: project2 (GPFS) ---------------- ---------------- ---------- ---------- ---------- -------- rcc blocks (group) 259.10T 500.00T 501.00T none files (group) 45825436 384500000 385500000 none ---------------- ---------------- ---------- ---------- ---------- -------- --------------------------------------------------------------------------- The following table describes the fields: Field Meaning fileset File set or file system where this quota is valid. type Type of quota. Blocks are the amount of consumed disk space. Files are the number of files in a directory. Blocks or files quotas can be set at the user or group level. used The amount of disk space consumed or the number of files in the specified location. quota The soft quota (disk space or file count) associated with the specified location. It is possible for usage to exceed the soft quota for the grace period or up to the hard limit. limit The hard quota (disk space or file count) associated with the specified location. When your usage exceeds this limit, you will NOT be able to write to that filesystem. grace The amount of time remaining that the soft quota can be exceeded. None means that the quota is not exceeded. After a soft quota has been exceeded for longer than the grace period, it will no longer be possible to create new files.","title":"Checking available storage"},{"location":"midway23/midway_data_storage/#persistent-space","text":"Persistent spaces are where data go for medium- to long-term storage. The two persistent storage locations on Midway are the home and project directories. Both directories have frequent file system snapshots and tape backup for data protection.","title":"Persistent Space"},{"location":"midway23/midway_data_storage/#home-directories","text":"Every RCC user has a home directory located at /home/<CNetID> . The HOME environment variable points to this location. The home directory is accessible from all RCC compute systems and is generally used for storing frequently used items such as source code, binaries, and scripts. By default, a home directory is only accessible by its owner (mode 0700 ) and is suitable for storing files which do not need to be shared with others.","title":"Home Directories"},{"location":"midway23/midway_data_storage/#project-directories","text":"All RCC PI Groups are allocated a Project Directory located at /project/<PI CNetID> or /project2/<PI CNetID> where is the CNetID of your RCC PI account holder. These directories are accessible by all members of the PI Group and are generally used for storing files which need to be shared by members of the group. The default permissions for files and directories created in a project directory allow group read/write with the group sticky bit set (mode 2770 ). The group ownership is set to the PI group.","title":"Project Directories"},{"location":"midway23/midway_data_storage/#scratch-space","text":"","title":"Scratch Space"},{"location":"midway23/midway_data_storage/#shared-scratch-space","text":"High performance shared scratch space can be accessed using the SCRATCH environment variable. This scratch space is intended to be used for reading or writing data required by jobs running on the cluster. If a user is over quota, they can use scratch space as a temporary location to hold files (and/or compress them for archival purposes) but as scratch space is neither snapshotted nor backed up, it should always be viewed as temporary. NOTE : It is the responsibility of the user to ensure any important data in scratch space is moved to persistent storage. Scratch space is meant to be used for temporary, short-term storage only. The default permissions for scratch space allow access only by its owner (mode 0700 ). The standard quota for the high performance scratch directory is 5 TB with a 100GB soft limit. The grace period that the soft limit may be exceeded is 30 days for shared scratch space.","title":"Shared Scratch Space"},{"location":"midway23/midway_data_storage/#cost-effective-data-storage","text":"In addition to high-performance GPFS file system, RCC also offers Cost-effective Data Storage (CDS) through Cluster Partnership Program for long-term data storage. CDS is only available from login nodes and is meant to be used as a storage for less frequently accessed data. Before performing any computation on the data stored on CDS, it first needs to be copied to the GPFS file system.","title":"Cost-Effective Data Storage"},{"location":"midway23/midway_data_storage/#data-recovery-and-backups","text":"","title":"Data Recovery and Backups"},{"location":"midway23/midway_data_storage/#snapshots","text":"Automated snapshots of home and project directories are available in case of accidental file deletion or other problems. Currently snapshots are available for these time periods: Directory Snapshot kept Snapshot Path $HOME 7 daily and 2 weekly /snapshots/home/SNAPSHOT/home/CNetID /project/<any_folder> 7 daily and 2 weekly /snapshots/project/SNAPSHOT/project/<any_folder> /project2/<any_folder> 7 daily and 4 weekly /snapshots/project2/SNAPSHOT/project2/<any_folder> The snapshots for the home and project directories are available from the login nodes. The {SNAPSHOT} refers to the time of the backup, e.g. daily-YYYY-MM-DD.05h30 or weekly-YYYY-MM-DD.05h30. To view the available snapshots of the home directory, for example, use the command ls /snapshots/home To restore a file from a snapshot, simply copy the file to where you want it with either cp or rsync .","title":"Snapshots"},{"location":"midway23/midway_data_transfer/","text":"Transferring Data to Midway This page provides information on how to transfer data to Midway from your local computer (and vice versa). The following table summarizes available data transfer methods and what tasks they are suited for: Transfer Method Suitable For Not Suitable For Secure Copy (SCP) Transferring a few files less than a few GB. Terminal users. Transferring large files or numbers of files SAMBA Transferring a few files less than a few GB. Desktop GUI users. Transferring large files or numbers of files Globus Transferring large files, datasets, and multiple directories. Quick transfer of few files HTTP Sharing data publically via web with collaborators. Sharing data with large number of users. Transferring data to Midway via HTTP is not possible. Secure Copy (SCP) Mac and Linux systems provide a scp command which can be accessed from the command line. To transfer files from your local computer to your home directory (see Data Storage for information on directories), open a terminal window and issue the command: For single files: Midway2 Midway3 scp <some file> <CNetID>@midway2.rcc.uchicago.edu: scp <some file> <CNetID>@midway3.rcc.uchicago.edu: For directories: Midway2 Midway3 scp -r <some dir> <CNetID>@midway2.rcc.uchicago.edu: scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu: To transfer to a directory other than your home directory (for example, project): Midway2 Midway3 scp -r <some dir> <CNetID>@midway2.rcc.uchicago.edu:/project2 scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu:/project When prompted, enter your CNet password. SAMBA SAMBA allows one to connect to (or \u201cmount\u201d) their home and project directories on their local computer. This method of accessing your RCC home and project space is only available from within the UChicago campus network. From off-campus you will need to first connect through the UChicago VPN. Connecting from Windows On a Windows computer, select \u201cMap Network Drive\u201d and enter one of the following UNC paths depending on which location on Midway you wish to connect to: Midway2 Midway3 Home Project2 Scratch \\\\midwaysmb.rcc.uchicago.edu\\homes \\\\midwaysmb.rcc.uchicago.edu\\project2 \\\\midwaysmb.rcc.uchicago.edu\\midway2-scratch Home Project Scratch \\\\midway3smb.rcc.uchicago.edu\\homes \\\\midway3smb.rcc.uchicago.edu\\project \\\\midway3smb.rcc.uchicago.edu\\midway3-scratch Enter ADLOCAL\\CNetID for the username and enter your CNet password. Connecting from Mac OS X On a Mac OS X computer, select \u201cConnect to Server\u201d (from \"Go\" dropdown in Finder) and enter one of the following URLs depending on which location on Midway you wish to connect to: Midway2 Midway3 Home Project2 Scratch smb://midwaysmb.rcc.uchicago.edu/homes smb://midwaysmb.rcc.uchicago.edu/project2 smb://midwaysmb.rcc.uchicago.edu/midway2-scratch Home Project Scratch smb://midway3smb.rcc.uchicago.edu/homes smb://midway3smb.rcc.uchicago.edu/project smb://midway3smb.rcc.uchicago.edu/midway3-scratch Enter ADLOCAL\\CNetID for the username and enter your CNet password. Globus Online Globus Online is a robust tool for transferring large data files to/from Midway. RCC has a customized Globus Online login site. Go to globus.rcc.uchicago.edu and Select \u201cUniversity of Chicago\u201d for the existing organizational login: Enter your CNetID and password when prompted You will need to link your University of Chicago credentials to a Globus Online account. Either create a new Globus Online account or sign in to your existing account if you have one. Once you are signed in, select the \"File Manager\" tab on the sidebar, then enter \"ucrcc#midway\". You can select \"UChicago RCC Midway\" to access your Midway2 files or \"UChicago RCC Midway3\" to access your Midway3 files. You will then be able to perform actions such as transfer files, share collections, or create new directories. To learn more about how to use these tools, please refer to the \"Help\" tab on the left toolbar. There is extensive documentation on the Globus Online site as to how to transfer files in different modes. Please refer to their documentation for more details or contact us with any RCC specific issues. HTTP RCC provides web access to data on their storage system via public_html directories in users\u2019 home directories. Local path Corresponding URL /home/[your_CNetID]/public_html/research.dat http://users.rcc.uchicago.edu/~[your_CNetID]/research.dat Ensure your home directories and public_html have the execute permissions. Optionally, ensure public_html has read permissions if you would like to allow indexing. You may set these permissions using the following commands: chmod o+x $HOME mkdir -p $HOME/public_html chmod o+x $HOME/public_html // optional; if you would like to allow directory listing. chmod o+r $HOME/public_html Files in public_html must also be readable by the web user, \"other\", but should not be made executable. You may set read permissions for web users/\"other\" using the following command: chmod o+r $HOME/public_html/research.dat NOTE : Use of these directories must conform with the RCC usage policy . Please notify RCC if you expect a large number of people to access data hosted here.","title":"Data Transfer"},{"location":"midway23/midway_data_transfer/#transferring-data-to-midway","text":"This page provides information on how to transfer data to Midway from your local computer (and vice versa). The following table summarizes available data transfer methods and what tasks they are suited for: Transfer Method Suitable For Not Suitable For Secure Copy (SCP) Transferring a few files less than a few GB. Terminal users. Transferring large files or numbers of files SAMBA Transferring a few files less than a few GB. Desktop GUI users. Transferring large files or numbers of files Globus Transferring large files, datasets, and multiple directories. Quick transfer of few files HTTP Sharing data publically via web with collaborators. Sharing data with large number of users. Transferring data to Midway via HTTP is not possible.","title":"Transferring Data to Midway"},{"location":"midway23/midway_data_transfer/#secure-copy-scp","text":"Mac and Linux systems provide a scp command which can be accessed from the command line. To transfer files from your local computer to your home directory (see Data Storage for information on directories), open a terminal window and issue the command: For single files: Midway2 Midway3 scp <some file> <CNetID>@midway2.rcc.uchicago.edu: scp <some file> <CNetID>@midway3.rcc.uchicago.edu: For directories: Midway2 Midway3 scp -r <some dir> <CNetID>@midway2.rcc.uchicago.edu: scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu: To transfer to a directory other than your home directory (for example, project): Midway2 Midway3 scp -r <some dir> <CNetID>@midway2.rcc.uchicago.edu:/project2 scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu:/project When prompted, enter your CNet password.","title":"Secure Copy (SCP)"},{"location":"midway23/midway_data_transfer/#samba","text":"SAMBA allows one to connect to (or \u201cmount\u201d) their home and project directories on their local computer. This method of accessing your RCC home and project space is only available from within the UChicago campus network. From off-campus you will need to first connect through the UChicago VPN. Connecting from Windows On a Windows computer, select \u201cMap Network Drive\u201d and enter one of the following UNC paths depending on which location on Midway you wish to connect to: Midway2 Midway3 Home Project2 Scratch \\\\midwaysmb.rcc.uchicago.edu\\homes \\\\midwaysmb.rcc.uchicago.edu\\project2 \\\\midwaysmb.rcc.uchicago.edu\\midway2-scratch Home Project Scratch \\\\midway3smb.rcc.uchicago.edu\\homes \\\\midway3smb.rcc.uchicago.edu\\project \\\\midway3smb.rcc.uchicago.edu\\midway3-scratch Enter ADLOCAL\\CNetID for the username and enter your CNet password. Connecting from Mac OS X On a Mac OS X computer, select \u201cConnect to Server\u201d (from \"Go\" dropdown in Finder) and enter one of the following URLs depending on which location on Midway you wish to connect to: Midway2 Midway3 Home Project2 Scratch smb://midwaysmb.rcc.uchicago.edu/homes smb://midwaysmb.rcc.uchicago.edu/project2 smb://midwaysmb.rcc.uchicago.edu/midway2-scratch Home Project Scratch smb://midway3smb.rcc.uchicago.edu/homes smb://midway3smb.rcc.uchicago.edu/project smb://midway3smb.rcc.uchicago.edu/midway3-scratch Enter ADLOCAL\\CNetID for the username and enter your CNet password.","title":"SAMBA"},{"location":"midway23/midway_data_transfer/#globus-online","text":"Globus Online is a robust tool for transferring large data files to/from Midway. RCC has a customized Globus Online login site. Go to globus.rcc.uchicago.edu and Select \u201cUniversity of Chicago\u201d for the existing organizational login: Enter your CNetID and password when prompted You will need to link your University of Chicago credentials to a Globus Online account. Either create a new Globus Online account or sign in to your existing account if you have one. Once you are signed in, select the \"File Manager\" tab on the sidebar, then enter \"ucrcc#midway\". You can select \"UChicago RCC Midway\" to access your Midway2 files or \"UChicago RCC Midway3\" to access your Midway3 files. You will then be able to perform actions such as transfer files, share collections, or create new directories. To learn more about how to use these tools, please refer to the \"Help\" tab on the left toolbar. There is extensive documentation on the Globus Online site as to how to transfer files in different modes. Please refer to their documentation for more details or contact us with any RCC specific issues.","title":"Globus Online"},{"location":"midway23/midway_data_transfer/#http","text":"RCC provides web access to data on their storage system via public_html directories in users\u2019 home directories. Local path Corresponding URL /home/[your_CNetID]/public_html/research.dat http://users.rcc.uchicago.edu/~[your_CNetID]/research.dat Ensure your home directories and public_html have the execute permissions. Optionally, ensure public_html has read permissions if you would like to allow indexing. You may set these permissions using the following commands: chmod o+x $HOME mkdir -p $HOME/public_html chmod o+x $HOME/public_html // optional; if you would like to allow directory listing. chmod o+r $HOME/public_html Files in public_html must also be readable by the web user, \"other\", but should not be made executable. You may set read permissions for web users/\"other\" using the following command: chmod o+r $HOME/public_html/research.dat NOTE : Use of these directories must conform with the RCC usage policy . Please notify RCC if you expect a large number of people to access data hosted here.","title":"HTTP"},{"location":"midway23/midway_file_permissions/","text":"File System Permissions Linux divides file permissions into read, write and execute (denoted by r, w, and x) for three user types: Owner, Group, and Other. Here is how you interpret this in the command line: To represent a given file's permissions for the three user types, combinations of the letters are commonly represented by numbers (i.e., 0700). Let\u2019s first summarize the default file system permission on Midway: Directory Permissions $HOME 0700 \u2013 Accessible only to the owner $SCRATCH 0700 \u2013 Accessible only to the owner /project/<PI CNetID> 2770 \u2013 Read/write for the project group /project2/<PI CNetID> 2770 \u2013 Read/write for the project group The default umask is 002 . When new files or directories are created, the umask influences the default permissions of those files and directories. With the umask set to 002 all files and directories will be group readable and writable by default. In your home directory, the group ownership will be set to your personal group, which is the same as your CNetID, so you will still be the only user that can access your files and directories. In the project directories, the group sticky bit causes the group ownership to be the same as the directory. This means files created in a project directory will be readable and writable by the project group, which is typically what is wanted in those directories. Here is an example of what this means in practice: $ ls -ld $HOME /project/rcc drwx------ 108 wettstein wettstein 32768 2013-01-15 10:51 /home/wettstein drwxrws--- 24 root rcc-staff 32768 2013-01-15 10:48 /project/rcc $ touch $HOME/newfile /project/rcc/newfile $ ls -l /project/rcc/newfile $HOME/newfile -rw-rw-r-- 1 wettstein wettstein 0 2013-01-15 10:48 /home/wettstein/newfile -rw-rw-r-- 1 wettstein rcc-staff 0 2013-01-15 10:48 /project/rcc/newfile Both files are readable and writable by the group owner due to the default umask, but the group owner differs due to the sticky bit being set on /project/rcc . NOTE : This applies only to newly created files and directories. If files or directories are moved from elsewhere, the ownership and permission may not work like this. Contact RCC help if you need assistance with setting filesystem permissions. Advanced Access Control via ACL General Instructions This section discusses a more flexible mechanism to administer data permissions. By default, only Linux-based permissions are set for folders and files, as described in File System Permissions. However, this only supports the permissions at the owner/group/others level. A second mechanism is called \u201cAccess Control Lists\u201d (ACL), which provides precise control over any data (files or directories) customizable for individual users or groups. Before applying ACL to your data, please read and understand the following caveats. By default no ACL is set for user data. ACL provides a highly flexible permission control, however, it also brings increased complexity to user access and management. PIs will normally want to share an entire project folder to all group members, and for this, the Linux-based permissions are enough. We suggest that users implement ACL controls only when necessary. One example is to protect confidential data in the project space by allowing only certain users to access confidential directories or files. After ACL is set, both Linux-based and ACL permissions will work together as a dual-guard system. The final effective access to data is granted only if permitted by both mechanisms. For example, if a folder is group-accessible to a user by Linux-based permission but restricted by ACL, the user cannot access this folder. Be sure you have enough knowledge setting up access via Linux-based permissions and ACL, i.e. you understand what \u201cusers\u201d, \u201cgroups\u201d and each attribute in \u201crwx\u201d mean and how to use them. Otherwise, please ask help@rcc.uchicago.edu for assistance managing your data access. We are here and happy to help you set up the permissions to keep your data safe and accessible as required. Example Suppose there is a folder tree as below, and you want to allow the folder my_folder to be accessible by the user jim only, and jim is already a member of your group rcctemp1 : /project2/rcctemp1 |- my_folder |- other_stuff Before using ACL, you need to confirm that this folder is permitted by all members in the group rcctemp1 : $ cd /project2 $ chgrp -R rcctemp1 my_folder $ chmod -R 770 rcctemp1 $ cd rcctemp1 At this moment, the folder rcctemp1 becomes readable and writable by all members of group rcctemp1 . Then, you can use the setfacl command to control the individual users access precisely. First, you need to remove the default group access by ACL: $ setfacl -m g::--- my_folder Although the command ls -l will still display group rwx access for the my_folder folder in the Linux-based permissions, users cannot access it anymore due to the permission set by ACL. Then, you can grant the user jim access to the folder: $ setfacl -m u:jim:rwx my_folder At this step, the user jim has both read and write permissions to the folder my_folder . You can set up permissions for each user the way you want. To view the list of configured accesses on the folder my_folder , run: $ getfacl my_folder # file: my_folder # owner: root # group: rcctemp1 user::rwx user:jim:rwx group::--- mask::rwx other::--- To revoke the permissions of the user jim to the folder: $ setfacl -x u:jim my_folder To clean up (remove) all ACL controls to the folder: $ setfacl -b my_folder For more information, please visit the ACL manual at https://wiki.archlinux.org/index.php/Access_Control_Lists","title":"File System Permissions"},{"location":"midway23/midway_file_permissions/#file-system-permissions","text":"Linux divides file permissions into read, write and execute (denoted by r, w, and x) for three user types: Owner, Group, and Other. Here is how you interpret this in the command line: To represent a given file's permissions for the three user types, combinations of the letters are commonly represented by numbers (i.e., 0700). Let\u2019s first summarize the default file system permission on Midway: Directory Permissions $HOME 0700 \u2013 Accessible only to the owner $SCRATCH 0700 \u2013 Accessible only to the owner /project/<PI CNetID> 2770 \u2013 Read/write for the project group /project2/<PI CNetID> 2770 \u2013 Read/write for the project group The default umask is 002 . When new files or directories are created, the umask influences the default permissions of those files and directories. With the umask set to 002 all files and directories will be group readable and writable by default. In your home directory, the group ownership will be set to your personal group, which is the same as your CNetID, so you will still be the only user that can access your files and directories. In the project directories, the group sticky bit causes the group ownership to be the same as the directory. This means files created in a project directory will be readable and writable by the project group, which is typically what is wanted in those directories. Here is an example of what this means in practice: $ ls -ld $HOME /project/rcc drwx------ 108 wettstein wettstein 32768 2013-01-15 10:51 /home/wettstein drwxrws--- 24 root rcc-staff 32768 2013-01-15 10:48 /project/rcc $ touch $HOME/newfile /project/rcc/newfile $ ls -l /project/rcc/newfile $HOME/newfile -rw-rw-r-- 1 wettstein wettstein 0 2013-01-15 10:48 /home/wettstein/newfile -rw-rw-r-- 1 wettstein rcc-staff 0 2013-01-15 10:48 /project/rcc/newfile Both files are readable and writable by the group owner due to the default umask, but the group owner differs due to the sticky bit being set on /project/rcc . NOTE : This applies only to newly created files and directories. If files or directories are moved from elsewhere, the ownership and permission may not work like this. Contact RCC help if you need assistance with setting filesystem permissions.","title":"File System Permissions"},{"location":"midway23/midway_file_permissions/#advanced-access-control-via-acl","text":"","title":"Advanced Access Control via ACL"},{"location":"midway23/midway_file_permissions/#general-instructions","text":"This section discusses a more flexible mechanism to administer data permissions. By default, only Linux-based permissions are set for folders and files, as described in File System Permissions. However, this only supports the permissions at the owner/group/others level. A second mechanism is called \u201cAccess Control Lists\u201d (ACL), which provides precise control over any data (files or directories) customizable for individual users or groups. Before applying ACL to your data, please read and understand the following caveats. By default no ACL is set for user data. ACL provides a highly flexible permission control, however, it also brings increased complexity to user access and management. PIs will normally want to share an entire project folder to all group members, and for this, the Linux-based permissions are enough. We suggest that users implement ACL controls only when necessary. One example is to protect confidential data in the project space by allowing only certain users to access confidential directories or files. After ACL is set, both Linux-based and ACL permissions will work together as a dual-guard system. The final effective access to data is granted only if permitted by both mechanisms. For example, if a folder is group-accessible to a user by Linux-based permission but restricted by ACL, the user cannot access this folder. Be sure you have enough knowledge setting up access via Linux-based permissions and ACL, i.e. you understand what \u201cusers\u201d, \u201cgroups\u201d and each attribute in \u201crwx\u201d mean and how to use them. Otherwise, please ask help@rcc.uchicago.edu for assistance managing your data access. We are here and happy to help you set up the permissions to keep your data safe and accessible as required.","title":"General Instructions"},{"location":"midway23/midway_file_permissions/#example","text":"Suppose there is a folder tree as below, and you want to allow the folder my_folder to be accessible by the user jim only, and jim is already a member of your group rcctemp1 : /project2/rcctemp1 |- my_folder |- other_stuff Before using ACL, you need to confirm that this folder is permitted by all members in the group rcctemp1 : $ cd /project2 $ chgrp -R rcctemp1 my_folder $ chmod -R 770 rcctemp1 $ cd rcctemp1 At this moment, the folder rcctemp1 becomes readable and writable by all members of group rcctemp1 . Then, you can use the setfacl command to control the individual users access precisely. First, you need to remove the default group access by ACL: $ setfacl -m g::--- my_folder Although the command ls -l will still display group rwx access for the my_folder folder in the Linux-based permissions, users cannot access it anymore due to the permission set by ACL. Then, you can grant the user jim access to the folder: $ setfacl -m u:jim:rwx my_folder At this step, the user jim has both read and write permissions to the folder my_folder . You can set up permissions for each user the way you want. To view the list of configured accesses on the folder my_folder , run: $ getfacl my_folder # file: my_folder # owner: root # group: rcctemp1 user::rwx user:jim:rwx group::--- mask::rwx other::--- To revoke the permissions of the user jim to the folder: $ setfacl -x u:jim my_folder To clean up (remove) all ACL controls to the folder: $ setfacl -b my_folder For more information, please visit the ACL manual at https://wiki.archlinux.org/index.php/Access_Control_Lists","title":"Example"},{"location":"midway23/midway_getting_started/","text":"Getting Started on Midway What is Midway? Midway2 and Midway3 are professionally-managed high performance computing clusters that constitute the core of the RCC\u2019s advanced computational infrastructure. Midway2 was introduced in 2016 as the successor to the RCC's first HPC cluster, Midway. Four years later in 2021, the Midway3 cluster was brought online. In this guide we use \"Midway\" to refer to both existing clusters, as much of the user experience for Midway2 and Midway3 is similar. We distinguish between the two whenever there are system-specific differences. Gaining Access The RCC offers two types of user accounts: a PI Account and a General User Account. All General Users must be sponsored by a PI with an active RCC account. More information about creating an account can be found on the accounts and allocations page . Connecting and Running Jobs After your RCC User account is created, you will connect to a Midway login node . There are several different ways to connect, depending on your operating system and desired user experience. Login nodes are the 'foyer' of the Midway supercomputer. They are connected to the internet and enable you to transfer data to and from the system. Once you successful connect to Midway and move data onto the system, you will be able to begin submitting and running jobs on compute nodes (running your programs on the cluster). Troubleshooting Solutions to most issues can be found at our Troubleshooting and FAQ page . For further assistance, please contact our Help Desk at help@rcc.uchicago.edu .","title":"Getting Started"},{"location":"midway23/midway_getting_started/#getting-started-on-midway","text":"","title":"Getting Started on Midway"},{"location":"midway23/midway_getting_started/#what-is-midway","text":"Midway2 and Midway3 are professionally-managed high performance computing clusters that constitute the core of the RCC\u2019s advanced computational infrastructure. Midway2 was introduced in 2016 as the successor to the RCC's first HPC cluster, Midway. Four years later in 2021, the Midway3 cluster was brought online. In this guide we use \"Midway\" to refer to both existing clusters, as much of the user experience for Midway2 and Midway3 is similar. We distinguish between the two whenever there are system-specific differences.","title":"What is Midway?"},{"location":"midway23/midway_getting_started/#gaining-access","text":"The RCC offers two types of user accounts: a PI Account and a General User Account. All General Users must be sponsored by a PI with an active RCC account. More information about creating an account can be found on the accounts and allocations page .","title":"Gaining Access"},{"location":"midway23/midway_getting_started/#connecting-and-running-jobs","text":"After your RCC User account is created, you will connect to a Midway login node . There are several different ways to connect, depending on your operating system and desired user experience. Login nodes are the 'foyer' of the Midway supercomputer. They are connected to the internet and enable you to transfer data to and from the system. Once you successful connect to Midway and move data onto the system, you will be able to begin submitting and running jobs on compute nodes (running your programs on the cluster).","title":"Connecting and Running Jobs"},{"location":"midway23/midway_getting_started/#troubleshooting","text":"Solutions to most issues can be found at our Troubleshooting and FAQ page . For further assistance, please contact our Help Desk at help@rcc.uchicago.edu .","title":"Troubleshooting"},{"location":"midway23/midway_hardware_overview/","text":"Hardware Overview This page provides technical details about the Midway2 and Midway3 compute clusters. Midway2 Midway3 Midway2 A professionally-managed high performance computing cluster forms the second generation core of RCC\u2019s advanced computational infrastructure. The key features of the Midway 2 hardware are listed as follows: 572 nodes total (16,016 cores) 210 standard compute nodes 4 NVIDIA K80 GPUs Total of 2.2 PB SSD local disk Operating System throughout cluster is Centos 8 Large shared memory nodes\u2014up to 1TB of memory per node with either 16, 28, or 32 Intel CPU cores GPU Nodes There are 4 GPU nodes that are part of the Midway2 communal resources. Each GPU node has the Standard Intel Compute Node specifications and the following GPU configurations: 1 GPU node w/ 4x NVIDIA K80 GPUs Big Memory Nodes There is 1 big memory node available to all users. The big memory node has the Standard Intel Compute Node specifications, but with the following larger memory configurations: 1 node w/ 512 GB of memory Midway3 The latest high performance computing cluster built, deployed and maintained by RCC. At this time, only the Intel specific resources are made available. The key features of the Midway3 Intel hardware are listed as follows: 220 nodes total (10,560 cores) 210 standard compute nodes 2 big memory nodes (1x768GB and 1x1.52TB) 11 GPU nodes All nodes have HDR InfiniBand (100 Gbps) network cards. Each node has 960 GB SSD local disk Operating System throughout cluster is Centos 8 GPU Nodes There are 11 GPU nodes that are part of the Midway3 communal resources. Each GPU node has the Standard Intel Compute Node specifications and the following GPU configurations: 5 GPU nodes w/ 4x NVIDIA V100 GPUs per node 5 GPU nodes w/ 4x NVIDIA Quadro RTX 6000 GPUs per node 1 GPU node w/ 4x NVIDIA A100 GPUs per node Big Memory Nodes There are 2 big memory nodes available to all users. The big memory node has the Standard Intel Compute Node specifications, but with the following larger memory configurations: 1 nodes w/ 768 GB of memory 1 nodes w/ 1.52 TB of memory","title":"Hardware Overview"},{"location":"midway23/midway_hardware_overview/#hardware-overview","text":"This page provides technical details about the Midway2 and Midway3 compute clusters. Midway2 Midway3","title":"Hardware Overview"},{"location":"midway23/midway_hardware_overview/#midway2","text":"A professionally-managed high performance computing cluster forms the second generation core of RCC\u2019s advanced computational infrastructure. The key features of the Midway 2 hardware are listed as follows: 572 nodes total (16,016 cores) 210 standard compute nodes 4 NVIDIA K80 GPUs Total of 2.2 PB SSD local disk Operating System throughout cluster is Centos 8 Large shared memory nodes\u2014up to 1TB of memory per node with either 16, 28, or 32 Intel CPU cores","title":"Midway2"},{"location":"midway23/midway_hardware_overview/#gpu-nodes","text":"There are 4 GPU nodes that are part of the Midway2 communal resources. Each GPU node has the Standard Intel Compute Node specifications and the following GPU configurations: 1 GPU node w/ 4x NVIDIA K80 GPUs","title":"GPU Nodes"},{"location":"midway23/midway_hardware_overview/#big-memory-nodes","text":"There is 1 big memory node available to all users. The big memory node has the Standard Intel Compute Node specifications, but with the following larger memory configurations: 1 node w/ 512 GB of memory","title":"Big Memory Nodes"},{"location":"midway23/midway_hardware_overview/#midway3","text":"The latest high performance computing cluster built, deployed and maintained by RCC. At this time, only the Intel specific resources are made available. The key features of the Midway3 Intel hardware are listed as follows: 220 nodes total (10,560 cores) 210 standard compute nodes 2 big memory nodes (1x768GB and 1x1.52TB) 11 GPU nodes All nodes have HDR InfiniBand (100 Gbps) network cards. Each node has 960 GB SSD local disk Operating System throughout cluster is Centos 8","title":"Midway3"},{"location":"midway23/midway_hardware_overview/#gpu-nodes_1","text":"There are 11 GPU nodes that are part of the Midway3 communal resources. Each GPU node has the Standard Intel Compute Node specifications and the following GPU configurations: 5 GPU nodes w/ 4x NVIDIA V100 GPUs per node 5 GPU nodes w/ 4x NVIDIA Quadro RTX 6000 GPUs per node 1 GPU node w/ 4x NVIDIA A100 GPUs per node","title":"GPU Nodes"},{"location":"midway23/midway_hardware_overview/#big-memory-nodes_1","text":"There are 2 big memory nodes available to all users. The big memory node has the Standard Intel Compute Node specifications, but with the following larger memory configurations: 1 nodes w/ 768 GB of memory 1 nodes w/ 1.52 TB of memory","title":"Big Memory Nodes"},{"location":"midway23/midway_job_management/","text":"Managing Jobs The Slurm job scheduler provides several command-line tools for checking on the status of your jobs and for managing them. For a complete list of Slurm commands, see the Slurm man pages. Checking Job Status Use the squeue command to check on the status of your jobs. squeue Any job with 0:00 under the TIME column is a job that is still waiting in the queue. To view only the jobs that you have submitted, use the --user flag. squeue --user=<CNetID> To get information about all jobs that are waiting to run on the bigmem2 partition, enter: squeue --state=PENDING --partition=bigmem2 To get information about all your jobs that are running on the bigmem2 partition, type: squeue --state=RUNNING --partition=bigmem2 --user=<CNetID> For more information, consult the command-line help by typing squeue --help , or visit the official online documentation. Monitoring Job Memory You can monitor your job by connecting to the compute node it is running on via SSH and using the htop command. To do this, run the following to see your running jobs and which compute nodes your jobs are running on: squeue --state=RUNNING --user=<CNetID> The last column of the output tells us which nodes are allocated for each job. You can connect to the compute node using the following command, where for example we are connecting to compute node midway2-0172. ssh midway2-0172 Finally run: htop To view processes running on the node, including your job's process which will be listed under your CNetID in the USER column. Canceling your jobs To cancel a job you have submitted, use the scancel command. This requires you to specify the id of the job you wish to cancel. For example, to cancel a job with id 8885128, do the following: scancel 8885128 If you are unsure what is the id of the job you would like to cancel, see the JOBID column from running squeue --user=<CNetID> . To cancel all jobs you have submitted that are either running or waiting in the queue, enter the following: scancel --user=<CNetID>","title":"Managing Jobs"},{"location":"midway23/midway_job_management/#managing-jobs","text":"The Slurm job scheduler provides several command-line tools for checking on the status of your jobs and for managing them. For a complete list of Slurm commands, see the Slurm man pages.","title":"Managing Jobs"},{"location":"midway23/midway_job_management/#checking-job-status","text":"Use the squeue command to check on the status of your jobs. squeue Any job with 0:00 under the TIME column is a job that is still waiting in the queue. To view only the jobs that you have submitted, use the --user flag. squeue --user=<CNetID> To get information about all jobs that are waiting to run on the bigmem2 partition, enter: squeue --state=PENDING --partition=bigmem2 To get information about all your jobs that are running on the bigmem2 partition, type: squeue --state=RUNNING --partition=bigmem2 --user=<CNetID> For more information, consult the command-line help by typing squeue --help , or visit the official online documentation.","title":"Checking Job Status"},{"location":"midway23/midway_job_management/#monitoring-job-memory","text":"You can monitor your job by connecting to the compute node it is running on via SSH and using the htop command. To do this, run the following to see your running jobs and which compute nodes your jobs are running on: squeue --state=RUNNING --user=<CNetID> The last column of the output tells us which nodes are allocated for each job. You can connect to the compute node using the following command, where for example we are connecting to compute node midway2-0172. ssh midway2-0172 Finally run: htop To view processes running on the node, including your job's process which will be listed under your CNetID in the USER column.","title":"Monitoring Job Memory"},{"location":"midway23/midway_job_management/#canceling-your-jobs","text":"To cancel a job you have submitted, use the scancel command. This requires you to specify the id of the job you wish to cancel. For example, to cancel a job with id 8885128, do the following: scancel 8885128 If you are unsure what is the id of the job you would like to cancel, see the JOBID column from running squeue --user=<CNetID> . To cancel all jobs you have submitted that are either running or waiting in the queue, enter the following: scancel --user=<CNetID>","title":"Canceling your jobs"},{"location":"midway23/midway_jobs_overview/","text":"Running Jobs on Midway This page describes core concepts for running programs on Midway2 or Midway3. We highly recommend reading it in its entirety. Service Units and Allocations All jobs running on Midway compute nodes consume Service Units (SUs), which are aquired through an allocation . When you submit a job, you specify the account/allocation to which the SUs will be charged. Briefly, SUs are a measure of the amount of computing resources (CPUs/GPUs) consumed on a compute cluster. In standard settings, 1 SU equals usage of 1 processing unit for 1 hour, but the exact calculation will vary depending on the amount of memory requested, as well as additional factors like the use of GPUs and CPU architecture. The aim of the Service Unit (SU) is to provide a \u201cfair\u201d account of computing resources. Midway2 and Midway3 are compute clusters shared by the entire University of Chicago community. Sharing computational resources creates unique challenges: Jobs must be scheduled in a way that is fair to all users. Consumption of resources needs to be recorded. Access to resources needs to be controlled. Slurm Workload Manager The compute clusters use a scheduler to manage requests for access to compute resources. These requests are called jobs , and contain directions about the scripts/programs the user wants to run. In particular, we use the Slurm workload manager to schedule batch jobs as well as interactive access to compute nodes. You can think of Slurm as the gatekeeper facilitating access the compute nodes. A user on a login node will submit a job to Slurm, which will decide (based on current node utilization and the requested parameters) which nodes to grant the user's job access to. Once you have submitted a job via Slurm, there are several commands you may use to monitor and manage it. Login nodes vs. Compute nodes Once you have connected to Midway2 or Midway3 (see Connecting to Midway ) you may work on one of the login nodes. Login nodes may be used for compiling and debugging code, installing software, editing and managing files, submitting jobs, or any other work that is not long-running or computationally intensive. Login nodes should never be used for computionally intensive work. All intensive computations should be performed on compute nodes. If you are unsure whether your computations will be intensive, please request an interactive session and continue your work once you have connected to the compute node. There are multiple types of compute nodes, organized into partitions . WARNING : Running computationally intensive jobs on the login nodes prevents other users from efficiently using the cluster. RCC System Administrators may terminate your processes without warning if your processes disrupt other users\u2019 work on the RCC cluster. Interactive vs. Batch Jobs There are two main ways to run programs on Midway: Interactively, via an \"interactive job\", and non-interactively, via a \"batch job\". Key point: In an interactive session, you will load software modules and run your scripts in real-time, whereas when submitting batch jobs, you specify the software modules to be loaded and scripts to be run in advance. Interactive jobs are the most intutive way to use Midway, as they allow you to interact with the program running on compute node/s (e.g., execute cells in a Jupyter Notebook) in real-time. This is great for exploratory work or troubleshooting. An interactive job will persist until you disconnect from the compute node, or until you reach the maximum requested time. * Batch jobs are non-interactive, as you submit a script to be executed on a compute node with no possibility of interactivity. A batch job doesn't require you to be logged in after submission, and ends when either (1) the script is finished running, (2) job's maximum time is reached, or (3) an error occurs. The next page, Submitting Jobs , will show you how to initiate both types of jobs. Partitions Midway compute nodes are organized into \"partitions\", subsets of nodes typically grouped by their hardware or ownership. When submitting a job you specify the partition it will run on by setting the --partition=<partition> flag. To view all partitions and node information, enter the command: rcchelp sinfo This table summarizes the available communal partitions: Midway2 Midway3 Partition Description Num. of Nodes Node Specifications* broadwl Default (CPU) partition; the choice for most jobs ? 64 GB Memory, ? GB SSD gpu2 GPU partition (4 cards per node); for ML or other GPU-accelerated jobs 6 GPU nodes with 4x NVIDIA K80 cards per node bigmem2 Small partition with large amounts of memory per node; for jobs requiring large amounts of memory 5 ? *Every node has ? Intel Broadwell CPUs (28 cores), ? GB of local SSD storage, and a 100 Gbps HDR InfiniBand network card. Partition Description Num. of Nodes Node Specifications* caslake Default (CPU) partition; the choice for most jobs 214 192 GB Memory gpu GPU partition (4 cards per node); for ML or other GPU-accelerated jobs 11 Either NVIDIA V100, RTX 6000, or A100 cards bigmem Small partition with large amounts of memory per node; for jobs requiring large amounts of memory 2 768 GB and 1.52 TB Memory amd CPU partition with AMD processors 36 AMD EPYC Rome CPU (2x per node) *Every node (except those in the amd partition) has 2 Intel Cascade Lake 6248R CPUs (48 cores), 900 GB of local SSD storage, and a 100 Gbps HDR InfiniBand network card. Job Limits and QOS To distribute computational resources fairly the RCC sets limits on the amount of computing resources that may be requested by a single user at any given time. These limits are enforced by the QOS (Quality of Service) assigned to each partition. A QOS is essentially a set of parameters, and each partition has its own, summarized in this table: Midway2 Midway3 QOS Max CPUs Per User Max Nodes Per User Max Jobs Per User Max Wall Time broadwl 2800 100 100 36 H gpu2 ? ? 10 36 H bigmem2 112 ? 5 36 H QOS Max CPUs Per User Max Nodes Per User Max Jobs Per User Max Wall Time caslake 100 4800 1000 36 H gpu 192 4 12 36 H bigmem 96 ? 10 36 H amd 1024 8 20 36 H Groups participating in the cluster parternership program may customize resources limits for their partitions. Additional information on limits can be found by entering the command rcchelp qos on any login or compute node on Midway2 or Midway3. Observe that these limits are often different depending on the partition. If your research requires a temporary exception to a particular limit, you may apply for a special allocation. Special allocations are evaluated on an individual basis and may or may not be granted.","title":"Overview"},{"location":"midway23/midway_jobs_overview/#running-jobs-on-midway","text":"This page describes core concepts for running programs on Midway2 or Midway3. We highly recommend reading it in its entirety.","title":"Running Jobs on Midway"},{"location":"midway23/midway_jobs_overview/#service-units-and-allocations","text":"All jobs running on Midway compute nodes consume Service Units (SUs), which are aquired through an allocation . When you submit a job, you specify the account/allocation to which the SUs will be charged. Briefly, SUs are a measure of the amount of computing resources (CPUs/GPUs) consumed on a compute cluster. In standard settings, 1 SU equals usage of 1 processing unit for 1 hour, but the exact calculation will vary depending on the amount of memory requested, as well as additional factors like the use of GPUs and CPU architecture. The aim of the Service Unit (SU) is to provide a \u201cfair\u201d account of computing resources. Midway2 and Midway3 are compute clusters shared by the entire University of Chicago community. Sharing computational resources creates unique challenges: Jobs must be scheduled in a way that is fair to all users. Consumption of resources needs to be recorded. Access to resources needs to be controlled.","title":"Service Units and Allocations"},{"location":"midway23/midway_jobs_overview/#slurm-workload-manager","text":"The compute clusters use a scheduler to manage requests for access to compute resources. These requests are called jobs , and contain directions about the scripts/programs the user wants to run. In particular, we use the Slurm workload manager to schedule batch jobs as well as interactive access to compute nodes. You can think of Slurm as the gatekeeper facilitating access the compute nodes. A user on a login node will submit a job to Slurm, which will decide (based on current node utilization and the requested parameters) which nodes to grant the user's job access to. Once you have submitted a job via Slurm, there are several commands you may use to monitor and manage it.","title":"Slurm Workload Manager"},{"location":"midway23/midway_jobs_overview/#login-nodes-vs-compute-nodes","text":"Once you have connected to Midway2 or Midway3 (see Connecting to Midway ) you may work on one of the login nodes. Login nodes may be used for compiling and debugging code, installing software, editing and managing files, submitting jobs, or any other work that is not long-running or computationally intensive. Login nodes should never be used for computionally intensive work. All intensive computations should be performed on compute nodes. If you are unsure whether your computations will be intensive, please request an interactive session and continue your work once you have connected to the compute node. There are multiple types of compute nodes, organized into partitions . WARNING : Running computationally intensive jobs on the login nodes prevents other users from efficiently using the cluster. RCC System Administrators may terminate your processes without warning if your processes disrupt other users\u2019 work on the RCC cluster.","title":"Login nodes vs. Compute nodes"},{"location":"midway23/midway_jobs_overview/#interactive-vs-batch-jobs","text":"There are two main ways to run programs on Midway: Interactively, via an \"interactive job\", and non-interactively, via a \"batch job\". Key point: In an interactive session, you will load software modules and run your scripts in real-time, whereas when submitting batch jobs, you specify the software modules to be loaded and scripts to be run in advance. Interactive jobs are the most intutive way to use Midway, as they allow you to interact with the program running on compute node/s (e.g., execute cells in a Jupyter Notebook) in real-time. This is great for exploratory work or troubleshooting. An interactive job will persist until you disconnect from the compute node, or until you reach the maximum requested time. * Batch jobs are non-interactive, as you submit a script to be executed on a compute node with no possibility of interactivity. A batch job doesn't require you to be logged in after submission, and ends when either (1) the script is finished running, (2) job's maximum time is reached, or (3) an error occurs. The next page, Submitting Jobs , will show you how to initiate both types of jobs.","title":"Interactive vs. Batch Jobs"},{"location":"midway23/midway_jobs_overview/#partitions","text":"Midway compute nodes are organized into \"partitions\", subsets of nodes typically grouped by their hardware or ownership. When submitting a job you specify the partition it will run on by setting the --partition=<partition> flag. To view all partitions and node information, enter the command: rcchelp sinfo This table summarizes the available communal partitions: Midway2 Midway3 Partition Description Num. of Nodes Node Specifications* broadwl Default (CPU) partition; the choice for most jobs ? 64 GB Memory, ? GB SSD gpu2 GPU partition (4 cards per node); for ML or other GPU-accelerated jobs 6 GPU nodes with 4x NVIDIA K80 cards per node bigmem2 Small partition with large amounts of memory per node; for jobs requiring large amounts of memory 5 ? *Every node has ? Intel Broadwell CPUs (28 cores), ? GB of local SSD storage, and a 100 Gbps HDR InfiniBand network card. Partition Description Num. of Nodes Node Specifications* caslake Default (CPU) partition; the choice for most jobs 214 192 GB Memory gpu GPU partition (4 cards per node); for ML or other GPU-accelerated jobs 11 Either NVIDIA V100, RTX 6000, or A100 cards bigmem Small partition with large amounts of memory per node; for jobs requiring large amounts of memory 2 768 GB and 1.52 TB Memory amd CPU partition with AMD processors 36 AMD EPYC Rome CPU (2x per node) *Every node (except those in the amd partition) has 2 Intel Cascade Lake 6248R CPUs (48 cores), 900 GB of local SSD storage, and a 100 Gbps HDR InfiniBand network card.","title":"Partitions"},{"location":"midway23/midway_jobs_overview/#job-limits-and-qos","text":"To distribute computational resources fairly the RCC sets limits on the amount of computing resources that may be requested by a single user at any given time. These limits are enforced by the QOS (Quality of Service) assigned to each partition. A QOS is essentially a set of parameters, and each partition has its own, summarized in this table: Midway2 Midway3 QOS Max CPUs Per User Max Nodes Per User Max Jobs Per User Max Wall Time broadwl 2800 100 100 36 H gpu2 ? ? 10 36 H bigmem2 112 ? 5 36 H QOS Max CPUs Per User Max Nodes Per User Max Jobs Per User Max Wall Time caslake 100 4800 1000 36 H gpu 192 4 12 36 H bigmem 96 ? 10 36 H amd 1024 8 20 36 H Groups participating in the cluster parternership program may customize resources limits for their partitions. Additional information on limits can be found by entering the command rcchelp qos on any login or compute node on Midway2 or Midway3. Observe that these limits are often different depending on the partition. If your research requires a temporary exception to a particular limit, you may apply for a special allocation. Special allocations are evaluated on an individual basis and may or may not be granted.","title":"Job Limits and QOS"},{"location":"midway23/midway_submitting_jobs/","text":"Submitting Jobs This page describes how users can submit jobs (either Interactive or Batch) to Midway. The flowchart below illustrates the main steps in that process. Interactive Jobs Interactive jobs are the most intutive way to use Midway, as they allow you to interact with the program running on compute node/s (e.g., execute cells in a Jupyter Notebook) in real-time. This is great for exploratory work or troubleshooting. An interactive job will persist until you disconnect from the compute node, or until you reach the maximum requested time. To request an interactive job, run the following command while connected to a login node: sinteractive As soon as the requested resources become available, sinteractive will do the following: 1. Log in to the compute node/s. 2. Change into the directory you were working in. 3. Set up X11 forwarding for displaying graphics. 4. Transfer your current shell environment, including any modules you have previously loaded. By default, an interactive session times out after 2 hours. If you would like more than 2 hours, be sure to include a --time=HH:MM:SS flag to specify the necessary amount of time. For example, to request an interactive session for 6 hours, run the following command: sinteractive --time=06:00:00 There are many additional options for the sinteractive command, including options to select the number of nodes, the number of cores per node, the amount of memory, and so on. For example, to request exclusive use of two compute nodes on the Midway2 broadwl partition for 8 hours, enter the following: sinteractive --exclusive --partition=broadwl --nodes=2 --time=08:00:00 For more details about these and other useful parameters, read below about the sbatch command. All options available in the sbatch command are also available for the sinteractive command. Debug QOS There is a debug QOS (Quality of Service) setup to help users quickly access some resources to debug or test their code before submitting their jobs to the main partition. The debug QOS will allow you to run one job and get up to 4 cores for 15 minutes without consuming SUs. To use the debug QOS, you have to specify --time as 15 minutes or less. For example, to get 2 cores for 15 minutes, you could run: sinteractive --qos=debug --time=00:15:00 --ntasks=2 Batch Jobs The sbatch command is used to request computing resources on the Midway clusters. Rather than specify all the options in the command line, users typically write an \u201csbatch script\u201d that contains all the commands and parameters neccessary to run a program on the cluster. Batch jobs are non-interactive, as you submit a program to be executed on a compute node with no possibility of interactivity. A batch job doesn't require you to be logged in after submission, and ends when either (1) the program is finished running, (2) job's maximum time is reached, or (3) an error occurs. SBATCH Scripts In an sbatch script, all Slurm parameters are declared with #SBATCH , followed by additional definitions. Here is an example of a Midway2 sbatch script: #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:05:00 #SBATCH --partition=broadwl #SBATCH --nodes=4 #SBATCH --ntasks-per-node=14 #SBATCH --mem-per-cpu=2000 module load openmpi mpirun ./hello-mpi And here is an explanation of what each of these parameters does: Option Description --job-name=example_sbatch Assigns name example_sbatch to the job. --output=example_sbatch.out Writes console output to file example_sbatch.out . --error=example_sbatch.err Writes error messages to file example_sbatch.err . --time=00:05:00 Reserves the computing resources for 5 minutes (or less if program completes before 5 min). --partition=broadwl Requests compute nodes from the broadwell partition on the Midway2 cluster. --nodes=4 Requests 4 compute nodes --ntasks-per-node=14 Requests 14 cores (CPUs) per node, for a total of 14 * 4 = 56 cores. --mem-per-cpu=2000 Requests 2000 MB (2 GB) of memory (RAM) per core, for a total of 2 * 14 = 28 GB per node. In this example, we have requested 4 compute nodes with 14 CPUs each. Therefore, we have requested a total of 56 CPUs for running our program. The last two lines of the script load the OpenMPI module and launch the MPI-based executable that we have called hello-mpi . Submitting a Batch Job Continuing the example above, suppose that the sbatch script is saved in the current directory into a file called example.sbatch . This script is submitted to the cluster using the following command: sbatch ./example.sbatch or more generally: sbatch ./<your_sbatch_file> Example Submission Scripts Here are some example sbatch job submission scripts that demonstrate the different options and jobs types you make wish to use. A single core job to the standard compute partition: Midway2 Midway3 #!/bin/bash #SBATCH --job-name=single-node-cpu-example #SBATCH --account=pi-[group] #SBATCH --partition=broadwl #SBATCH --ntasks-per-node=1 # number of tasks #SBATCH --cpus-per-task=1 # number of threads per task # LOAD MODULES module load python # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python hello.py #!/bin/sh #SBATCH --job-name=single-node-cpu-example #SBATCH --account=pi-[group] #SBATCH --partition=caslake #SBATCH --ntasks-per-node=1 # number of tasks #SBATCH --cpus-per-task=1 # number of threads per task # LOAD MODULES module load python # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python hello.py A single node gpu job to the gpu partition: Midway2 Midway3 #!/bin/bash #SBATCH --job-name=1gpu-example #SBATCH --account=pi-[group] #SBATCH --partition=gpu2 #SBATCH --gres=gpu:1 #SBATCH --ntasks-per-node=1 # num cores to drive each gpu #SBATCH --cpus-per-task=1 # set this to the desired number of threads # LOAD MODULES module load tensorflow module load cudnn # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python training.py #!/bin/sh #SBATCH --job-name=1gpu-example #SBATCH --account=pi-[group] #SBATCH --partition=gpu #SBATCH --gres=gpu:1 # TO USE V100 specify --constraint=v100 # TO USE RTX600 specify --constraint=rtx6000 #SBATCH --constraint=v100 # constraint job runs on V100 GPU use #SBATCH --ntasks-per-node=1 # num cores to drive each gpu #SBATCH --cpus-per-task=1 # set this to the desired number of threads # LOAD MODULES module load tensorflow module load cudnn # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python training.py You can find more example sbatch submission scripts in the RCC SLURM workshop materials","title":"Submitting Jobs"},{"location":"midway23/midway_submitting_jobs/#submitting-jobs","text":"This page describes how users can submit jobs (either Interactive or Batch) to Midway. The flowchart below illustrates the main steps in that process.","title":"Submitting Jobs"},{"location":"midway23/midway_submitting_jobs/#interactive-jobs","text":"Interactive jobs are the most intutive way to use Midway, as they allow you to interact with the program running on compute node/s (e.g., execute cells in a Jupyter Notebook) in real-time. This is great for exploratory work or troubleshooting. An interactive job will persist until you disconnect from the compute node, or until you reach the maximum requested time. To request an interactive job, run the following command while connected to a login node: sinteractive As soon as the requested resources become available, sinteractive will do the following: 1. Log in to the compute node/s. 2. Change into the directory you were working in. 3. Set up X11 forwarding for displaying graphics. 4. Transfer your current shell environment, including any modules you have previously loaded. By default, an interactive session times out after 2 hours. If you would like more than 2 hours, be sure to include a --time=HH:MM:SS flag to specify the necessary amount of time. For example, to request an interactive session for 6 hours, run the following command: sinteractive --time=06:00:00 There are many additional options for the sinteractive command, including options to select the number of nodes, the number of cores per node, the amount of memory, and so on. For example, to request exclusive use of two compute nodes on the Midway2 broadwl partition for 8 hours, enter the following: sinteractive --exclusive --partition=broadwl --nodes=2 --time=08:00:00 For more details about these and other useful parameters, read below about the sbatch command. All options available in the sbatch command are also available for the sinteractive command.","title":"Interactive Jobs"},{"location":"midway23/midway_submitting_jobs/#debug-qos","text":"There is a debug QOS (Quality of Service) setup to help users quickly access some resources to debug or test their code before submitting their jobs to the main partition. The debug QOS will allow you to run one job and get up to 4 cores for 15 minutes without consuming SUs. To use the debug QOS, you have to specify --time as 15 minutes or less. For example, to get 2 cores for 15 minutes, you could run: sinteractive --qos=debug --time=00:15:00 --ntasks=2","title":"Debug QOS"},{"location":"midway23/midway_submitting_jobs/#batch-jobs","text":"The sbatch command is used to request computing resources on the Midway clusters. Rather than specify all the options in the command line, users typically write an \u201csbatch script\u201d that contains all the commands and parameters neccessary to run a program on the cluster. Batch jobs are non-interactive, as you submit a program to be executed on a compute node with no possibility of interactivity. A batch job doesn't require you to be logged in after submission, and ends when either (1) the program is finished running, (2) job's maximum time is reached, or (3) an error occurs.","title":"Batch Jobs"},{"location":"midway23/midway_submitting_jobs/#sbatch-scripts","text":"In an sbatch script, all Slurm parameters are declared with #SBATCH , followed by additional definitions. Here is an example of a Midway2 sbatch script: #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:05:00 #SBATCH --partition=broadwl #SBATCH --nodes=4 #SBATCH --ntasks-per-node=14 #SBATCH --mem-per-cpu=2000 module load openmpi mpirun ./hello-mpi And here is an explanation of what each of these parameters does: Option Description --job-name=example_sbatch Assigns name example_sbatch to the job. --output=example_sbatch.out Writes console output to file example_sbatch.out . --error=example_sbatch.err Writes error messages to file example_sbatch.err . --time=00:05:00 Reserves the computing resources for 5 minutes (or less if program completes before 5 min). --partition=broadwl Requests compute nodes from the broadwell partition on the Midway2 cluster. --nodes=4 Requests 4 compute nodes --ntasks-per-node=14 Requests 14 cores (CPUs) per node, for a total of 14 * 4 = 56 cores. --mem-per-cpu=2000 Requests 2000 MB (2 GB) of memory (RAM) per core, for a total of 2 * 14 = 28 GB per node. In this example, we have requested 4 compute nodes with 14 CPUs each. Therefore, we have requested a total of 56 CPUs for running our program. The last two lines of the script load the OpenMPI module and launch the MPI-based executable that we have called hello-mpi .","title":"SBATCH Scripts"},{"location":"midway23/midway_submitting_jobs/#submitting-a-batch-job","text":"Continuing the example above, suppose that the sbatch script is saved in the current directory into a file called example.sbatch . This script is submitted to the cluster using the following command: sbatch ./example.sbatch or more generally: sbatch ./<your_sbatch_file>","title":"Submitting a Batch Job"},{"location":"midway23/midway_submitting_jobs/#example-submission-scripts","text":"Here are some example sbatch job submission scripts that demonstrate the different options and jobs types you make wish to use. A single core job to the standard compute partition: Midway2 Midway3 #!/bin/bash #SBATCH --job-name=single-node-cpu-example #SBATCH --account=pi-[group] #SBATCH --partition=broadwl #SBATCH --ntasks-per-node=1 # number of tasks #SBATCH --cpus-per-task=1 # number of threads per task # LOAD MODULES module load python # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python hello.py #!/bin/sh #SBATCH --job-name=single-node-cpu-example #SBATCH --account=pi-[group] #SBATCH --partition=caslake #SBATCH --ntasks-per-node=1 # number of tasks #SBATCH --cpus-per-task=1 # number of threads per task # LOAD MODULES module load python # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python hello.py A single node gpu job to the gpu partition: Midway2 Midway3 #!/bin/bash #SBATCH --job-name=1gpu-example #SBATCH --account=pi-[group] #SBATCH --partition=gpu2 #SBATCH --gres=gpu:1 #SBATCH --ntasks-per-node=1 # num cores to drive each gpu #SBATCH --cpus-per-task=1 # set this to the desired number of threads # LOAD MODULES module load tensorflow module load cudnn # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python training.py #!/bin/sh #SBATCH --job-name=1gpu-example #SBATCH --account=pi-[group] #SBATCH --partition=gpu #SBATCH --gres=gpu:1 # TO USE V100 specify --constraint=v100 # TO USE RTX600 specify --constraint=rtx6000 #SBATCH --constraint=v100 # constraint job runs on V100 GPU use #SBATCH --ntasks-per-node=1 # num cores to drive each gpu #SBATCH --cpus-per-task=1 # set this to the desired number of threads # LOAD MODULES module load tensorflow module load cudnn # DO COMPUTE WORK export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK python training.py You can find more example sbatch submission scripts in the RCC SLURM workshop materials","title":"Example Submission Scripts"},{"location":"midway23/midway_troubleshooting/","text":"Frequently Asked Questions This page provides answers to frequently asked questions when using the Midway2 and Midway3. Midway2 Midway3 How can I list the software packages available? At the terminal on the login node you can run module list You can list all the versions of a software package, or a software bundle, with module avail module avail [package-name] For example, module avail cuda or module avail python You can then show the dependencies of a particular module, its location and the environment variables that are set: module show [package-name] You can load the software package into your environment module load [package-name] which essentially updates the environment variables so that you can access to the binaries (with the prepended PATH ) or your application can link with the package libraries (with the modified LD_LIBRARY_PATH ). How can I install new python packges? The recommended practice is to load an existing python module, create your own environment and then install the packge(s) into your own environment. module load python/anaconda-2021.05 then create and activate your own environment conda create --prefix=/path/to/your/env/my-env python=3.8 conda activate my-env or with venv python -m venv /path/to/your/env/my-env source /path/to/your/env/my-env/bin/activate Finally, install your packages via conda install or pip install : conda install -c conda-forge numpy or pip install numpy Whenever possible, specify the variant of the package that you want explicitly that matches the modules you already loaded. pip install tensorflow cudatoolkit=11.2 cudnn=8.1.0 or pip3 install torch==1.12.0+cu113 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113 Reference: PyTorch documentation How can I use Tensorflow and PyTorch with GPUs? You need to install these packages with the existing CUDA toolkit modules on Midway2 (see module avail cuda ), then load the module via module load cuda/11.3 . Then install Tensorflow and PyTorch into your own environment (see above). You can check if the installed version of Tensorflow in your enviroment can access to the GPUs on a GPU compute node via python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" Similarly for PyTorch with GPU support python -c \"import torch; print(torch.vresion.cuda)\" Reference: Tensorflow documentation How can I compile third-party packages or my own codes? How can I list the software packages available? At the terminal on the login node you can run module list You can list all the versions of a software package, or a software bundle, with module avail module avail [package-name] For example, module avail cuda or module avail python You can then show the dependencies of a particular module, its location and the environment variables that are set: module show [package-name] You can load the software package into your environment module load [package-name] which essentially updates the environment variables so that you can access to the binaries (with the prepended PATH ) or your application can link with the package libraries (with the modified LD_LIBRARY_PATH ). How can I install new python packges? How can I compile third-party packages or my own codes? How can I use Tensorflow and PyTorch with GPUs?","title":"Troubleshooting and FAQ"},{"location":"midway23/midway_troubleshooting/#frequently-asked-questions","text":"This page provides answers to frequently asked questions when using the Midway2 and Midway3. Midway2 Midway3","title":"Frequently Asked Questions"},{"location":"midway23/midway_troubleshooting/#how-can-i-list-the-software-packages-available","text":"At the terminal on the login node you can run module list You can list all the versions of a software package, or a software bundle, with module avail module avail [package-name] For example, module avail cuda or module avail python You can then show the dependencies of a particular module, its location and the environment variables that are set: module show [package-name] You can load the software package into your environment module load [package-name] which essentially updates the environment variables so that you can access to the binaries (with the prepended PATH ) or your application can link with the package libraries (with the modified LD_LIBRARY_PATH ).","title":"How can I list the software packages available?"},{"location":"midway23/midway_troubleshooting/#how-can-i-install-new-python-packges","text":"The recommended practice is to load an existing python module, create your own environment and then install the packge(s) into your own environment. module load python/anaconda-2021.05 then create and activate your own environment conda create --prefix=/path/to/your/env/my-env python=3.8 conda activate my-env or with venv python -m venv /path/to/your/env/my-env source /path/to/your/env/my-env/bin/activate Finally, install your packages via conda install or pip install : conda install -c conda-forge numpy or pip install numpy Whenever possible, specify the variant of the package that you want explicitly that matches the modules you already loaded. pip install tensorflow cudatoolkit=11.2 cudnn=8.1.0 or pip3 install torch==1.12.0+cu113 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113 Reference: PyTorch documentation","title":"How can I install new python packges?"},{"location":"midway23/midway_troubleshooting/#how-can-i-use-tensorflow-and-pytorch-with-gpus","text":"You need to install these packages with the existing CUDA toolkit modules on Midway2 (see module avail cuda ), then load the module via module load cuda/11.3 . Then install Tensorflow and PyTorch into your own environment (see above). You can check if the installed version of Tensorflow in your enviroment can access to the GPUs on a GPU compute node via python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\" Similarly for PyTorch with GPU support python -c \"import torch; print(torch.vresion.cuda)\" Reference: Tensorflow documentation","title":"How can I use Tensorflow and PyTorch with GPUs?"},{"location":"midway23/midway_troubleshooting/#how-can-i-compile-third-party-packages-or-my-own-codes","text":"","title":"How can I compile third-party packages or my own codes?"},{"location":"midway23/midway_troubleshooting/#how-can-i-list-the-software-packages-available_1","text":"At the terminal on the login node you can run module list You can list all the versions of a software package, or a software bundle, with module avail module avail [package-name] For example, module avail cuda or module avail python You can then show the dependencies of a particular module, its location and the environment variables that are set: module show [package-name] You can load the software package into your environment module load [package-name] which essentially updates the environment variables so that you can access to the binaries (with the prepended PATH ) or your application can link with the package libraries (with the modified LD_LIBRARY_PATH ).","title":"How can I list the software packages available?"},{"location":"midway23/midway_troubleshooting/#how-can-i-install-new-python-packges_1","text":"","title":"How can I install new python packges?"},{"location":"midway23/midway_troubleshooting/#how-can-i-compile-third-party-packages-or-my-own-codes_1","text":"","title":"How can I compile third-party packages or my own codes?"},{"location":"midway23/midway_troubleshooting/#how-can-i-use-tensorflow-and-pytorch-with-gpus_1","text":"","title":"How can I use Tensorflow and PyTorch with GPUs?"},{"location":"midway23/request_and_manage_allocations/","text":"Checking Your SU Balance and Usage The rcchelp tool can be used to check account balances. After logging into Midway 2 or Midway 3, simply type: rcchelp balance If you are a member of multiple groups, this will display the allocations and usage for all your groups. The rcchelp balance command has a number of options for summarizing allocation usage. For information on these options, type rcchelp balance --help To see an overall summary of your usage, simply enter: rcchelp usage","title":"Request and manage allocations"},{"location":"midway23/request_and_manage_allocations/#checking-your-su-balance-and-usage","text":"The rcchelp tool can be used to check account balances. After logging into Midway 2 or Midway 3, simply type: rcchelp balance If you are a member of multiple groups, this will display the allocations and usage for all your groups. The rcchelp balance command has a number of options for summarizing allocation usage. For information on these options, type rcchelp balance --help To see an overall summary of your usage, simply enter: rcchelp usage","title":"Checking Your SU Balance and Usage"},{"location":"midway23/examples/array/","text":"Job arrays Slurm job arrays provide a convenient way to submit a large number of independent processing jobs. For example, Slurm job arrays can be useful for applying the same or similar computation to a collection of data sets. When a job array script is submitted, a specified number of \u201carray tasks\u201d are created based on the \u201cmaster\u201d sbatch script. Consider the following example (from array.sbatch ): #!/bin/bash #SBATCH --job-name=array #SBATCH --output=array_%A_%a.out #SBATCH --error=array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=1 #SBATCH --mem=4G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. In this simple example, --array=1-16 requests 16 array tasks (numbered 1 through 16). The \u201carray tasks\u201d are copies of the master script that are automatically submitted to the scheduler on your behalf. In each array task, the environment variable SLURM_ARRAY_TASK_ID is set to a unique value (in this example, numbers ranging from 1 to 16). Job array indices can be specified in several different ways. Here are some examples: # A job array with array tasks numbered from 0 to 31. #SBATCH --array=0-31 # A job array with array tasks numbered 1, 2, 5, 19, 27. #SBATCH --array=1,2,5,19,27 # A job array with array tasks numbered 1, 3, 5 and 7. #SBATCH --array=1-7:2 In the example sbatch script above, the %A_%a notation is filled in with the master job id ( %A ) and the array task id ( %a ). This is a simple way to create output files in which the file name is different for each job in the array. The remaining options in the sbatch script are the same as the options used in other, non-array settings; in this example, we are requesting that each array task be allocated 1 CPU ( --ntasks=1 ) and 4 GB of memory ( --mem=4G ) on the broadwl partition ( --partition=broadwl ) for up to one hour ( --time=01:00:00 ). Most partitions have limits on the number of array tasks that can run simultaneously. To achieve a higher throughput, consider Parallel batch jobs . For more information about Slurm job arrays, consule the the `Slurm documentation on job arrays`_","title":"Job arrays"},{"location":"midway23/examples/array/#job-arrays","text":"Slurm job arrays provide a convenient way to submit a large number of independent processing jobs. For example, Slurm job arrays can be useful for applying the same or similar computation to a collection of data sets. When a job array script is submitted, a specified number of \u201carray tasks\u201d are created based on the \u201cmaster\u201d sbatch script. Consider the following example (from array.sbatch ): #!/bin/bash #SBATCH --job-name=array #SBATCH --output=array_%A_%a.out #SBATCH --error=array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=1 #SBATCH --mem=4G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. In this simple example, --array=1-16 requests 16 array tasks (numbered 1 through 16). The \u201carray tasks\u201d are copies of the master script that are automatically submitted to the scheduler on your behalf. In each array task, the environment variable SLURM_ARRAY_TASK_ID is set to a unique value (in this example, numbers ranging from 1 to 16). Job array indices can be specified in several different ways. Here are some examples: # A job array with array tasks numbered from 0 to 31. #SBATCH --array=0-31 # A job array with array tasks numbered 1, 2, 5, 19, 27. #SBATCH --array=1,2,5,19,27 # A job array with array tasks numbered 1, 3, 5 and 7. #SBATCH --array=1-7:2 In the example sbatch script above, the %A_%a notation is filled in with the master job id ( %A ) and the array task id ( %a ). This is a simple way to create output files in which the file name is different for each job in the array. The remaining options in the sbatch script are the same as the options used in other, non-array settings; in this example, we are requesting that each array task be allocated 1 CPU ( --ntasks=1 ) and 4 GB of memory ( --mem=4G ) on the broadwl partition ( --partition=broadwl ) for up to one hour ( --time=01:00:00 ). Most partitions have limits on the number of array tasks that can run simultaneously. To achieve a higher throughput, consider Parallel batch jobs . For more information about Slurm job arrays, consule the the `Slurm documentation on job arrays`_","title":"Job arrays"},{"location":"midway23/examples/bigmem/","text":"Large-memory jobs The bigmem2 partition is particularly well suited for computations that need more than about 60 GB of memory; see Types of Compute Nodes for technical specifications of the bigmem2 nodes. Running a large-memory job To submit a job to bigmem2, include this line in your sbatch script: #SBATCH --partition=bigmem2 Additionally, it is important to use the --mem or --mem-per-cpu options. For example, to request 8 CPU cores and 128 GB of memory on a bigmem2 node, add the following to your sbatch script: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=128G Interactive computing on bigmem2 These same options can also be used to set up an sinteractive session. For example, to access a bigmem2 node with 1 CPU and 128 GB of memory, run: sinteractive --partition = bigmem2 --ntasks = 1 --cpus-per-task = 8 --mem = 128G","title":"Large-memory jobs"},{"location":"midway23/examples/bigmem/#large-memory-jobs","text":"The bigmem2 partition is particularly well suited for computations that need more than about 60 GB of memory; see Types of Compute Nodes for technical specifications of the bigmem2 nodes.","title":"Large-memory jobs"},{"location":"midway23/examples/bigmem/#running-a-large-memory-job","text":"To submit a job to bigmem2, include this line in your sbatch script: #SBATCH --partition=bigmem2 Additionally, it is important to use the --mem or --mem-per-cpu options. For example, to request 8 CPU cores and 128 GB of memory on a bigmem2 node, add the following to your sbatch script: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=128G","title":"Running a large-memory job"},{"location":"midway23/examples/bigmem/#interactive-computing-on-bigmem2","text":"These same options can also be used to set up an sinteractive session. For example, to access a bigmem2 node with 1 CPU and 128 GB of memory, run: sinteractive --partition = bigmem2 --ntasks = 1 --cpus-per-task = 8 --mem = 128G","title":"Interactive computing on bigmem2"},{"location":"midway23/examples/cron/","text":"Cron-like jobs Cron jobs persist until they are canceled or encounter an error. The Midway2 cluster has a dedicated partition, cron , for running Cron jobs. Please email help@rcc.uchicago.edu to request submitting Cron-like jobs. These jobs are subject to scheduling limits and will be monitored. Here is an example of an sbatch script that runs a Cron job (see also cron.sbatch ): #!/bin/bash #SBATCH --time=00:05:00 #SBATCH --output=cron.log #SBATCH --open-mode=append #SBATCH --account=cron-account #SBATCH --partition=cron #SBATCH --qos=cron # Specify a valid Cron string for the schedule. This specifies that # the Cron job run once per day at 5:15a. SCHEDULE = '15 5 * * *' # Here is an example of a simple command that prints the host name and # the date and time. echo \"Hello on $( hostname ) at $( date ) .\" # This schedules the next run. sbatch --quiet --begin = $( next-cron-time \" $SCHEDULE \" ) cron.sbatch After executing a simple command (print the host name, date and time), the script schedules the next run with another call to sbatch with the --begin option.","title":"Cron-like jobs"},{"location":"midway23/examples/cron/#cron-like-jobs","text":"Cron jobs persist until they are canceled or encounter an error. The Midway2 cluster has a dedicated partition, cron , for running Cron jobs. Please email help@rcc.uchicago.edu to request submitting Cron-like jobs. These jobs are subject to scheduling limits and will be monitored. Here is an example of an sbatch script that runs a Cron job (see also cron.sbatch ): #!/bin/bash #SBATCH --time=00:05:00 #SBATCH --output=cron.log #SBATCH --open-mode=append #SBATCH --account=cron-account #SBATCH --partition=cron #SBATCH --qos=cron # Specify a valid Cron string for the schedule. This specifies that # the Cron job run once per day at 5:15a. SCHEDULE = '15 5 * * *' # Here is an example of a simple command that prints the host name and # the date and time. echo \"Hello on $( hostname ) at $( date ) .\" # This schedules the next run. sbatch --quiet --begin = $( next-cron-time \" $SCHEDULE \" ) cron.sbatch After executing a simple command (print the host name, date and time), the script schedules the next run with another call to sbatch with the --begin option.","title":"Cron-like jobs"},{"location":"midway23/examples/example_job_scripts/","text":"Illustrative examples Below are a number of example submission scripts that you can adapt to run your jobs on Midway. See also the materials from the RCC Slurm workshop for additional examples. Job arrays Slurm job arrays provide a convenient way to submit a large number of independent processing jobs. For example, Slurm job arrays can be useful for applying the same or similar computation to a collection of data sets. When a job array script is submitted, a specified number of \u201carray tasks\u201d are created based on the \u201cmaster\u201d sbatch script. Consider the following example (from array.sbatch ): #!/bin/bash #SBATCH --job-name=array #SBATCH --output=array_%A_%a.out #SBATCH --error=array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=1 #SBATCH --mem=4G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. In this simple example, --array=1-16 requests 16 array tasks (numbered 1 through 16). The \u201carray tasks\u201d are copies of the master script that are automatically submitted to the scheduler on your behalf. In each array task, the environment variable SLURM_ARRAY_TASK_ID is set to a unique value (in this example, numbers ranging from 1 to 16). Job array indices can be specified in several different ways. Here are some examples: # A job array with array tasks numbered from 0 to 31. #SBATCH --array=0-31 # A job array with array tasks numbered 1, 2, 5, 19, 27. #SBATCH --array=1,2,5,19,27 # A job array with array tasks numbered 1, 3, 5 and 7. #SBATCH --array=1-7:2 In the example sbatch script above, the %A_%a notation is filled in with the master job id ( %A ) and the array task id ( %a ). This is a simple way to create output files in which the file name is different for each job in the array. The remaining options in the sbatch script are the same as the options used in other, non-array settings; in this example, we are requesting that each array task be allocated 1 CPU ( --ntasks=1 ) and 4 GB of memory ( --mem=4G ) on the broadwl partition ( --partition=broadwl ) for up to one hour ( --time=01:00:00 ). Most partitions have limits on the number of array tasks that can run simultaneously. To achieve a higher throughput, consider Parallel batch jobs . For more information about Slurm job arrays, consule the the `Slurm documentation on job arrays`_ Large-memory jobs The bigmem2 partition is particularly well suited for computations that need more than about 60 GB of memory; see Types of Compute Nodes for technical specifications of the bigmem2 nodes. Running a large-memory job To submit a job to bigmem2, include this line in your sbatch script: #SBATCH --partition=bigmem2 Additionally, it is important to use the --mem or --mem-per-cpu options. For example, to request 8 CPU cores and 128 GB of memory on a bigmem2 node, add the following to your sbatch script: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=128G Interactive computing on bigmem2 These same options can also be used to set up an sinteractive session. For example, to access a bigmem2 node with 1 CPU and 128 GB of memory, run: sinteractive --partition = bigmem2 --ntasks = 1 --cpus-per-task = 8 --mem = 128G Parallel batch jobs Computations involving a very large number of independent computations should be combined in some way to reduce the number of jobs submitted to Slurm. Here we illustrate one strategy for doing this using GNU Parallel and srun . The parallel program executes tasks simultaneously until all tasks have been completed. Here\u2019s an example script, parallel.sbatch : #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=28 #SBATCH --mem-per-cpu=2G # NOTE DO NOT USE THE --mem= OPTION # Load the default version of GNU parallel. module load parallel # When running a large number of tasks simultaneously, it may be # necessary to increase the user process limit. ulimit -u 10000 # This specifies the options used to run srun. The \"-N1 -n1\" options are # used to allocates a single core to each task. srun = \"srun --exclusive -N1 -n1\" # This specifies the options used to run GNU parallel: # # --delay of 0.2 prevents overloading the controlling node. # # -j is the number of tasks run simultaneously. # # The combination of --joblog and --resume create a task log that # can be used to monitor progress. # parallel = \"parallel --delay 0.2 -j $SLURM_NTASKS --joblog runtask.log --resume\" # Run a script, runtask.sh, using GNU parallel and srun. Parallel # will run the runtask script for the numbers 1 through 128. To # illustrate, the first job will run like this: # # srun --exclusive -N1 -n1 ./runtask.sh arg1:1 > runtask.1 # $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..128 } # Note that if your program does not take any input, use the -n0 option to # call the parallel command: # # $parallel -n0 \"$srun ./run_noinput_task.sh > output.{1}\" ::: {1..128} In this example, our aim is to run script runtask.sh 128 times. The --ntasks option is set to 28, so at most 28 tasks can be run simultaneously. Here is the runtask.sh script that is run by GNU parallel: #!/bin/sh # This script outputs some useful information so we can see what parallel # and srun are doing. sleepsecs = $ [ ( $RANDOM % 10 ) + 10 ] s # $1 is arg1:{1} from GNU parallel. # # $PARALLEL_SEQ is a special variable from GNU parallel. It gives the # number of the job in the sequence. # # Here we print the sleep time, host name, and the date and time. echo task $1 seq: $PARALLEL_SEQ sleep: $sleepsecs host: $( hostname ) date: $( date ) # Sleep a random amount of time. sleep $sleepsecs To submit this job, copy both parallel.sbatch and runtask.sh to the same directory, and run chmod +x runtask.sh to make runtask.sh executable. Then the job can be submitted to the Slurm queue: sbatch parallel.sbatch When this job completes, you should see output files with names runtask.sh.N , where N is a number between 1 and 128. The content of the first output file (i.e., runtask.sh.1 ) should look something like this: task arg1:1 seq:1 sleep:14s host:midway2-0002 date:Thu Jan 10 09:17:36 CST 2017 Another file runtask.log is also created. It gives a list of the completed jobs. (Note: If the sbatch is submitted again, nothing will be run until runtask.log is removed.) It is also possible to use this same technique to run multithreaded tasks in parallel. Here is an example sbatch script, parallel-hybrid.sbatch , that distributes multithreaded computations (each using 28 CPUs) across 2 nodes: #!/bin/sh #SBATCH --partition=broadwl #SBATCH --time=01:00:00 #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=28 #SBATCH --exclusive # Load the default version of GNU parallel. module load parallel srun = \"srun --exclusive -N1 -n1 --cpus-per-task $SLURM_CPUS_PER_TASK \" # Instead of $SLURM_NTASKS, use $SLURM_NNODES to determine how # many jobs should be run simultaneously. parallel = \"parallel --delay 0.2 -j $SLURM_NNODES --joblog runtask.log --resume\" # Run the parallel command. $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..6 } MPI jobs Here we present a couple small examples illustrating how to use MPI for distributed computing Midway. For more information on the MPI libraries available on Midway, see Message Passing Interface (MPI) . Our main illustration is a \u201chello world\u201d example. See file hellompi.c for the C code we will compile and run. #include <stdio.h> #include <stdlib.h> #include <mpi.h> int main ( int argc , char * argv [], char * envp []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); printf ( \"Process %d on %s out of %d \\n \" , rank , processor_name , numprocs ); MPI_Finalize (); } Copy hellompi.c to your home directory on Midway, load the default OpenMPI module, and compile the program on a Midway login node: module load openmpi mpicc hellompi.c -o hellompi hellompi.sbatch is a submission script that can be used to submit a job to Midway to run the hellompi program: #!/bin/bash #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out #SBATCH --ntasks=56 #SBATCH --partition=broadwl #SBATCH --constraint=fdr # Load the default OpenMPI module. module load openmpi # Run the hellompi program with mpirun. The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi Submit the MPI job to the Slurm job scheduler from a Midway login node: sbatch hellompi.sbatch Here is an example output: Process 1 on midway2-0172.rcc.local out of 56 Process 3 on midway2-0172.rcc.local out of 56 Process 5 on midway2-0172.rcc.local out of 56 Process 7 on midway2-0172.rcc.local out of 56 Process 6 on midway2-0172.rcc.local out of 56 Process 9 on midway2-0172.rcc.local out of 56 Process 10 on midway2-0172.rcc.local out of 56 Process 11 on midway2-0172.rcc.local out of 56 Process 13 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0172.rcc.local out of 56 Process 2 on midway2-0172.rcc.local out of 56 Process 4 on midway2-0172.rcc.local out of 56 Process 8 on midway2-0172.rcc.local out of 56 Process 12 on midway2-0172.rcc.local out of 56 Process 17 on midway2-0173.rcc.local out of 56 Process 19 on midway2-0173.rcc.local out of 56 Process 21 on midway2-0173.rcc.local out of 56 Process 28 on midway2-0173.rcc.local out of 56 Process 29 on midway2-0173.rcc.local out of 56 Process 31 on midway2-0173.rcc.local out of 56 Process 32 on midway2-0173.rcc.local out of 56 Process 15 on midway2-0173.rcc.local out of 56 Process 18 on midway2-0173.rcc.local out of 56 Process 20 on midway2-0173.rcc.local out of 56 Process 22 on midway2-0173.rcc.local out of 56 Process 23 on midway2-0173.rcc.local out of 56 Process 24 on midway2-0173.rcc.local out of 56 Process 52 on midway2-0175.rcc.local out of 56 Process 25 on midway2-0173.rcc.local out of 56 Process 53 on midway2-0175.rcc.local out of 56 Process 26 on midway2-0173.rcc.local out of 56 Process 54 on midway2-0175.rcc.local out of 56 Process 27 on midway2-0173.rcc.local out of 56 Process 55 on midway2-0175.rcc.local out of 56 Process 14 on midway2-0173.rcc.local out of 56 Process 16 on midway2-0173.rcc.local out of 56 Process 30 on midway2-0173.rcc.local out of 56 Process 43 on midway2-0174.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 48 on midway2-0174.rcc.local out of 56 Process 49 on midway2-0174.rcc.local out of 56 Process 46 on midway2-0174.rcc.local out of 56 Process 44 on midway2-0174.rcc.local out of 56 Process 45 on midway2-0174.rcc.local out of 56 Process 40 on midway2-0174.rcc.local out of 56 Process 42 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0174.rcc.local out of 56 Process 37 on midway2-0174.rcc.local out of 56 Process 47 on midway2-0174.rcc.local out of 56 Process 34 on midway2-0174.rcc.local out of 56 Process 38 on midway2-0174.rcc.local out of 56 Process 35 on midway2-0174.rcc.local out of 56 Process 39 on midway2-0174.rcc.local out of 56 Process 41 on midway2-0174.rcc.local out of 56 Process 50 on midway2-0174.rcc.local out of 56 Process 33 on midway2-0174.rcc.local out of 56 From this output, we observe that the computation is distributed on 4 different nodes, with many threads running simultaneously on the same node. When developing your own sbatch script for MPI-based computations, keep in mind the following points. Without the --constraint=fdr or --constraint=edr options, your computations may run on nodes with FDR, EDR, or combination of both. It is possible to control the number of tasks that are run per node with the --ntasks-per-node option. For example, submitting the job like this to Midway: sbatch --ntasks-per-node=1 hellompi.sbatch results in each thread running on a different node: Process 52 on midway2-0175.rcc.local out of 56 Process 50 on midway2-0173.rcc.local out of 56 Process 49 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0002.rcc.local out of 56 Process 13 on midway2-0052.rcc.local out of 56 Process 2 on midway2-0013.rcc.local out of 56 Process 9 on midway2-0033.rcc.local out of 56 Process 54 on midway2-0177.rcc.local out of 56 Process 53 on midway2-0176.rcc.local out of 56 Process 44 on midway2-0157.rcc.local out of 56 Process 55 on midway2-0178.rcc.local out of 56 Process 11 on midway2-0041.rcc.local out of 56 Process 40 on midway2-0152.rcc.local out of 56 Process 43 on midway2-0156.rcc.local out of 56 Process 45 on midway2-0161.rcc.local out of 56 Process 46 on midway2-0162.rcc.local out of 56 Process 38 on midway2-0147.rcc.local out of 56 Process 39 on midway2-0148.rcc.local out of 56 Process 24 on midway2-0101.rcc.local out of 56 Process 3 on midway2-0014.rcc.local out of 56 Process 42 on midway2-0155.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0145.rcc.local out of 56 Process 25 on midway2-0102.rcc.local out of 56 Process 16 on midway2-0055.rcc.local out of 56 Process 18 on midway2-0057.rcc.local out of 56 Process 12 on midway2-0051.rcc.local out of 56 Process 14 on midway2-0053.rcc.local out of 56 Process 4 on midway2-0015.rcc.local out of 56 Process 15 on midway2-0054.rcc.local out of 56 Process 48 on midway2-0164.rcc.local out of 56 Process 28 on midway2-0105.rcc.local out of 56 Process 22 on midway2-0097.rcc.local out of 56 Process 30 on midway2-0118.rcc.local out of 56 Process 1 on midway2-0012.rcc.local out of 56 Process 35 on midway2-0144.rcc.local out of 56 Process 17 on midway2-0056.rcc.local out of 56 Process 37 on midway2-0146.rcc.local out of 56 Process 32 on midway2-0141.rcc.local out of 56 Process 34 on midway2-0143.rcc.local out of 56 Process 27 on midway2-0104.rcc.local out of 56 Process 31 on midway2-0119.rcc.local out of 56 Process 29 on midway2-0106.rcc.local out of 56 Process 10 on midway2-0034.rcc.local out of 56 Process 47 on midway2-0163.rcc.local out of 56 Process 33 on midway2-0142.rcc.local out of 56 Process 26 on midway2-0103.rcc.local out of 56 Process 7 on midway2-0019.rcc.local out of 56 Process 20 on midway2-0073.rcc.local out of 56 Process 5 on midway2-0016.rcc.local out of 56 Process 8 on midway2-0027.rcc.local out of 56 Process 41 on midway2-0153.rcc.local out of 56 Process 6 on midway2-0017.rcc.local out of 56 Process 21 on midway2-0096.rcc.local out of 56 Process 19 on midway2-0072.rcc.local out of 56 Process 23 on midway2-0100.rcc.local out of 56 Additional notes Both OpenMPI and IntelMPI have the ability to launch MPI programs directly with the Slurm command srun . It is not necessary to use this mode for most jobs, but it may provide additional job launch options. For example, from a Midway login node it is possible to launch the above hellompi program using OpenMPI using 28 MPI processes: srun -n28 hellompi With IntelMPI, you need to also set an environment variable for this to work: export I_MPI_PMI_LIBRARY=/software/slurm-current-$DISTARCH/lib/libpmi.so srun -n28 hellompi Hybrid MPI/OpenMP jobs MPI and OpenMP can be used at the same time to create a Hybrid MPI/OpenMP program. Let\u2019s look at an example Hybrid MPI/OpenMP hello world program and explain the steps needed to compile and submit it to the queue. An example hybrid MPI hello world program: hellohybrid.c #include <stdio.h> #include <omp.h> #include \"mpi.h\" int main ( int argc , char * argv []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; int iam = 0 , np = 1 ; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); #pragma omp parallel default(shared) private(iam, np) { np = omp_get_num_threads (); iam = omp_get_thread_num (); printf ( \"Hello from thread %d out of %d from process %d out of %d on %s \\n \" , iam , np , rank , numprocs , processor_name ); } MPI_Finalize (); } To run the program on the RCC cluster, copy hellohybrid.c and hellohybrid.sbatch to your home directory, then compile the code interactively by entering the following commands into a terminal on a Midway2 login node: module load openmpi mpicc -fopenmp hellohybrid.c -o hellohybrid Here we load the default MPI compiler, but it should be possible to use any available MPI compiler to compile and run this example. Note that the option -fopenmp must be used here to compile the program because the code includes OpenMP directives (use -openmp for the Intel compiler and -mp for the PGI compiler). hellohybrid.sbatch is a submission script that can be used to submit a job to Midway2 to run the hellohybrid program. #!/bin/bash # A job submission script for running a hybrid MPI/OpenMP job on # Midway2. #SBATCH --job-name=hellohybrid #SBATCH --output=hellohybrid.out #SBATCH --ntasks=4 #SBATCH --cpus-per-task=8 #SBATCH --partition=broadwl #SBATCH --constraint=edr # Load the default OpenMPI module. module load openmpi # Set OMP_NUM_THREADS to the number of CPUs per task we asked for. export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK # Run the process with mpirun. Note that the -n option is not required # in this case; mpirun will automatically determine how many processes # to run from the Slurm settings. mpirun ./hellohybrid The options are similar to running an MPI job, with some differences: * ` --ntasks = 4 ` specifies the number of MPI processes ( \u201ctasks\u201d ) . * ` --cpus-per-task = 8 ` allocates 8 CPUs for each task. * ` export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ` sets the number of OpenMP threads to the number of requested cores ( CPUs ) for each task. You can submit ` hellohybrid.sbatch ` using the following command from one of Midway2 login nodes: ``` default sbatch hellohybrid.sbatch Here is an example output of this program submitted to the broadwl partition on Midway2: Hello from thread 0 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 7 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 4 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 6 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 2 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 5 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 1 out of 8 from process 3 out of 4 on midway2-0270.rcc.local GPU jobs The gpu2 partition is dedicated to software that uses Graphical Processing Units (GPUs). See Types of Compute Nodes for technical specifications of the gpu2 nodes. CUDA and OpenACC are two widely used tools for developing GPU-based software. For information on compiling code with CUDA and OpenACC, see CUDA and OpenACC Compilers . Running GPU code on Midway2 To submit a job to one of the GPU nodes, you must include the following lines in your sbatch script: #SBATCH --partition=gpu2 #SBATCH --gres=gpu:N where N is the number of GPUs requested. Allowable settings for N range from 1 to 4. If your application is entirely GPU driven, then you do not need to explicilty request cores as one CPU core will be assigned by default to act as a master to launch the GPU based calculation. If however your application is mixed CPU-GPU then you will need to request the number of cores with \u2013ntasks as is required by your job. Here is an example sbatch script that allocates GPU resources and loads the CUDA compilers (see also gpu.sbatch ): #!/bin/bash #SBATCH --job-name=gpu # job name #SBATCH --output=gpu.out # output log file #SBATCH --error=gpu.err # error file #SBATCH --time=01:00:00 # 1 hour of wall time #SBATCH --nodes=1 # 1 GPU node #SBATCH --partition=gpu2 # GPU2 partition #SBATCH --ntasks=1 # 1 CPU core to drive GPU #SBATCH --gres=gpu:1 # Request 1 GPU # Load all required modules below. As an example we load cuda/9.1 module load cuda/9.1 # Add lines here to run your GPU-based computations. Cron-like jobs Cron jobs persist until they are canceled or encounter an error. The Midway2 cluster has a dedicated partition, cron , for running Cron jobs. Please email help@rcc.uchicago.edu to request submitting Cron-like jobs. These jobs are subject to scheduling limits and will be monitored. Here is an example of an sbatch script that runs a Cron job (see also cron.sbatch ): #!/bin/bash #SBATCH --time=00:05:00 #SBATCH --output=cron.log #SBATCH --open-mode=append #SBATCH --account=cron-account #SBATCH --partition=cron #SBATCH --qos=cron # Specify a valid Cron string for the schedule. This specifies that # the Cron job run once per day at 5:15a. SCHEDULE = '15 5 * * *' # Here is an example of a simple command that prints the host name and # the date and time. echo \"Hello on $( hostname ) at $( date ) .\" # This schedules the next run. sbatch --quiet --begin = $( next-cron-time \" $SCHEDULE \" ) cron.sbatch After executing a simple command (print the host name, date and time), the script schedules the next run with another call to sbatch with the --begin option.","title":"Example Job Submission Scripts"},{"location":"midway23/examples/example_job_scripts/#illustrative-examples","text":"Below are a number of example submission scripts that you can adapt to run your jobs on Midway. See also the materials from the RCC Slurm workshop for additional examples.","title":"Illustrative examples"},{"location":"midway23/examples/example_job_scripts/#job-arrays","text":"Slurm job arrays provide a convenient way to submit a large number of independent processing jobs. For example, Slurm job arrays can be useful for applying the same or similar computation to a collection of data sets. When a job array script is submitted, a specified number of \u201carray tasks\u201d are created based on the \u201cmaster\u201d sbatch script. Consider the following example (from array.sbatch ): #!/bin/bash #SBATCH --job-name=array #SBATCH --output=array_%A_%a.out #SBATCH --error=array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=1 #SBATCH --mem=4G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. In this simple example, --array=1-16 requests 16 array tasks (numbered 1 through 16). The \u201carray tasks\u201d are copies of the master script that are automatically submitted to the scheduler on your behalf. In each array task, the environment variable SLURM_ARRAY_TASK_ID is set to a unique value (in this example, numbers ranging from 1 to 16). Job array indices can be specified in several different ways. Here are some examples: # A job array with array tasks numbered from 0 to 31. #SBATCH --array=0-31 # A job array with array tasks numbered 1, 2, 5, 19, 27. #SBATCH --array=1,2,5,19,27 # A job array with array tasks numbered 1, 3, 5 and 7. #SBATCH --array=1-7:2 In the example sbatch script above, the %A_%a notation is filled in with the master job id ( %A ) and the array task id ( %a ). This is a simple way to create output files in which the file name is different for each job in the array. The remaining options in the sbatch script are the same as the options used in other, non-array settings; in this example, we are requesting that each array task be allocated 1 CPU ( --ntasks=1 ) and 4 GB of memory ( --mem=4G ) on the broadwl partition ( --partition=broadwl ) for up to one hour ( --time=01:00:00 ). Most partitions have limits on the number of array tasks that can run simultaneously. To achieve a higher throughput, consider Parallel batch jobs . For more information about Slurm job arrays, consule the the `Slurm documentation on job arrays`_","title":"Job arrays"},{"location":"midway23/examples/example_job_scripts/#large-memory-jobs","text":"The bigmem2 partition is particularly well suited for computations that need more than about 60 GB of memory; see Types of Compute Nodes for technical specifications of the bigmem2 nodes.","title":"Large-memory jobs"},{"location":"midway23/examples/example_job_scripts/#running-a-large-memory-job","text":"To submit a job to bigmem2, include this line in your sbatch script: #SBATCH --partition=bigmem2 Additionally, it is important to use the --mem or --mem-per-cpu options. For example, to request 8 CPU cores and 128 GB of memory on a bigmem2 node, add the following to your sbatch script: #SBATCH --ntasks=1 #SBATCH --cpus-per-task=8 #SBATCH --mem=128G","title":"Running a large-memory job"},{"location":"midway23/examples/example_job_scripts/#interactive-computing-on-bigmem2","text":"These same options can also be used to set up an sinteractive session. For example, to access a bigmem2 node with 1 CPU and 128 GB of memory, run: sinteractive --partition = bigmem2 --ntasks = 1 --cpus-per-task = 8 --mem = 128G","title":"Interactive computing on bigmem2"},{"location":"midway23/examples/example_job_scripts/#parallel-batch-jobs","text":"Computations involving a very large number of independent computations should be combined in some way to reduce the number of jobs submitted to Slurm. Here we illustrate one strategy for doing this using GNU Parallel and srun . The parallel program executes tasks simultaneously until all tasks have been completed. Here\u2019s an example script, parallel.sbatch : #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=28 #SBATCH --mem-per-cpu=2G # NOTE DO NOT USE THE --mem= OPTION # Load the default version of GNU parallel. module load parallel # When running a large number of tasks simultaneously, it may be # necessary to increase the user process limit. ulimit -u 10000 # This specifies the options used to run srun. The \"-N1 -n1\" options are # used to allocates a single core to each task. srun = \"srun --exclusive -N1 -n1\" # This specifies the options used to run GNU parallel: # # --delay of 0.2 prevents overloading the controlling node. # # -j is the number of tasks run simultaneously. # # The combination of --joblog and --resume create a task log that # can be used to monitor progress. # parallel = \"parallel --delay 0.2 -j $SLURM_NTASKS --joblog runtask.log --resume\" # Run a script, runtask.sh, using GNU parallel and srun. Parallel # will run the runtask script for the numbers 1 through 128. To # illustrate, the first job will run like this: # # srun --exclusive -N1 -n1 ./runtask.sh arg1:1 > runtask.1 # $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..128 } # Note that if your program does not take any input, use the -n0 option to # call the parallel command: # # $parallel -n0 \"$srun ./run_noinput_task.sh > output.{1}\" ::: {1..128} In this example, our aim is to run script runtask.sh 128 times. The --ntasks option is set to 28, so at most 28 tasks can be run simultaneously. Here is the runtask.sh script that is run by GNU parallel: #!/bin/sh # This script outputs some useful information so we can see what parallel # and srun are doing. sleepsecs = $ [ ( $RANDOM % 10 ) + 10 ] s # $1 is arg1:{1} from GNU parallel. # # $PARALLEL_SEQ is a special variable from GNU parallel. It gives the # number of the job in the sequence. # # Here we print the sleep time, host name, and the date and time. echo task $1 seq: $PARALLEL_SEQ sleep: $sleepsecs host: $( hostname ) date: $( date ) # Sleep a random amount of time. sleep $sleepsecs To submit this job, copy both parallel.sbatch and runtask.sh to the same directory, and run chmod +x runtask.sh to make runtask.sh executable. Then the job can be submitted to the Slurm queue: sbatch parallel.sbatch When this job completes, you should see output files with names runtask.sh.N , where N is a number between 1 and 128. The content of the first output file (i.e., runtask.sh.1 ) should look something like this: task arg1:1 seq:1 sleep:14s host:midway2-0002 date:Thu Jan 10 09:17:36 CST 2017 Another file runtask.log is also created. It gives a list of the completed jobs. (Note: If the sbatch is submitted again, nothing will be run until runtask.log is removed.) It is also possible to use this same technique to run multithreaded tasks in parallel. Here is an example sbatch script, parallel-hybrid.sbatch , that distributes multithreaded computations (each using 28 CPUs) across 2 nodes: #!/bin/sh #SBATCH --partition=broadwl #SBATCH --time=01:00:00 #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=28 #SBATCH --exclusive # Load the default version of GNU parallel. module load parallel srun = \"srun --exclusive -N1 -n1 --cpus-per-task $SLURM_CPUS_PER_TASK \" # Instead of $SLURM_NTASKS, use $SLURM_NNODES to determine how # many jobs should be run simultaneously. parallel = \"parallel --delay 0.2 -j $SLURM_NNODES --joblog runtask.log --resume\" # Run the parallel command. $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..6 }","title":"Parallel batch jobs"},{"location":"midway23/examples/example_job_scripts/#mpi-jobs","text":"Here we present a couple small examples illustrating how to use MPI for distributed computing Midway. For more information on the MPI libraries available on Midway, see Message Passing Interface (MPI) . Our main illustration is a \u201chello world\u201d example. See file hellompi.c for the C code we will compile and run. #include <stdio.h> #include <stdlib.h> #include <mpi.h> int main ( int argc , char * argv [], char * envp []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); printf ( \"Process %d on %s out of %d \\n \" , rank , processor_name , numprocs ); MPI_Finalize (); } Copy hellompi.c to your home directory on Midway, load the default OpenMPI module, and compile the program on a Midway login node: module load openmpi mpicc hellompi.c -o hellompi hellompi.sbatch is a submission script that can be used to submit a job to Midway to run the hellompi program: #!/bin/bash #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out #SBATCH --ntasks=56 #SBATCH --partition=broadwl #SBATCH --constraint=fdr # Load the default OpenMPI module. module load openmpi # Run the hellompi program with mpirun. The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi Submit the MPI job to the Slurm job scheduler from a Midway login node: sbatch hellompi.sbatch Here is an example output: Process 1 on midway2-0172.rcc.local out of 56 Process 3 on midway2-0172.rcc.local out of 56 Process 5 on midway2-0172.rcc.local out of 56 Process 7 on midway2-0172.rcc.local out of 56 Process 6 on midway2-0172.rcc.local out of 56 Process 9 on midway2-0172.rcc.local out of 56 Process 10 on midway2-0172.rcc.local out of 56 Process 11 on midway2-0172.rcc.local out of 56 Process 13 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0172.rcc.local out of 56 Process 2 on midway2-0172.rcc.local out of 56 Process 4 on midway2-0172.rcc.local out of 56 Process 8 on midway2-0172.rcc.local out of 56 Process 12 on midway2-0172.rcc.local out of 56 Process 17 on midway2-0173.rcc.local out of 56 Process 19 on midway2-0173.rcc.local out of 56 Process 21 on midway2-0173.rcc.local out of 56 Process 28 on midway2-0173.rcc.local out of 56 Process 29 on midway2-0173.rcc.local out of 56 Process 31 on midway2-0173.rcc.local out of 56 Process 32 on midway2-0173.rcc.local out of 56 Process 15 on midway2-0173.rcc.local out of 56 Process 18 on midway2-0173.rcc.local out of 56 Process 20 on midway2-0173.rcc.local out of 56 Process 22 on midway2-0173.rcc.local out of 56 Process 23 on midway2-0173.rcc.local out of 56 Process 24 on midway2-0173.rcc.local out of 56 Process 52 on midway2-0175.rcc.local out of 56 Process 25 on midway2-0173.rcc.local out of 56 Process 53 on midway2-0175.rcc.local out of 56 Process 26 on midway2-0173.rcc.local out of 56 Process 54 on midway2-0175.rcc.local out of 56 Process 27 on midway2-0173.rcc.local out of 56 Process 55 on midway2-0175.rcc.local out of 56 Process 14 on midway2-0173.rcc.local out of 56 Process 16 on midway2-0173.rcc.local out of 56 Process 30 on midway2-0173.rcc.local out of 56 Process 43 on midway2-0174.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 48 on midway2-0174.rcc.local out of 56 Process 49 on midway2-0174.rcc.local out of 56 Process 46 on midway2-0174.rcc.local out of 56 Process 44 on midway2-0174.rcc.local out of 56 Process 45 on midway2-0174.rcc.local out of 56 Process 40 on midway2-0174.rcc.local out of 56 Process 42 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0174.rcc.local out of 56 Process 37 on midway2-0174.rcc.local out of 56 Process 47 on midway2-0174.rcc.local out of 56 Process 34 on midway2-0174.rcc.local out of 56 Process 38 on midway2-0174.rcc.local out of 56 Process 35 on midway2-0174.rcc.local out of 56 Process 39 on midway2-0174.rcc.local out of 56 Process 41 on midway2-0174.rcc.local out of 56 Process 50 on midway2-0174.rcc.local out of 56 Process 33 on midway2-0174.rcc.local out of 56 From this output, we observe that the computation is distributed on 4 different nodes, with many threads running simultaneously on the same node. When developing your own sbatch script for MPI-based computations, keep in mind the following points. Without the --constraint=fdr or --constraint=edr options, your computations may run on nodes with FDR, EDR, or combination of both. It is possible to control the number of tasks that are run per node with the --ntasks-per-node option. For example, submitting the job like this to Midway: sbatch --ntasks-per-node=1 hellompi.sbatch results in each thread running on a different node: Process 52 on midway2-0175.rcc.local out of 56 Process 50 on midway2-0173.rcc.local out of 56 Process 49 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0002.rcc.local out of 56 Process 13 on midway2-0052.rcc.local out of 56 Process 2 on midway2-0013.rcc.local out of 56 Process 9 on midway2-0033.rcc.local out of 56 Process 54 on midway2-0177.rcc.local out of 56 Process 53 on midway2-0176.rcc.local out of 56 Process 44 on midway2-0157.rcc.local out of 56 Process 55 on midway2-0178.rcc.local out of 56 Process 11 on midway2-0041.rcc.local out of 56 Process 40 on midway2-0152.rcc.local out of 56 Process 43 on midway2-0156.rcc.local out of 56 Process 45 on midway2-0161.rcc.local out of 56 Process 46 on midway2-0162.rcc.local out of 56 Process 38 on midway2-0147.rcc.local out of 56 Process 39 on midway2-0148.rcc.local out of 56 Process 24 on midway2-0101.rcc.local out of 56 Process 3 on midway2-0014.rcc.local out of 56 Process 42 on midway2-0155.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0145.rcc.local out of 56 Process 25 on midway2-0102.rcc.local out of 56 Process 16 on midway2-0055.rcc.local out of 56 Process 18 on midway2-0057.rcc.local out of 56 Process 12 on midway2-0051.rcc.local out of 56 Process 14 on midway2-0053.rcc.local out of 56 Process 4 on midway2-0015.rcc.local out of 56 Process 15 on midway2-0054.rcc.local out of 56 Process 48 on midway2-0164.rcc.local out of 56 Process 28 on midway2-0105.rcc.local out of 56 Process 22 on midway2-0097.rcc.local out of 56 Process 30 on midway2-0118.rcc.local out of 56 Process 1 on midway2-0012.rcc.local out of 56 Process 35 on midway2-0144.rcc.local out of 56 Process 17 on midway2-0056.rcc.local out of 56 Process 37 on midway2-0146.rcc.local out of 56 Process 32 on midway2-0141.rcc.local out of 56 Process 34 on midway2-0143.rcc.local out of 56 Process 27 on midway2-0104.rcc.local out of 56 Process 31 on midway2-0119.rcc.local out of 56 Process 29 on midway2-0106.rcc.local out of 56 Process 10 on midway2-0034.rcc.local out of 56 Process 47 on midway2-0163.rcc.local out of 56 Process 33 on midway2-0142.rcc.local out of 56 Process 26 on midway2-0103.rcc.local out of 56 Process 7 on midway2-0019.rcc.local out of 56 Process 20 on midway2-0073.rcc.local out of 56 Process 5 on midway2-0016.rcc.local out of 56 Process 8 on midway2-0027.rcc.local out of 56 Process 41 on midway2-0153.rcc.local out of 56 Process 6 on midway2-0017.rcc.local out of 56 Process 21 on midway2-0096.rcc.local out of 56 Process 19 on midway2-0072.rcc.local out of 56 Process 23 on midway2-0100.rcc.local out of 56","title":"MPI jobs"},{"location":"midway23/examples/example_job_scripts/#additional-notes","text":"Both OpenMPI and IntelMPI have the ability to launch MPI programs directly with the Slurm command srun . It is not necessary to use this mode for most jobs, but it may provide additional job launch options. For example, from a Midway login node it is possible to launch the above hellompi program using OpenMPI using 28 MPI processes: srun -n28 hellompi With IntelMPI, you need to also set an environment variable for this to work: export I_MPI_PMI_LIBRARY=/software/slurm-current-$DISTARCH/lib/libpmi.so srun -n28 hellompi","title":"Additional notes"},{"location":"midway23/examples/example_job_scripts/#hybrid-mpiopenmp-jobs","text":"MPI and OpenMP can be used at the same time to create a Hybrid MPI/OpenMP program. Let\u2019s look at an example Hybrid MPI/OpenMP hello world program and explain the steps needed to compile and submit it to the queue. An example hybrid MPI hello world program: hellohybrid.c #include <stdio.h> #include <omp.h> #include \"mpi.h\" int main ( int argc , char * argv []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; int iam = 0 , np = 1 ; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); #pragma omp parallel default(shared) private(iam, np) { np = omp_get_num_threads (); iam = omp_get_thread_num (); printf ( \"Hello from thread %d out of %d from process %d out of %d on %s \\n \" , iam , np , rank , numprocs , processor_name ); } MPI_Finalize (); } To run the program on the RCC cluster, copy hellohybrid.c and hellohybrid.sbatch to your home directory, then compile the code interactively by entering the following commands into a terminal on a Midway2 login node: module load openmpi mpicc -fopenmp hellohybrid.c -o hellohybrid Here we load the default MPI compiler, but it should be possible to use any available MPI compiler to compile and run this example. Note that the option -fopenmp must be used here to compile the program because the code includes OpenMP directives (use -openmp for the Intel compiler and -mp for the PGI compiler). hellohybrid.sbatch is a submission script that can be used to submit a job to Midway2 to run the hellohybrid program. #!/bin/bash # A job submission script for running a hybrid MPI/OpenMP job on # Midway2. #SBATCH --job-name=hellohybrid #SBATCH --output=hellohybrid.out #SBATCH --ntasks=4 #SBATCH --cpus-per-task=8 #SBATCH --partition=broadwl #SBATCH --constraint=edr # Load the default OpenMPI module. module load openmpi # Set OMP_NUM_THREADS to the number of CPUs per task we asked for. export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK # Run the process with mpirun. Note that the -n option is not required # in this case; mpirun will automatically determine how many processes # to run from the Slurm settings. mpirun ./hellohybrid The options are similar to running an MPI job, with some differences: * ` --ntasks = 4 ` specifies the number of MPI processes ( \u201ctasks\u201d ) . * ` --cpus-per-task = 8 ` allocates 8 CPUs for each task. * ` export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ` sets the number of OpenMP threads to the number of requested cores ( CPUs ) for each task. You can submit ` hellohybrid.sbatch ` using the following command from one of Midway2 login nodes: ``` default sbatch hellohybrid.sbatch Here is an example output of this program submitted to the broadwl partition on Midway2: Hello from thread 0 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 7 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 4 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 6 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 2 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 5 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 1 out of 8 from process 3 out of 4 on midway2-0270.rcc.local","title":"Hybrid MPI/OpenMP jobs"},{"location":"midway23/examples/example_job_scripts/#gpu-jobs","text":"The gpu2 partition is dedicated to software that uses Graphical Processing Units (GPUs). See Types of Compute Nodes for technical specifications of the gpu2 nodes. CUDA and OpenACC are two widely used tools for developing GPU-based software. For information on compiling code with CUDA and OpenACC, see CUDA and OpenACC Compilers .","title":"GPU jobs"},{"location":"midway23/examples/example_job_scripts/#running-gpu-code-on-midway2","text":"To submit a job to one of the GPU nodes, you must include the following lines in your sbatch script: #SBATCH --partition=gpu2 #SBATCH --gres=gpu:N where N is the number of GPUs requested. Allowable settings for N range from 1 to 4. If your application is entirely GPU driven, then you do not need to explicilty request cores as one CPU core will be assigned by default to act as a master to launch the GPU based calculation. If however your application is mixed CPU-GPU then you will need to request the number of cores with \u2013ntasks as is required by your job. Here is an example sbatch script that allocates GPU resources and loads the CUDA compilers (see also gpu.sbatch ): #!/bin/bash #SBATCH --job-name=gpu # job name #SBATCH --output=gpu.out # output log file #SBATCH --error=gpu.err # error file #SBATCH --time=01:00:00 # 1 hour of wall time #SBATCH --nodes=1 # 1 GPU node #SBATCH --partition=gpu2 # GPU2 partition #SBATCH --ntasks=1 # 1 CPU core to drive GPU #SBATCH --gres=gpu:1 # Request 1 GPU # Load all required modules below. As an example we load cuda/9.1 module load cuda/9.1 # Add lines here to run your GPU-based computations.","title":"Running GPU code on Midway2"},{"location":"midway23/examples/example_job_scripts/#cron-like-jobs","text":"Cron jobs persist until they are canceled or encounter an error. The Midway2 cluster has a dedicated partition, cron , for running Cron jobs. Please email help@rcc.uchicago.edu to request submitting Cron-like jobs. These jobs are subject to scheduling limits and will be monitored. Here is an example of an sbatch script that runs a Cron job (see also cron.sbatch ): #!/bin/bash #SBATCH --time=00:05:00 #SBATCH --output=cron.log #SBATCH --open-mode=append #SBATCH --account=cron-account #SBATCH --partition=cron #SBATCH --qos=cron # Specify a valid Cron string for the schedule. This specifies that # the Cron job run once per day at 5:15a. SCHEDULE = '15 5 * * *' # Here is an example of a simple command that prints the host name and # the date and time. echo \"Hello on $( hostname ) at $( date ) .\" # This schedules the next run. sbatch --quiet --begin = $( next-cron-time \" $SCHEDULE \" ) cron.sbatch After executing a simple command (print the host name, date and time), the script schedules the next run with another call to sbatch with the --begin option.","title":"Cron-like jobs"},{"location":"midway23/examples/gpu/","text":"GPU jobs The gpu2 partition is dedicated to software that uses Graphical Processing Units (GPUs). See Types of Compute Nodes for technical specifications of the gpu2 nodes. CUDA and OpenACC are two widely used tools for developing GPU-based software. For information on compiling code with CUDA and OpenACC, see CUDA and OpenACC Compilers . Running GPU code on Midway2 To submit a job to one of the GPU nodes, you must include the following lines in your sbatch script: #SBATCH --partition=gpu2 #SBATCH --gres=gpu:N where N is the number of GPUs requested. Allowable settings for N range from 1 to 4. If your application is entirely GPU driven, then you do not need to explicilty request cores as one CPU core will be assigned by default to act as a master to launch the GPU based calculation. If however your application is mixed CPU-GPU then you will need to request the number of cores with \u2013ntasks as is required by your job. Here is an example sbatch script that allocates GPU resources and loads the CUDA compilers (see also gpu.sbatch ): #!/bin/bash #SBATCH --job-name=gpu # job name #SBATCH --output=gpu.out # output log file #SBATCH --error=gpu.err # error file #SBATCH --time=01:00:00 # 1 hour of wall time #SBATCH --nodes=1 # 1 GPU node #SBATCH --partition=gpu2 # GPU2 partition #SBATCH --ntasks=1 # 1 CPU core to drive GPU #SBATCH --gres=gpu:1 # Request 1 GPU # Load all required modules below. As an example we load cuda/9.1 module load cuda/9.1 # Add lines here to run your GPU-based computations.","title":"GPU jobs"},{"location":"midway23/examples/gpu/#gpu-jobs","text":"The gpu2 partition is dedicated to software that uses Graphical Processing Units (GPUs). See Types of Compute Nodes for technical specifications of the gpu2 nodes. CUDA and OpenACC are two widely used tools for developing GPU-based software. For information on compiling code with CUDA and OpenACC, see CUDA and OpenACC Compilers .","title":"GPU jobs"},{"location":"midway23/examples/gpu/#running-gpu-code-on-midway2","text":"To submit a job to one of the GPU nodes, you must include the following lines in your sbatch script: #SBATCH --partition=gpu2 #SBATCH --gres=gpu:N where N is the number of GPUs requested. Allowable settings for N range from 1 to 4. If your application is entirely GPU driven, then you do not need to explicilty request cores as one CPU core will be assigned by default to act as a master to launch the GPU based calculation. If however your application is mixed CPU-GPU then you will need to request the number of cores with \u2013ntasks as is required by your job. Here is an example sbatch script that allocates GPU resources and loads the CUDA compilers (see also gpu.sbatch ): #!/bin/bash #SBATCH --job-name=gpu # job name #SBATCH --output=gpu.out # output log file #SBATCH --error=gpu.err # error file #SBATCH --time=01:00:00 # 1 hour of wall time #SBATCH --nodes=1 # 1 GPU node #SBATCH --partition=gpu2 # GPU2 partition #SBATCH --ntasks=1 # 1 CPU core to drive GPU #SBATCH --gres=gpu:1 # Request 1 GPU # Load all required modules below. As an example we load cuda/9.1 module load cuda/9.1 # Add lines here to run your GPU-based computations.","title":"Running GPU code on Midway2"},{"location":"midway23/examples/hybrid/","text":"Hybrid MPI/OpenMP jobs MPI and OpenMP can be used at the same time to create a Hybrid MPI/OpenMP program. Let\u2019s look at an example Hybrid MPI/OpenMP hello world program and explain the steps needed to compile and submit it to the queue. An example hybrid MPI hello world program: hellohybrid.c #include <stdio.h> #include <omp.h> #include \"mpi.h\" int main ( int argc , char * argv []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; int iam = 0 , np = 1 ; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); #pragma omp parallel default(shared) private(iam, np) { np = omp_get_num_threads (); iam = omp_get_thread_num (); printf ( \"Hello from thread %d out of %d from process %d out of %d on %s \\n \" , iam , np , rank , numprocs , processor_name ); } MPI_Finalize (); } To run the program on the RCC cluster, copy hellohybrid.c and hellohybrid.sbatch to your home directory, then compile the code interactively by entering the following commands into a terminal on a Midway2 login node: module load openmpi mpicc -fopenmp hellohybrid.c -o hellohybrid Here we load the default MPI compiler, but it should be possible to use any available MPI compiler to compile and run this example. Note that the option -fopenmp must be used here to compile the program because the code includes OpenMP directives (use -openmp for the Intel compiler and -mp for the PGI compiler). hellohybrid.sbatch is a submission script that can be used to submit a job to Midway2 to run the hellohybrid program. #!/bin/bash # A job submission script for running a hybrid MPI/OpenMP job on # Midway2. #SBATCH --job-name=hellohybrid #SBATCH --output=hellohybrid.out #SBATCH --ntasks=4 #SBATCH --cpus-per-task=8 #SBATCH --partition=broadwl #SBATCH --constraint=edr # Load the default OpenMPI module. module load openmpi # Set OMP_NUM_THREADS to the number of CPUs per task we asked for. export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK # Run the process with mpirun. Note that the -n option is not required # in this case; mpirun will automatically determine how many processes # to run from the Slurm settings. mpirun ./hellohybrid The options are similar to running an MPI job, with some differences: * ` --ntasks = 4 ` specifies the number of MPI processes ( \u201ctasks\u201d ) . * ` --cpus-per-task = 8 ` allocates 8 CPUs for each task. * ` export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ` sets the number of OpenMP threads to the number of requested cores ( CPUs ) for each task. You can submit ` hellohybrid.sbatch ` using the following command from one of Midway2 login nodes: ``` default sbatch hellohybrid.sbatch Here is an example output of this program submitted to the broadwl partition on Midway2: Hello from thread 0 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 7 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 4 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 6 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 2 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 5 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 1 out of 8 from process 3 out of 4 on midway2-0270.rcc.local","title":"Hybrid MPI/OpenMP jobs"},{"location":"midway23/examples/hybrid/#hybrid-mpiopenmp-jobs","text":"MPI and OpenMP can be used at the same time to create a Hybrid MPI/OpenMP program. Let\u2019s look at an example Hybrid MPI/OpenMP hello world program and explain the steps needed to compile and submit it to the queue. An example hybrid MPI hello world program: hellohybrid.c #include <stdio.h> #include <omp.h> #include \"mpi.h\" int main ( int argc , char * argv []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; int iam = 0 , np = 1 ; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); #pragma omp parallel default(shared) private(iam, np) { np = omp_get_num_threads (); iam = omp_get_thread_num (); printf ( \"Hello from thread %d out of %d from process %d out of %d on %s \\n \" , iam , np , rank , numprocs , processor_name ); } MPI_Finalize (); } To run the program on the RCC cluster, copy hellohybrid.c and hellohybrid.sbatch to your home directory, then compile the code interactively by entering the following commands into a terminal on a Midway2 login node: module load openmpi mpicc -fopenmp hellohybrid.c -o hellohybrid Here we load the default MPI compiler, but it should be possible to use any available MPI compiler to compile and run this example. Note that the option -fopenmp must be used here to compile the program because the code includes OpenMP directives (use -openmp for the Intel compiler and -mp for the PGI compiler). hellohybrid.sbatch is a submission script that can be used to submit a job to Midway2 to run the hellohybrid program. #!/bin/bash # A job submission script for running a hybrid MPI/OpenMP job on # Midway2. #SBATCH --job-name=hellohybrid #SBATCH --output=hellohybrid.out #SBATCH --ntasks=4 #SBATCH --cpus-per-task=8 #SBATCH --partition=broadwl #SBATCH --constraint=edr # Load the default OpenMPI module. module load openmpi # Set OMP_NUM_THREADS to the number of CPUs per task we asked for. export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK # Run the process with mpirun. Note that the -n option is not required # in this case; mpirun will automatically determine how many processes # to run from the Slurm settings. mpirun ./hellohybrid The options are similar to running an MPI job, with some differences: * ` --ntasks = 4 ` specifies the number of MPI processes ( \u201ctasks\u201d ) . * ` --cpus-per-task = 8 ` allocates 8 CPUs for each task. * ` export OMP_NUM_THREADS = $SLURM_CPUS_PER_TASK ` sets the number of OpenMP threads to the number of requested cores ( CPUs ) for each task. You can submit ` hellohybrid.sbatch ` using the following command from one of Midway2 login nodes: ``` default sbatch hellohybrid.sbatch Here is an example output of this program submitted to the broadwl partition on Midway2: Hello from thread 0 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 0 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 1 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 7 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 4 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 5 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 1 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 6 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 3 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 2 out of 8 from process 2 out of 4 on midway2-0269.rcc.local Hello from thread 0 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 7 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 4 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 6 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 2 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 5 out of 8 from process 3 out of 4 on midway2-0270.rcc.local Hello from thread 1 out of 8 from process 3 out of 4 on midway2-0270.rcc.local","title":"Hybrid MPI/OpenMP jobs"},{"location":"midway23/examples/mpi/","text":"MPI jobs Here we present a couple small examples illustrating how to use MPI for distributed computing Midway. For more information on the MPI libraries available on Midway, see Message Passing Interface (MPI) . Our main illustration is a \u201chello world\u201d example. See file hellompi.c for the C code we will compile and run. #include <stdio.h> #include <stdlib.h> #include <mpi.h> int main ( int argc , char * argv [], char * envp []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); printf ( \"Process %d on %s out of %d \\n \" , rank , processor_name , numprocs ); MPI_Finalize (); } Copy hellompi.c to your home directory on Midway, load the default OpenMPI module, and compile the program on a Midway login node: module load openmpi mpicc hellompi.c -o hellompi hellompi.sbatch is a submission script that can be used to submit a job to Midway to run the hellompi program: #!/bin/bash #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out #SBATCH --ntasks=56 #SBATCH --partition=broadwl #SBATCH --constraint=fdr # Load the default OpenMPI module. module load openmpi # Run the hellompi program with mpirun. The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi Submit the MPI job to the Slurm job scheduler from a Midway login node: sbatch hellompi.sbatch Here is an example output: Process 1 on midway2-0172.rcc.local out of 56 Process 3 on midway2-0172.rcc.local out of 56 Process 5 on midway2-0172.rcc.local out of 56 Process 7 on midway2-0172.rcc.local out of 56 Process 6 on midway2-0172.rcc.local out of 56 Process 9 on midway2-0172.rcc.local out of 56 Process 10 on midway2-0172.rcc.local out of 56 Process 11 on midway2-0172.rcc.local out of 56 Process 13 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0172.rcc.local out of 56 Process 2 on midway2-0172.rcc.local out of 56 Process 4 on midway2-0172.rcc.local out of 56 Process 8 on midway2-0172.rcc.local out of 56 Process 12 on midway2-0172.rcc.local out of 56 Process 17 on midway2-0173.rcc.local out of 56 Process 19 on midway2-0173.rcc.local out of 56 Process 21 on midway2-0173.rcc.local out of 56 Process 28 on midway2-0173.rcc.local out of 56 Process 29 on midway2-0173.rcc.local out of 56 Process 31 on midway2-0173.rcc.local out of 56 Process 32 on midway2-0173.rcc.local out of 56 Process 15 on midway2-0173.rcc.local out of 56 Process 18 on midway2-0173.rcc.local out of 56 Process 20 on midway2-0173.rcc.local out of 56 Process 22 on midway2-0173.rcc.local out of 56 Process 23 on midway2-0173.rcc.local out of 56 Process 24 on midway2-0173.rcc.local out of 56 Process 52 on midway2-0175.rcc.local out of 56 Process 25 on midway2-0173.rcc.local out of 56 Process 53 on midway2-0175.rcc.local out of 56 Process 26 on midway2-0173.rcc.local out of 56 Process 54 on midway2-0175.rcc.local out of 56 Process 27 on midway2-0173.rcc.local out of 56 Process 55 on midway2-0175.rcc.local out of 56 Process 14 on midway2-0173.rcc.local out of 56 Process 16 on midway2-0173.rcc.local out of 56 Process 30 on midway2-0173.rcc.local out of 56 Process 43 on midway2-0174.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 48 on midway2-0174.rcc.local out of 56 Process 49 on midway2-0174.rcc.local out of 56 Process 46 on midway2-0174.rcc.local out of 56 Process 44 on midway2-0174.rcc.local out of 56 Process 45 on midway2-0174.rcc.local out of 56 Process 40 on midway2-0174.rcc.local out of 56 Process 42 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0174.rcc.local out of 56 Process 37 on midway2-0174.rcc.local out of 56 Process 47 on midway2-0174.rcc.local out of 56 Process 34 on midway2-0174.rcc.local out of 56 Process 38 on midway2-0174.rcc.local out of 56 Process 35 on midway2-0174.rcc.local out of 56 Process 39 on midway2-0174.rcc.local out of 56 Process 41 on midway2-0174.rcc.local out of 56 Process 50 on midway2-0174.rcc.local out of 56 Process 33 on midway2-0174.rcc.local out of 56 From this output, we observe that the computation is distributed on 4 different nodes, with many threads running simultaneously on the same node. When developing your own sbatch script for MPI-based computations, keep in mind the following points. Without the --constraint=fdr or --constraint=edr options, your computations may run on nodes with FDR, EDR, or combination of both. It is possible to control the number of tasks that are run per node with the --ntasks-per-node option. For example, submitting the job like this to Midway: sbatch --ntasks-per-node=1 hellompi.sbatch results in each thread running on a different node: Process 52 on midway2-0175.rcc.local out of 56 Process 50 on midway2-0173.rcc.local out of 56 Process 49 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0002.rcc.local out of 56 Process 13 on midway2-0052.rcc.local out of 56 Process 2 on midway2-0013.rcc.local out of 56 Process 9 on midway2-0033.rcc.local out of 56 Process 54 on midway2-0177.rcc.local out of 56 Process 53 on midway2-0176.rcc.local out of 56 Process 44 on midway2-0157.rcc.local out of 56 Process 55 on midway2-0178.rcc.local out of 56 Process 11 on midway2-0041.rcc.local out of 56 Process 40 on midway2-0152.rcc.local out of 56 Process 43 on midway2-0156.rcc.local out of 56 Process 45 on midway2-0161.rcc.local out of 56 Process 46 on midway2-0162.rcc.local out of 56 Process 38 on midway2-0147.rcc.local out of 56 Process 39 on midway2-0148.rcc.local out of 56 Process 24 on midway2-0101.rcc.local out of 56 Process 3 on midway2-0014.rcc.local out of 56 Process 42 on midway2-0155.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0145.rcc.local out of 56 Process 25 on midway2-0102.rcc.local out of 56 Process 16 on midway2-0055.rcc.local out of 56 Process 18 on midway2-0057.rcc.local out of 56 Process 12 on midway2-0051.rcc.local out of 56 Process 14 on midway2-0053.rcc.local out of 56 Process 4 on midway2-0015.rcc.local out of 56 Process 15 on midway2-0054.rcc.local out of 56 Process 48 on midway2-0164.rcc.local out of 56 Process 28 on midway2-0105.rcc.local out of 56 Process 22 on midway2-0097.rcc.local out of 56 Process 30 on midway2-0118.rcc.local out of 56 Process 1 on midway2-0012.rcc.local out of 56 Process 35 on midway2-0144.rcc.local out of 56 Process 17 on midway2-0056.rcc.local out of 56 Process 37 on midway2-0146.rcc.local out of 56 Process 32 on midway2-0141.rcc.local out of 56 Process 34 on midway2-0143.rcc.local out of 56 Process 27 on midway2-0104.rcc.local out of 56 Process 31 on midway2-0119.rcc.local out of 56 Process 29 on midway2-0106.rcc.local out of 56 Process 10 on midway2-0034.rcc.local out of 56 Process 47 on midway2-0163.rcc.local out of 56 Process 33 on midway2-0142.rcc.local out of 56 Process 26 on midway2-0103.rcc.local out of 56 Process 7 on midway2-0019.rcc.local out of 56 Process 20 on midway2-0073.rcc.local out of 56 Process 5 on midway2-0016.rcc.local out of 56 Process 8 on midway2-0027.rcc.local out of 56 Process 41 on midway2-0153.rcc.local out of 56 Process 6 on midway2-0017.rcc.local out of 56 Process 21 on midway2-0096.rcc.local out of 56 Process 19 on midway2-0072.rcc.local out of 56 Process 23 on midway2-0100.rcc.local out of 56 Additional notes Both OpenMPI and IntelMPI have the ability to launch MPI programs directly with the Slurm command srun . It is not necessary to use this mode for most jobs, but it may provide additional job launch options. For example, from a Midway login node it is possible to launch the above hellompi program using OpenMPI using 28 MPI processes: srun -n28 hellompi With IntelMPI, you need to also set an environment variable for this to work: export I_MPI_PMI_LIBRARY=/software/slurm-current-$DISTARCH/lib/libpmi.so srun -n28 hellompi","title":"MPI jobs"},{"location":"midway23/examples/mpi/#mpi-jobs","text":"Here we present a couple small examples illustrating how to use MPI for distributed computing Midway. For more information on the MPI libraries available on Midway, see Message Passing Interface (MPI) . Our main illustration is a \u201chello world\u201d example. See file hellompi.c for the C code we will compile and run. #include <stdio.h> #include <stdlib.h> #include <mpi.h> int main ( int argc , char * argv [], char * envp []) { int numprocs , rank , namelen ; char processor_name [ MPI_MAX_PROCESSOR_NAME ]; MPI_Init ( & argc , & argv ); MPI_Comm_size ( MPI_COMM_WORLD , & numprocs ); MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); MPI_Get_processor_name ( processor_name , & namelen ); printf ( \"Process %d on %s out of %d \\n \" , rank , processor_name , numprocs ); MPI_Finalize (); } Copy hellompi.c to your home directory on Midway, load the default OpenMPI module, and compile the program on a Midway login node: module load openmpi mpicc hellompi.c -o hellompi hellompi.sbatch is a submission script that can be used to submit a job to Midway to run the hellompi program: #!/bin/bash #SBATCH --job-name=hellompi #SBATCH --output=hellompi.out #SBATCH --ntasks=56 #SBATCH --partition=broadwl #SBATCH --constraint=fdr # Load the default OpenMPI module. module load openmpi # Run the hellompi program with mpirun. The -n flag is not required; # mpirun will automatically figure out the best configuration from the # Slurm environment variables. mpirun ./hellompi Submit the MPI job to the Slurm job scheduler from a Midway login node: sbatch hellompi.sbatch Here is an example output: Process 1 on midway2-0172.rcc.local out of 56 Process 3 on midway2-0172.rcc.local out of 56 Process 5 on midway2-0172.rcc.local out of 56 Process 7 on midway2-0172.rcc.local out of 56 Process 6 on midway2-0172.rcc.local out of 56 Process 9 on midway2-0172.rcc.local out of 56 Process 10 on midway2-0172.rcc.local out of 56 Process 11 on midway2-0172.rcc.local out of 56 Process 13 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0172.rcc.local out of 56 Process 2 on midway2-0172.rcc.local out of 56 Process 4 on midway2-0172.rcc.local out of 56 Process 8 on midway2-0172.rcc.local out of 56 Process 12 on midway2-0172.rcc.local out of 56 Process 17 on midway2-0173.rcc.local out of 56 Process 19 on midway2-0173.rcc.local out of 56 Process 21 on midway2-0173.rcc.local out of 56 Process 28 on midway2-0173.rcc.local out of 56 Process 29 on midway2-0173.rcc.local out of 56 Process 31 on midway2-0173.rcc.local out of 56 Process 32 on midway2-0173.rcc.local out of 56 Process 15 on midway2-0173.rcc.local out of 56 Process 18 on midway2-0173.rcc.local out of 56 Process 20 on midway2-0173.rcc.local out of 56 Process 22 on midway2-0173.rcc.local out of 56 Process 23 on midway2-0173.rcc.local out of 56 Process 24 on midway2-0173.rcc.local out of 56 Process 52 on midway2-0175.rcc.local out of 56 Process 25 on midway2-0173.rcc.local out of 56 Process 53 on midway2-0175.rcc.local out of 56 Process 26 on midway2-0173.rcc.local out of 56 Process 54 on midway2-0175.rcc.local out of 56 Process 27 on midway2-0173.rcc.local out of 56 Process 55 on midway2-0175.rcc.local out of 56 Process 14 on midway2-0173.rcc.local out of 56 Process 16 on midway2-0173.rcc.local out of 56 Process 30 on midway2-0173.rcc.local out of 56 Process 43 on midway2-0174.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 48 on midway2-0174.rcc.local out of 56 Process 49 on midway2-0174.rcc.local out of 56 Process 46 on midway2-0174.rcc.local out of 56 Process 44 on midway2-0174.rcc.local out of 56 Process 45 on midway2-0174.rcc.local out of 56 Process 40 on midway2-0174.rcc.local out of 56 Process 42 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0174.rcc.local out of 56 Process 37 on midway2-0174.rcc.local out of 56 Process 47 on midway2-0174.rcc.local out of 56 Process 34 on midway2-0174.rcc.local out of 56 Process 38 on midway2-0174.rcc.local out of 56 Process 35 on midway2-0174.rcc.local out of 56 Process 39 on midway2-0174.rcc.local out of 56 Process 41 on midway2-0174.rcc.local out of 56 Process 50 on midway2-0174.rcc.local out of 56 Process 33 on midway2-0174.rcc.local out of 56 From this output, we observe that the computation is distributed on 4 different nodes, with many threads running simultaneously on the same node. When developing your own sbatch script for MPI-based computations, keep in mind the following points. Without the --constraint=fdr or --constraint=edr options, your computations may run on nodes with FDR, EDR, or combination of both. It is possible to control the number of tasks that are run per node with the --ntasks-per-node option. For example, submitting the job like this to Midway: sbatch --ntasks-per-node=1 hellompi.sbatch results in each thread running on a different node: Process 52 on midway2-0175.rcc.local out of 56 Process 50 on midway2-0173.rcc.local out of 56 Process 49 on midway2-0172.rcc.local out of 56 Process 0 on midway2-0002.rcc.local out of 56 Process 13 on midway2-0052.rcc.local out of 56 Process 2 on midway2-0013.rcc.local out of 56 Process 9 on midway2-0033.rcc.local out of 56 Process 54 on midway2-0177.rcc.local out of 56 Process 53 on midway2-0176.rcc.local out of 56 Process 44 on midway2-0157.rcc.local out of 56 Process 55 on midway2-0178.rcc.local out of 56 Process 11 on midway2-0041.rcc.local out of 56 Process 40 on midway2-0152.rcc.local out of 56 Process 43 on midway2-0156.rcc.local out of 56 Process 45 on midway2-0161.rcc.local out of 56 Process 46 on midway2-0162.rcc.local out of 56 Process 38 on midway2-0147.rcc.local out of 56 Process 39 on midway2-0148.rcc.local out of 56 Process 24 on midway2-0101.rcc.local out of 56 Process 3 on midway2-0014.rcc.local out of 56 Process 42 on midway2-0155.rcc.local out of 56 Process 51 on midway2-0174.rcc.local out of 56 Process 36 on midway2-0145.rcc.local out of 56 Process 25 on midway2-0102.rcc.local out of 56 Process 16 on midway2-0055.rcc.local out of 56 Process 18 on midway2-0057.rcc.local out of 56 Process 12 on midway2-0051.rcc.local out of 56 Process 14 on midway2-0053.rcc.local out of 56 Process 4 on midway2-0015.rcc.local out of 56 Process 15 on midway2-0054.rcc.local out of 56 Process 48 on midway2-0164.rcc.local out of 56 Process 28 on midway2-0105.rcc.local out of 56 Process 22 on midway2-0097.rcc.local out of 56 Process 30 on midway2-0118.rcc.local out of 56 Process 1 on midway2-0012.rcc.local out of 56 Process 35 on midway2-0144.rcc.local out of 56 Process 17 on midway2-0056.rcc.local out of 56 Process 37 on midway2-0146.rcc.local out of 56 Process 32 on midway2-0141.rcc.local out of 56 Process 34 on midway2-0143.rcc.local out of 56 Process 27 on midway2-0104.rcc.local out of 56 Process 31 on midway2-0119.rcc.local out of 56 Process 29 on midway2-0106.rcc.local out of 56 Process 10 on midway2-0034.rcc.local out of 56 Process 47 on midway2-0163.rcc.local out of 56 Process 33 on midway2-0142.rcc.local out of 56 Process 26 on midway2-0103.rcc.local out of 56 Process 7 on midway2-0019.rcc.local out of 56 Process 20 on midway2-0073.rcc.local out of 56 Process 5 on midway2-0016.rcc.local out of 56 Process 8 on midway2-0027.rcc.local out of 56 Process 41 on midway2-0153.rcc.local out of 56 Process 6 on midway2-0017.rcc.local out of 56 Process 21 on midway2-0096.rcc.local out of 56 Process 19 on midway2-0072.rcc.local out of 56 Process 23 on midway2-0100.rcc.local out of 56","title":"MPI jobs"},{"location":"midway23/examples/mpi/#additional-notes","text":"Both OpenMPI and IntelMPI have the ability to launch MPI programs directly with the Slurm command srun . It is not necessary to use this mode for most jobs, but it may provide additional job launch options. For example, from a Midway login node it is possible to launch the above hellompi program using OpenMPI using 28 MPI processes: srun -n28 hellompi With IntelMPI, you need to also set an environment variable for this to work: export I_MPI_PMI_LIBRARY=/software/slurm-current-$DISTARCH/lib/libpmi.so srun -n28 hellompi","title":"Additional notes"},{"location":"midway23/examples/srun/","text":"Parallel batch jobs Computations involving a very large number of independent computations should be combined in some way to reduce the number of jobs submitted to Slurm. Here we illustrate one strategy for doing this using GNU Parallel and srun . The parallel program executes tasks simultaneously until all tasks have been completed. Here\u2019s an example script, parallel.sbatch : #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=28 #SBATCH --mem-per-cpu=2G # NOTE DO NOT USE THE --mem= OPTION # Load the default version of GNU parallel. module load parallel # When running a large number of tasks simultaneously, it may be # necessary to increase the user process limit. ulimit -u 10000 # This specifies the options used to run srun. The \"-N1 -n1\" options are # used to allocates a single core to each task. srun = \"srun --exclusive -N1 -n1\" # This specifies the options used to run GNU parallel: # # --delay of 0.2 prevents overloading the controlling node. # # -j is the number of tasks run simultaneously. # # The combination of --joblog and --resume create a task log that # can be used to monitor progress. # parallel = \"parallel --delay 0.2 -j $SLURM_NTASKS --joblog runtask.log --resume\" # Run a script, runtask.sh, using GNU parallel and srun. Parallel # will run the runtask script for the numbers 1 through 128. To # illustrate, the first job will run like this: # # srun --exclusive -N1 -n1 ./runtask.sh arg1:1 > runtask.1 # $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..128 } # Note that if your program does not take any input, use the -n0 option to # call the parallel command: # # $parallel -n0 \"$srun ./run_noinput_task.sh > output.{1}\" ::: {1..128} In this example, our aim is to run script runtask.sh 128 times. The --ntasks option is set to 28, so at most 28 tasks can be run simultaneously. Here is the runtask.sh script that is run by GNU parallel: #!/bin/sh # This script outputs some useful information so we can see what parallel # and srun are doing. sleepsecs = $ [ ( $RANDOM % 10 ) + 10 ] s # $1 is arg1:{1} from GNU parallel. # # $PARALLEL_SEQ is a special variable from GNU parallel. It gives the # number of the job in the sequence. # # Here we print the sleep time, host name, and the date and time. echo task $1 seq: $PARALLEL_SEQ sleep: $sleepsecs host: $( hostname ) date: $( date ) # Sleep a random amount of time. sleep $sleepsecs To submit this job, copy both parallel.sbatch and runtask.sh to the same directory, and run chmod +x runtask.sh to make runtask.sh executable. Then the job can be submitted to the Slurm queue: sbatch parallel.sbatch When this job completes, you should see output files with names runtask.sh.N , where N is a number between 1 and 128. The content of the first output file (i.e., runtask.sh.1 ) should look something like this: task arg1:1 seq:1 sleep:14s host:midway2-0002 date:Thu Jan 10 09:17:36 CST 2017 Another file runtask.log is also created. It gives a list of the completed jobs. (Note: If the sbatch is submitted again, nothing will be run until runtask.log is removed.) It is also possible to use this same technique to run multithreaded tasks in parallel. Here is an example sbatch script, parallel-hybrid.sbatch , that distributes multithreaded computations (each using 28 CPUs) across 2 nodes: #!/bin/sh #SBATCH --partition=broadwl #SBATCH --time=01:00:00 #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=28 #SBATCH --exclusive # Load the default version of GNU parallel. module load parallel srun = \"srun --exclusive -N1 -n1 --cpus-per-task $SLURM_CPUS_PER_TASK \" # Instead of $SLURM_NTASKS, use $SLURM_NNODES to determine how # many jobs should be run simultaneously. parallel = \"parallel --delay 0.2 -j $SLURM_NNODES --joblog runtask.log --resume\" # Run the parallel command. $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..6 }","title":"Parallel batch jobs"},{"location":"midway23/examples/srun/#parallel-batch-jobs","text":"Computations involving a very large number of independent computations should be combined in some way to reduce the number of jobs submitted to Slurm. Here we illustrate one strategy for doing this using GNU Parallel and srun . The parallel program executes tasks simultaneously until all tasks have been completed. Here\u2019s an example script, parallel.sbatch : #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --partition=broadwl #SBATCH --ntasks=28 #SBATCH --mem-per-cpu=2G # NOTE DO NOT USE THE --mem= OPTION # Load the default version of GNU parallel. module load parallel # When running a large number of tasks simultaneously, it may be # necessary to increase the user process limit. ulimit -u 10000 # This specifies the options used to run srun. The \"-N1 -n1\" options are # used to allocates a single core to each task. srun = \"srun --exclusive -N1 -n1\" # This specifies the options used to run GNU parallel: # # --delay of 0.2 prevents overloading the controlling node. # # -j is the number of tasks run simultaneously. # # The combination of --joblog and --resume create a task log that # can be used to monitor progress. # parallel = \"parallel --delay 0.2 -j $SLURM_NTASKS --joblog runtask.log --resume\" # Run a script, runtask.sh, using GNU parallel and srun. Parallel # will run the runtask script for the numbers 1 through 128. To # illustrate, the first job will run like this: # # srun --exclusive -N1 -n1 ./runtask.sh arg1:1 > runtask.1 # $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..128 } # Note that if your program does not take any input, use the -n0 option to # call the parallel command: # # $parallel -n0 \"$srun ./run_noinput_task.sh > output.{1}\" ::: {1..128} In this example, our aim is to run script runtask.sh 128 times. The --ntasks option is set to 28, so at most 28 tasks can be run simultaneously. Here is the runtask.sh script that is run by GNU parallel: #!/bin/sh # This script outputs some useful information so we can see what parallel # and srun are doing. sleepsecs = $ [ ( $RANDOM % 10 ) + 10 ] s # $1 is arg1:{1} from GNU parallel. # # $PARALLEL_SEQ is a special variable from GNU parallel. It gives the # number of the job in the sequence. # # Here we print the sleep time, host name, and the date and time. echo task $1 seq: $PARALLEL_SEQ sleep: $sleepsecs host: $( hostname ) date: $( date ) # Sleep a random amount of time. sleep $sleepsecs To submit this job, copy both parallel.sbatch and runtask.sh to the same directory, and run chmod +x runtask.sh to make runtask.sh executable. Then the job can be submitted to the Slurm queue: sbatch parallel.sbatch When this job completes, you should see output files with names runtask.sh.N , where N is a number between 1 and 128. The content of the first output file (i.e., runtask.sh.1 ) should look something like this: task arg1:1 seq:1 sleep:14s host:midway2-0002 date:Thu Jan 10 09:17:36 CST 2017 Another file runtask.log is also created. It gives a list of the completed jobs. (Note: If the sbatch is submitted again, nothing will be run until runtask.log is removed.) It is also possible to use this same technique to run multithreaded tasks in parallel. Here is an example sbatch script, parallel-hybrid.sbatch , that distributes multithreaded computations (each using 28 CPUs) across 2 nodes: #!/bin/sh #SBATCH --partition=broadwl #SBATCH --time=01:00:00 #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=28 #SBATCH --exclusive # Load the default version of GNU parallel. module load parallel srun = \"srun --exclusive -N1 -n1 --cpus-per-task $SLURM_CPUS_PER_TASK \" # Instead of $SLURM_NTASKS, use $SLURM_NNODES to determine how # many jobs should be run simultaneously. parallel = \"parallel --delay 0.2 -j $SLURM_NNODES --joblog runtask.log --resume\" # Run the parallel command. $parallel \" $srun ./runtask.sh arg1:{1} > runtask.sh.{1}\" ::: { 1 ..6 }","title":"Parallel batch jobs"},{"location":"midway23/software/midway_software_overview/","text":"Software This page contains software-specific information. For more general information on how to run different types of jobs on Midway2, consult Running Jobs . RCC has a large selection of software available, but if you need software not currently available in the module system, send a detailed request to help@rcc.uchicago.edu . Using Software Modules RCC uses Environment Modules for managing software. The modules system permits us to set up the shell environment to make running and compiling software easier. It also allows us to make available many software packages and libraries that would otherwise conflict with one another. When you first log into Midway, you will be entered into a very 'bare-bones' user environment with minimal software available. The module system is a script based system used to manage the user environment and to \u201cactivate\u201d software packages. In order to access software that is installed on Midway, you must first load the corresponding software module. Basic module commands: Command Description module avail lists all available software modules module avail [name] lists modules matching [name] module load [name] loads the named module module unload [name] unloads the named module module list lists the modules currently loaded for the user For more information on how to use software modules on Midway2, consult Using software modules on Midway . Full Software Module List The best way to view the most current software offerings of RCC is to check the list of available software modules with the module avail command on Midway2. You may also view the complete Software Module List online.","title":"Overview"},{"location":"midway23/software/midway_software_overview/#software","text":"This page contains software-specific information. For more general information on how to run different types of jobs on Midway2, consult Running Jobs . RCC has a large selection of software available, but if you need software not currently available in the module system, send a detailed request to help@rcc.uchicago.edu .","title":"Software"},{"location":"midway23/software/midway_software_overview/#using-software-modules","text":"RCC uses Environment Modules for managing software. The modules system permits us to set up the shell environment to make running and compiling software easier. It also allows us to make available many software packages and libraries that would otherwise conflict with one another. When you first log into Midway, you will be entered into a very 'bare-bones' user environment with minimal software available. The module system is a script based system used to manage the user environment and to \u201cactivate\u201d software packages. In order to access software that is installed on Midway, you must first load the corresponding software module. Basic module commands: Command Description module avail lists all available software modules module avail [name] lists modules matching [name] module load [name] loads the named module module unload [name] unloads the named module module list lists the modules currently loaded for the user For more information on how to use software modules on Midway2, consult Using software modules on Midway .","title":"Using Software Modules"},{"location":"midway23/software/midway_software_overview/#full-software-module-list","text":"The best way to view the most current software offerings of RCC is to check the list of available software modules with the module avail command on Midway2. You may also view the complete Software Module List online.","title":"Full Software Module List"},{"location":"midway23/software/applications/ccsm3/","text":"CCSM3 CCSM3 can be built and run on Midway. For more information about CESM and how to acquire the source code, see http://www.cesm.ucar.edu/models/ccsm3.0/ We recommend using the intel compiler suite with CCSM3. The relevant modules that need to be loaded when working with CESM are therefore: $ module load intelmpi/4.1+intel-12.1 $ module load netcdf/4.2+intel-12.1 To provide users with a starting point in getting CCSM3 running on Midway, we provide here a version of CCSM3 which contains copy of CCSM3 that was downloaded on October 20, 2014 and has been modified to work on Midway. NOTE : CCSM3 has not been validated on Midway and RCC strongly reccomends that users perform numerical validation tests before completing production runs on Midway. The options set in these files (especially env.linux.midway) may not be optimal for your particular use case. We strongly reccomend that you downlaod CCSM3 from http://www.cesm.ucar.edu/models/ccsm3.0/ and move the modified files into your instance of the software one by one and take time to understand the changes/additions that were made. Many of the edits in the following version of CCSM3 were inspired by following the documention on this site: http://nf.nci.org.au/facilities/software/CCSM3/CCSM3_ac.html ccsm3_0-midway.tar contains all of the files distributed with CCSM3 as of October 20, 2014 along with the following additions/edits: ccsm3_0/wrk/example.sh A shell script which shows the steps necessary to build and submit a CCSM3 job to Midway. ccsm3_0/scripts/ccsm_utils/Tools/check_machine Line 21: Add \u201cmidway\u201d to the list ccsm3_0/models/bld/Macros.Linux This file contains the appropriate compiler flags for builsing ccsm3 models with the intlmpi/4.1+intel-12.1 and netcdf/4.2+intel-12.1 on Midway. You will probably not need to modify this file. ccsm3_0/models/utils/mct/mpeu/get_zeits.c This file was modified to avoid an issue with the now obsolete CLK_TCK constant. Line 24 : #if !defined(CLK_TCK) Line 25 : # include <limits.h> Line 26 : #endif becomes Line 24 : #define CLK_TCK CLOCKS_PER_SEC ccsm3_0/scripts/ccsm_utils/Machines/modules.linux.midway You will need to modify this file if you intend to build ccsm3 with a different compiler, MPI library, or version of netCDF. Here, we have set it up to work with intelmpi/4.1+intel-12.1 and netcdf/4.2+intel-12.1 ccsm3_0/scripts/ccsm_utils/Machines/run.linux.midway This file was adapted from run.linux.jazz and may need further modification. It provides the skeleton script for starting a ccsm3 job. ccsm3_0/scripts/ccsm_utils/Machines/batch.linux.midway This file was adapted from batch.linux.jazz and may need further modification. It provides the skeleton submission script for running a ccsm3 job through SLURM. ccsm3_0/scripts/ccsm_utils/Machines/env.linux.midway This file may need further modification to set run configuration. This file contains environment variables including DIN_LOC_ROOT and SCRATCH. You can also control the location of EXEROOT and RUNROOT from this file. In this tarball, we have pointed DIN_LOC_ROOT to /project/databases/cesm which contains a copy of the CESM inputdata set which was downloaded in late-2012. You may want to point this to a different location if you require different or more up to date inputdata.","title":"[CCSM3](single:CCSM3)"},{"location":"midway23/software/applications/ccsm3/#ccsm3","text":"CCSM3 can be built and run on Midway. For more information about CESM and how to acquire the source code, see http://www.cesm.ucar.edu/models/ccsm3.0/ We recommend using the intel compiler suite with CCSM3. The relevant modules that need to be loaded when working with CESM are therefore: $ module load intelmpi/4.1+intel-12.1 $ module load netcdf/4.2+intel-12.1 To provide users with a starting point in getting CCSM3 running on Midway, we provide here a version of CCSM3 which contains copy of CCSM3 that was downloaded on October 20, 2014 and has been modified to work on Midway. NOTE : CCSM3 has not been validated on Midway and RCC strongly reccomends that users perform numerical validation tests before completing production runs on Midway. The options set in these files (especially env.linux.midway) may not be optimal for your particular use case. We strongly reccomend that you downlaod CCSM3 from http://www.cesm.ucar.edu/models/ccsm3.0/ and move the modified files into your instance of the software one by one and take time to understand the changes/additions that were made. Many of the edits in the following version of CCSM3 were inspired by following the documention on this site: http://nf.nci.org.au/facilities/software/CCSM3/CCSM3_ac.html ccsm3_0-midway.tar contains all of the files distributed with CCSM3 as of October 20, 2014 along with the following additions/edits:","title":"CCSM3"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0wrkexamplesh","text":"A shell script which shows the steps necessary to build and submit a CCSM3 job to Midway.","title":"ccsm3_0/wrk/example.sh"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0scriptsccsm_utilstoolscheck_machine","text":"Line 21: Add \u201cmidway\u201d to the list","title":"ccsm3_0/scripts/ccsm_utils/Tools/check_machine"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0modelsbldmacroslinux","text":"This file contains the appropriate compiler flags for builsing ccsm3 models with the intlmpi/4.1+intel-12.1 and netcdf/4.2+intel-12.1 on Midway. You will probably not need to modify this file.","title":"ccsm3_0/models/bld/Macros.Linux"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0modelsutilsmctmpeuget_zeitsc","text":"This file was modified to avoid an issue with the now obsolete CLK_TCK constant. Line 24 : #if !defined(CLK_TCK) Line 25 : # include <limits.h> Line 26 : #endif becomes Line 24 : #define CLK_TCK CLOCKS_PER_SEC","title":"ccsm3_0/models/utils/mct/mpeu/get_zeits.c"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0scriptsccsm_utilsmachinesmoduleslinuxmidway","text":"You will need to modify this file if you intend to build ccsm3 with a different compiler, MPI library, or version of netCDF. Here, we have set it up to work with intelmpi/4.1+intel-12.1 and netcdf/4.2+intel-12.1","title":"ccsm3_0/scripts/ccsm_utils/Machines/modules.linux.midway"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0scriptsccsm_utilsmachinesrunlinuxmidway","text":"This file was adapted from run.linux.jazz and may need further modification. It provides the skeleton script for starting a ccsm3 job.","title":"ccsm3_0/scripts/ccsm_utils/Machines/run.linux.midway"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0scriptsccsm_utilsmachinesbatchlinuxmidway","text":"This file was adapted from batch.linux.jazz and may need further modification. It provides the skeleton submission script for running a ccsm3 job through SLURM.","title":"ccsm3_0/scripts/ccsm_utils/Machines/batch.linux.midway"},{"location":"midway23/software/applications/ccsm3/#ccsm3_0scriptsccsm_utilsmachinesenvlinuxmidway","text":"This file may need further modification to set run configuration. This file contains environment variables including DIN_LOC_ROOT and SCRATCH. You can also control the location of EXEROOT and RUNROOT from this file. In this tarball, we have pointed DIN_LOC_ROOT to /project/databases/cesm which contains a copy of the CESM inputdata set which was downloaded in late-2012. You may want to point this to a different location if you require different or more up to date inputdata.","title":"ccsm3_0/scripts/ccsm_utils/Machines/env.linux.midway"},{"location":"midway23/software/applications/cesm/","text":"CESM CESM can be built and run on Midway. For more information about CESM and how to acquire the source code, see http://www.cesm.ucar.edu/ To date, CESM versions 1.0.4 and 1.1.1 have been run on Midway. The port validation procedure has been completed for CESM version 1.0.4 in conjunction with the Intel compiler suite version 12.1. Port validation results were found to be within tolerances (documentation coming soon). RCC has also downloaded the entire CESM inputdata repository and maintains a local copy. This can be found at /project/databases/cesm/inputdata . We recommend using the intel compiler suite with CESM. The relevant modules that need to be loaded when working with CESM are therefore: $ module load intelmpi $ module load netcdf/4.2+intel-12.1 To configure CESM on Midway, download the CESM source and insert the following configuration files for the specific version of CESM you are using into your local copy of CESM. CESM 1.0.4 These files should be placed in /path/to/cesm1_0_4/scripts/ccsm_utils/Machines/ (overwriting the default files) cesm1_0_4/Macros.midway cesm1_0_4/mkbatch.midway cesm1_0_4/env_machopts.midway cesm1_0_4/config_machines.xml \u2013 see note about how to edit this file You will need to edit your copy of config_machines.xml to point to appropriate locations. On lines 7 and 13, replace the substring \u201c/path/to/cesm1_0_4\u201d with the path to your local installation of CESM 1.0.4 Once you have edited and inserted these files, you can run an example model with CESM 1.0.4 by running the following script (take note to edit the first line to point to your local installation of CESM 1.0.4): cesm1_0_4/example.sh CESM 1.1.1 and CESM 1.2.1 These files should be placed in /path/to/cesm1_1_1/scripts/ccsm_utils/Machines/ (overwriting the default files) cesm1_1_1/config_compilers.xml cesm1_1_1/mkbatch.midway cesm1_1_1/env_mach_specific.midway cesm1_1_1/config_machines.xml \u2013 see note about how to edit this file You will need to edit your copy of config_machines.xml to point to appropriate locations. On lines 59, 60, and 63, replace the substring \u201c/path/to/cesm1_1_1\u201d with the path to your local installation of CESM 1.1.1 Once you have edited and inserted these files, you can run an example model with CESM 1.1.1 by running the following script (take note to edit the first line to point to your local installation of CESM 1.1.1): cesm1_1_1/example.sh","title":"[CESM](single:CESM)"},{"location":"midway23/software/applications/cesm/#cesm","text":"CESM can be built and run on Midway. For more information about CESM and how to acquire the source code, see http://www.cesm.ucar.edu/ To date, CESM versions 1.0.4 and 1.1.1 have been run on Midway. The port validation procedure has been completed for CESM version 1.0.4 in conjunction with the Intel compiler suite version 12.1. Port validation results were found to be within tolerances (documentation coming soon). RCC has also downloaded the entire CESM inputdata repository and maintains a local copy. This can be found at /project/databases/cesm/inputdata . We recommend using the intel compiler suite with CESM. The relevant modules that need to be loaded when working with CESM are therefore: $ module load intelmpi $ module load netcdf/4.2+intel-12.1 To configure CESM on Midway, download the CESM source and insert the following configuration files for the specific version of CESM you are using into your local copy of CESM.","title":"CESM"},{"location":"midway23/software/applications/cesm/#cesm-104","text":"These files should be placed in /path/to/cesm1_0_4/scripts/ccsm_utils/Machines/ (overwriting the default files) cesm1_0_4/Macros.midway cesm1_0_4/mkbatch.midway cesm1_0_4/env_machopts.midway cesm1_0_4/config_machines.xml \u2013 see note about how to edit this file You will need to edit your copy of config_machines.xml to point to appropriate locations. On lines 7 and 13, replace the substring \u201c/path/to/cesm1_0_4\u201d with the path to your local installation of CESM 1.0.4 Once you have edited and inserted these files, you can run an example model with CESM 1.0.4 by running the following script (take note to edit the first line to point to your local installation of CESM 1.0.4): cesm1_0_4/example.sh","title":"CESM 1.0.4"},{"location":"midway23/software/applications/cesm/#cesm-111-and-cesm-121","text":"These files should be placed in /path/to/cesm1_1_1/scripts/ccsm_utils/Machines/ (overwriting the default files) cesm1_1_1/config_compilers.xml cesm1_1_1/mkbatch.midway cesm1_1_1/env_mach_specific.midway cesm1_1_1/config_machines.xml \u2013 see note about how to edit this file You will need to edit your copy of config_machines.xml to point to appropriate locations. On lines 59, 60, and 63, replace the substring \u201c/path/to/cesm1_1_1\u201d with the path to your local installation of CESM 1.1.1 Once you have edited and inserted these files, you can run an example model with CESM 1.1.1 by running the following script (take note to edit the first line to point to your local installation of CESM 1.1.1): cesm1_1_1/example.sh","title":"CESM 1.1.1 and CESM 1.2.1"},{"location":"midway23/software/applications/cp2k/","text":"CP2K cp2k.sbatch demonstrates how to run CP2K. cp2k-H2O.tgz is a tarball with the H2O CP2K input deck used for this example. #!/bin/sh #SBATCH --output=cp2k-%j.out #SBATCH --constraint=ib #SBATCH --exclusive #SBATCH --nodes=2 module load cp2k mpirun cp2k.popt H2O-32.inp You can submit to the queue with this command: sbatch cp2k.sbatch","title":"[CP2K](http://www.cp2k.org)"},{"location":"midway23/software/applications/cp2k/#cp2k","text":"cp2k.sbatch demonstrates how to run CP2K. cp2k-H2O.tgz is a tarball with the H2O CP2K input deck used for this example. #!/bin/sh #SBATCH --output=cp2k-%j.out #SBATCH --constraint=ib #SBATCH --exclusive #SBATCH --nodes=2 module load cp2k mpirun cp2k.popt H2O-32.inp You can submit to the queue with this command: sbatch cp2k.sbatch","title":"CP2K"},{"location":"midway23/software/applications/gamess/","text":"GAMESS gamess.sbatch demonstrates how to submit a GAMESS job to Midway. In this example, we are going to use 4 nodes from the sandyb partition exclusively for a total of 64 cores across 4 nodes. An example input deck is here: example.inp #!/bin/bash #SBATCH --output=gamess_example.out #SBATCH --time=00:10:00 #SBATCH --partition=sandyb #SBATCH --nodes=4 #SBATCH --exclusive #SBATCH --constraint=ib module load gamess rungms example.inp $SLURM_NTASKS $SLURM_CPUS_ON_NODE You can submit to the queue with this command: sbatch gamess.sbatch","title":"GAMESS"},{"location":"midway23/software/applications/gamess/#gamess","text":"gamess.sbatch demonstrates how to submit a GAMESS job to Midway. In this example, we are going to use 4 nodes from the sandyb partition exclusively for a total of 64 cores across 4 nodes. An example input deck is here: example.inp #!/bin/bash #SBATCH --output=gamess_example.out #SBATCH --time=00:10:00 #SBATCH --partition=sandyb #SBATCH --nodes=4 #SBATCH --exclusive #SBATCH --constraint=ib module load gamess rungms example.inp $SLURM_NTASKS $SLURM_CPUS_ON_NODE You can submit to the queue with this command: sbatch gamess.sbatch","title":"GAMESS"},{"location":"midway23/software/applications/getpdb/","text":"Get PDB tool getpdb downloads PDB files from the Protein Data Bank to your present working directory. Pass in any number of 4-character pdb codes to download respective pdb files. Use only lowercase characters to represent letters. Report issues to help@rcc.uchicago.edu Usage: getpdb 1hlw 1md9 [...]","title":"Get PDB tool"},{"location":"midway23/software/applications/getpdb/#get-pdb-tool","text":"getpdb downloads PDB files from the Protein Data Bank to your present working directory. Pass in any number of 4-character pdb codes to download respective pdb files. Use only lowercase characters to represent letters. Report issues to help@rcc.uchicago.edu Usage: getpdb 1hlw 1md9 [...]","title":"Get PDB tool"},{"location":"midway23/software/applications/gnuplot/","text":"Gnuplot Gnuplot can run as a command within Slurm such as srun gnuplot [input files...] or sbatch gnuplot.sbatch or within an sinteractive session To work with a command line only interface, the output must be redirected. See the included file surface1.1.gnu as an example. # Format the output to plot the output as a png set terminal pngcairo transparent enhanced font \"arial,10\" fontscale 1 .0 size 500 , 350 # Redirect the output to a named file set output 'surface1.1.png' set label 1 \"This is the surface boundary\" at -10, -5, 150 centre norotate back nopoint offset character 0 , 0 , 0 set arrow 1 from -10, -5, 120 to -10, 0 , 0 nohead back nofilled linetype -1 linewidth 1 .000 set arrow 2 from -10, -5, 120 to 10 , 0 , 0 nohead back nofilled linetype -1 linewidth 1 .000 set arrow 3 from -10, -5, 120 to 0 , 10 , 0 nohead back nofilled linetype -1 linewidth 1 .000 set arrow 4 from -10, -5, 120 to 0 , -10, 0 nohead back nofilled linetype -1 linewidth 1 .000 set samples 21 , 21 set isosamples 11 , 11 set title \"3D gnuplot demo\" set xlabel \"X axis\" set xlabel offset character -3, -2, 0 font \"\" textcolor lt -1 norotate set xrange [ -10.0000 : 10 .0000 ] noreverse nowriteback set ylabel \"Y axis\" set ylabel offset character 3 , -2, 0 font \"\" textcolor lt -1 rotate by -270 set yrange [ -10.0000 : 10 .0000 ] noreverse nowriteback set zlabel \"Z axis\" set zlabel offset character -5, 0 , 0 font \"\" textcolor lt -1 norotate splot x*y","title":"Gnuplot"},{"location":"midway23/software/applications/gnuplot/#gnuplot","text":"Gnuplot can run as a command within Slurm such as srun gnuplot [input files...] or sbatch gnuplot.sbatch or within an sinteractive session To work with a command line only interface, the output must be redirected. See the included file surface1.1.gnu as an example. # Format the output to plot the output as a png set terminal pngcairo transparent enhanced font \"arial,10\" fontscale 1 .0 size 500 , 350 # Redirect the output to a named file set output 'surface1.1.png' set label 1 \"This is the surface boundary\" at -10, -5, 150 centre norotate back nopoint offset character 0 , 0 , 0 set arrow 1 from -10, -5, 120 to -10, 0 , 0 nohead back nofilled linetype -1 linewidth 1 .000 set arrow 2 from -10, -5, 120 to 10 , 0 , 0 nohead back nofilled linetype -1 linewidth 1 .000 set arrow 3 from -10, -5, 120 to 0 , 10 , 0 nohead back nofilled linetype -1 linewidth 1 .000 set arrow 4 from -10, -5, 120 to 0 , -10, 0 nohead back nofilled linetype -1 linewidth 1 .000 set samples 21 , 21 set isosamples 11 , 11 set title \"3D gnuplot demo\" set xlabel \"X axis\" set xlabel offset character -3, -2, 0 font \"\" textcolor lt -1 norotate set xrange [ -10.0000 : 10 .0000 ] noreverse nowriteback set ylabel \"Y axis\" set ylabel offset character 3 , -2, 0 font \"\" textcolor lt -1 rotate by -270 set yrange [ -10.0000 : 10 .0000 ] noreverse nowriteback set zlabel \"Z axis\" set zlabel offset character -5, 0 , 0 font \"\" textcolor lt -1 norotate splot x*y","title":"Gnuplot"},{"location":"midway23/software/applications/gromacs/","text":"GROMACS There are two different primary configuration of GROMACS: gromacs-X.Y.Z is single precision (float) gromacs-plumed-X.Y.Z+ is double precision, compiled with the stated compiler and MPI code, with PLUMED and Reconnaissance Metadynamics gromacs.sbatch demonstrates how to run a short Gromacs job (the d.dppc test case) in parallel. Submit to the queue by: cd $HOME /rcchelp/software/gromacs.rcc-docs sbatch gromacs.sbatch # and / or sbatch gromacs-plumed.sbatch The submission scripts can be modified to suit your needs","title":"GROMACS"},{"location":"midway23/software/applications/gromacs/#gromacs","text":"There are two different primary configuration of GROMACS: gromacs-X.Y.Z is single precision (float) gromacs-plumed-X.Y.Z+ is double precision, compiled with the stated compiler and MPI code, with PLUMED and Reconnaissance Metadynamics gromacs.sbatch demonstrates how to run a short Gromacs job (the d.dppc test case) in parallel. Submit to the queue by: cd $HOME /rcchelp/software/gromacs.rcc-docs sbatch gromacs.sbatch # and / or sbatch gromacs-plumed.sbatch The submission scripts can be modified to suit your needs","title":"GROMACS"},{"location":"midway23/software/applications/hoomd/","text":"HOOMD For full performance HOOMD should be run on GPUs. It can be run on a CPU only but performance will not be comparable. hoomd.sbatch demonstrates how to run the polymer_bmark.hoomd and lj_liquid_bmark.hoomd on a GPU device. #!/bin/sh #SBATCH --time=0:10:00 #SBATCH --job-name=hoomd #SBATCH --output=hoomd-%j.out #SBATCH --partition=gpu #SBATCH --gres=gpu:1 module load hoomd hoomd polymer_bmark.hoomd hoomd lj_liquid_bmark.hoomd The script can be submitted with this command: sbatch hoomd.sbatch","title":"[HOOMD](http://codeblue.umich.edu/hoomd-blue/)"},{"location":"midway23/software/applications/hoomd/#hoomd","text":"For full performance HOOMD should be run on GPUs. It can be run on a CPU only but performance will not be comparable. hoomd.sbatch demonstrates how to run the polymer_bmark.hoomd and lj_liquid_bmark.hoomd on a GPU device. #!/bin/sh #SBATCH --time=0:10:00 #SBATCH --job-name=hoomd #SBATCH --output=hoomd-%j.out #SBATCH --partition=gpu #SBATCH --gres=gpu:1 module load hoomd hoomd polymer_bmark.hoomd hoomd lj_liquid_bmark.hoomd The script can be submitted with this command: sbatch hoomd.sbatch","title":"HOOMD"},{"location":"midway23/software/applications/lammps/","text":"LAMMPS Table of Contents Quick Start Intro of LAMMPS Get Best Performance Benchmarks Copyright Quick Start If you\u2019re familiar with LAMMPS software, this section gives you quick steps of using LAMMPS, which has been installed and optimized on the Midway cluster at RCC. LAMMPS is installed with RCC Module system. You can use either of the following commands to load it into the shell environment: module load lammps This module is built from the SVN source hosted at svn://svn.icms.temple.edu/lammps-ro/trunk, Version 30Sep14. The SVN trunk provides the up-to-date code from LAMMPS developers. The optimiztion package \u201cOPT\u201d is compiled along with this binary: module load lammps-plumed This module is built from the TARBALL source hosted at http://lammps.sandia.gov/download.html , Version 5Sep14, which is the latest stable distribution. There are two important features in this installation: (1) two packages USER-OMP and USER-INTEL were added for optimization; (2) The offsite pakcage \u201cUSER-PLUMED\u201d was added to provide free energy techniques. After loading either of the two modules, you can run LAMMPS with the binary \u201clmp_intelmpi\u201d. Although the two modules were compiled with different vesions of Intel MPI libraries, the module system can automatically load the correct one. A typical SLURM script of running LAMMPS jobs is following: #!/bin/sh #SBATCH --job-name=lammps #SBATCH --output=lammps-%j.out #SBATCH --constraint=ib #SBATCH --exclusive #SBATCH --nodes=4 module load lammps mpirun lmp_intelmpi < in .lj GPU support has been also patched in lammps-plumed module. Two packages, GPU and USER-CUDE were compiled with CUDA-4.2. The module is named with suffix with suffix of \u201c-cuda\u201d for you to load or use them: module load lammps-plumed/5Sep14-cuda+intelmpi-5.0+intel-15.0 mpirun lmp_intelmpi-cuda -c on -sf cuda < in.lj Intro of LAMMPS LAMMPS is a simulation software for particle systems. It is specially designed for molecular dynamics technique and large-scalse parallel simulations. It is an open-source code and developed and maintained by Sandia National Liboratory (SNL). It has been widely used for studies of methodology & algorithm developments and simulations of material science, chemistry, physics and biology. For more information of LAMMPS, please visit its offical website: http://http//lammps.sandia.gov/ To gain more advice of using LAMMPS efficiently, please read its discussion section on \u201cAcceleration\u201d at: http://lammps.sandia.gov/doc/Section_accelerate.html Get Best Performance The module lammps-plumed is installed with following packages: ASPHERE BODY CLASS2 COLLOID DIPOLE FLD GRANULAR MANYBODY KSPACE MC MISC MOLECULE REPLICA RIGID SHOCK SRD USER-CG-CMM USER-EFF USER-FEP USER-LB USER-MISC USER-MOLFILE USER-OMP USER-SPH USER-PLUMED USER-INTEL To know more information about these packages, please read http://lammps.sandia.gov/doc/Section_start.html#start_3 If you need other packages to be installed in this module, please contact yuxing@uchicago.edu The binaries of LAMMPS compiled here are aim to provide RCC users the optimized solutions. Therefore, three important packages are specially discussed here: OPT, USER-OMP and USER-INTEL. OPT Quote from LAMMPS website: \u201c The OPT package was developed by James Fischer (High Performance Technologies), David Richie, and Vincent Natoli (Stone Ridge Technologies). It contains a handful of pair styles whose compute() methods were rewritten in C++ templated form to reduce the overhead due to if tests and other conditional code. \u201d To use OPT acceleration, you just need to put \u201c-sf opt\u201d in your job command: Ex: lmp_intelmpi -sf opt < lj.in However, only part of pair-styles are optimized in this package. USER-OMP The USER-OMP package was developed by Axel Kohlmeyer at Temple University. The purpose of package is to introduce the OpenMP/MPI hybrid parallel scheme into LAMMPS to gain benefits from the state-of-art multicore processors. Therefore, the parallel jobs are run in the combination of SMP threads x MPI tasks . For example, if you request the resource of 64 cores (4 nodes) on Sandyb partition through SLURM systems, you can choose different combination (16x4, 8x8, 4x16, et al) to receive the optimal performace. To do this, please set the following options correctly. For example, if I want to run 8 MPI tasks total, and each of them allocate 8 threads, which means one MPI task per Sandy-Bridge processor: --nnodes=4 // allocate 4 nodes total --ntasks-per-node=2 // execute 2 MPI tasks per node (1 per processor) --cpus-per-task=8 // allocate 8 openmp threads per MPI task You can also set environment variable OMP_NUM_THREADS=8 for this. (Not neccessary) Besides, you need to also turn on the OMP suffix in job command: Ex: lmp_intelmpi -sf omp < lj.in At the beginning of the LAMMPS in-script (i.e., lj.in in this example), you need to also specify the loading of pacakge by: package omp $N $N is the number of OMP threads, which equals to 8 in this example. Unfortunately we didn\u2019t a boost of speed on this hybrid code. For most of the cases, OMP_NUM_THREADS=1 gives the best performance, which means hybrid isn\u2019t actually used. However, the USER-OMP pacakge did optimize lots of the codes, from different force calculations to integrations, which results a significant acceleration although OMP_NUM_THREADS=1 is used. USER-INTEL This is a very new package that was developed by Intel technicians. The purpose of this pacakages is to implement the MIC support into LAMMPS. However, without having a MIC card, the code can be also accelerated a lot on CPU-only clusters, because the codes were rewritten to support the INTEL AVX vectoring tenchnique. USER-INTEL also provides a large number of optimized codes for LAMMPS functions. To use OPT acceleration, you just need to put \u201c-sf opt\u201d in your job command: Ex: lmp_intelmpi -sf intel < lj.in At the beginning of the LAMMPS in-script (i.e., lj.in in this example), you need to also specify the loading of pacakge by: package intel Benchmarks Testing infomation: Hardware: Haswell E5-2660 v3. (2.6GHz, 20 core) DDR4, 2133MHz System: Pure water box (100 x 100 x 100 A^3), 95,577 atoms PPPM: double precision FFTW3, 1E-4 CUTOFF: 10A Steps: 1000 | Timings | Plain | OPT | USER-OMP | USER-INTEL | | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------- | --------------- | ---------- | | | | | | | | | | | | Total | 50.07 | 45.20 | 41.41 | 33.75 | | Pair | 37.55 | 33.11 | 29.07 | 21.54 | | Kspace | 6.48 | 6.86 | 5.89 | 5.75 | | Neighbor | 3.18 | 2.29 | 3.08 | 2.95 | | Communication | 0.90 | 0.85 | 1.02 | 1.01 | | Output | 0.01 | 0.01 | 0.01 | 0.01 | | Other | 1.60 | 1.76 | 2.07 | 2.12 | Numbers are in seconds. Lower is better. Copyright To get the permission of using the content (text, images and benchmark data) of this document in anywhere else, please send an Email to yuxing@uchicago.edu .","title":"[LAMMPS](http://lammps.sandia.gov/)"},{"location":"midway23/software/applications/lammps/#lammps","text":"","title":"LAMMPS"},{"location":"midway23/software/applications/lammps/#table-of-contents","text":"Quick Start Intro of LAMMPS Get Best Performance Benchmarks Copyright","title":"Table of Contents"},{"location":"midway23/software/applications/lammps/#quick-start","text":"If you\u2019re familiar with LAMMPS software, this section gives you quick steps of using LAMMPS, which has been installed and optimized on the Midway cluster at RCC. LAMMPS is installed with RCC Module system. You can use either of the following commands to load it into the shell environment: module load lammps This module is built from the SVN source hosted at svn://svn.icms.temple.edu/lammps-ro/trunk, Version 30Sep14. The SVN trunk provides the up-to-date code from LAMMPS developers. The optimiztion package \u201cOPT\u201d is compiled along with this binary: module load lammps-plumed This module is built from the TARBALL source hosted at http://lammps.sandia.gov/download.html , Version 5Sep14, which is the latest stable distribution. There are two important features in this installation: (1) two packages USER-OMP and USER-INTEL were added for optimization; (2) The offsite pakcage \u201cUSER-PLUMED\u201d was added to provide free energy techniques. After loading either of the two modules, you can run LAMMPS with the binary \u201clmp_intelmpi\u201d. Although the two modules were compiled with different vesions of Intel MPI libraries, the module system can automatically load the correct one. A typical SLURM script of running LAMMPS jobs is following: #!/bin/sh #SBATCH --job-name=lammps #SBATCH --output=lammps-%j.out #SBATCH --constraint=ib #SBATCH --exclusive #SBATCH --nodes=4 module load lammps mpirun lmp_intelmpi < in .lj GPU support has been also patched in lammps-plumed module. Two packages, GPU and USER-CUDE were compiled with CUDA-4.2. The module is named with suffix with suffix of \u201c-cuda\u201d for you to load or use them: module load lammps-plumed/5Sep14-cuda+intelmpi-5.0+intel-15.0 mpirun lmp_intelmpi-cuda -c on -sf cuda < in.lj","title":"Quick Start"},{"location":"midway23/software/applications/lammps/#intro-of-lammps","text":"LAMMPS is a simulation software for particle systems. It is specially designed for molecular dynamics technique and large-scalse parallel simulations. It is an open-source code and developed and maintained by Sandia National Liboratory (SNL). It has been widely used for studies of methodology & algorithm developments and simulations of material science, chemistry, physics and biology. For more information of LAMMPS, please visit its offical website: http://http//lammps.sandia.gov/ To gain more advice of using LAMMPS efficiently, please read its discussion section on \u201cAcceleration\u201d at: http://lammps.sandia.gov/doc/Section_accelerate.html","title":"Intro of LAMMPS"},{"location":"midway23/software/applications/lammps/#get-best-performance","text":"The module lammps-plumed is installed with following packages: ASPHERE BODY CLASS2 COLLOID DIPOLE FLD GRANULAR MANYBODY KSPACE MC MISC MOLECULE REPLICA RIGID SHOCK SRD USER-CG-CMM USER-EFF USER-FEP USER-LB USER-MISC USER-MOLFILE USER-OMP USER-SPH USER-PLUMED USER-INTEL To know more information about these packages, please read http://lammps.sandia.gov/doc/Section_start.html#start_3 If you need other packages to be installed in this module, please contact yuxing@uchicago.edu The binaries of LAMMPS compiled here are aim to provide RCC users the optimized solutions. Therefore, three important packages are specially discussed here: OPT, USER-OMP and USER-INTEL.","title":"Get Best Performance"},{"location":"midway23/software/applications/lammps/#opt","text":"Quote from LAMMPS website: \u201c The OPT package was developed by James Fischer (High Performance Technologies), David Richie, and Vincent Natoli (Stone Ridge Technologies). It contains a handful of pair styles whose compute() methods were rewritten in C++ templated form to reduce the overhead due to if tests and other conditional code. \u201d To use OPT acceleration, you just need to put \u201c-sf opt\u201d in your job command: Ex: lmp_intelmpi -sf opt < lj.in However, only part of pair-styles are optimized in this package.","title":"OPT"},{"location":"midway23/software/applications/lammps/#user-omp","text":"The USER-OMP package was developed by Axel Kohlmeyer at Temple University. The purpose of package is to introduce the OpenMP/MPI hybrid parallel scheme into LAMMPS to gain benefits from the state-of-art multicore processors. Therefore, the parallel jobs are run in the combination of SMP threads x MPI tasks . For example, if you request the resource of 64 cores (4 nodes) on Sandyb partition through SLURM systems, you can choose different combination (16x4, 8x8, 4x16, et al) to receive the optimal performace. To do this, please set the following options correctly. For example, if I want to run 8 MPI tasks total, and each of them allocate 8 threads, which means one MPI task per Sandy-Bridge processor: --nnodes=4 // allocate 4 nodes total --ntasks-per-node=2 // execute 2 MPI tasks per node (1 per processor) --cpus-per-task=8 // allocate 8 openmp threads per MPI task You can also set environment variable OMP_NUM_THREADS=8 for this. (Not neccessary) Besides, you need to also turn on the OMP suffix in job command: Ex: lmp_intelmpi -sf omp < lj.in At the beginning of the LAMMPS in-script (i.e., lj.in in this example), you need to also specify the loading of pacakge by: package omp $N $N is the number of OMP threads, which equals to 8 in this example. Unfortunately we didn\u2019t a boost of speed on this hybrid code. For most of the cases, OMP_NUM_THREADS=1 gives the best performance, which means hybrid isn\u2019t actually used. However, the USER-OMP pacakge did optimize lots of the codes, from different force calculations to integrations, which results a significant acceleration although OMP_NUM_THREADS=1 is used.","title":"USER-OMP"},{"location":"midway23/software/applications/lammps/#user-intel","text":"This is a very new package that was developed by Intel technicians. The purpose of this pacakages is to implement the MIC support into LAMMPS. However, without having a MIC card, the code can be also accelerated a lot on CPU-only clusters, because the codes were rewritten to support the INTEL AVX vectoring tenchnique. USER-INTEL also provides a large number of optimized codes for LAMMPS functions. To use OPT acceleration, you just need to put \u201c-sf opt\u201d in your job command: Ex: lmp_intelmpi -sf intel < lj.in At the beginning of the LAMMPS in-script (i.e., lj.in in this example), you need to also specify the loading of pacakge by: package intel","title":"USER-INTEL"},{"location":"midway23/software/applications/lammps/#benchmarks","text":"Testing infomation: Hardware: Haswell E5-2660 v3. (2.6GHz, 20 core) DDR4, 2133MHz System: Pure water box (100 x 100 x 100 A^3), 95,577 atoms PPPM: double precision FFTW3, 1E-4 CUTOFF: 10A Steps: 1000 | Timings | Plain | OPT | USER-OMP | USER-INTEL | | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------- | --------------- | ---------- | | | | | | | | | | | | Total | 50.07 | 45.20 | 41.41 | 33.75 | | Pair | 37.55 | 33.11 | 29.07 | 21.54 | | Kspace | 6.48 | 6.86 | 5.89 | 5.75 | | Neighbor | 3.18 | 2.29 | 3.08 | 2.95 | | Communication | 0.90 | 0.85 | 1.02 | 1.01 | | Output | 0.01 | 0.01 | 0.01 | 0.01 | | Other | 1.60 | 1.76 | 2.07 | 2.12 | Numbers are in seconds. Lower is better.","title":"Benchmarks"},{"location":"midway23/software/applications/lammps/#copyright","text":"To get the permission of using the content (text, images and benchmark data) of this document in anywhere else, please send an Email to yuxing@uchicago.edu .","title":"Copyright"},{"location":"midway23/software/applications/mitgcm/","text":"MITgcm MITgcm (the MIT General Circulation Model) can be built and run on Midway. For more information about MITgcm and how to acquire the source code, see http://mitgcm.org/ We recommend using the intel compiler suite with MITgcm. The relevant modules that need to be loaded when working with MITgcm are therefore: $ module load intelmpi/4.1+intel-12.1 $ module load netcdf/4.2+intel-12.1 The only Midway-specific customization you will need to use MITgcm on Midway is the following optfile for use with genmake2: linux_amd64_ifort+mpi_midway You should place this file in your instance of MITgcm\u2019s tools/build_options/ directory and point genmake2 to it with the -of= options. Otherwise, all of the standard MITgcm documentation can be followed verbatim.","title":"[MITgcm](single:MITgcm)"},{"location":"midway23/software/applications/mitgcm/#mitgcm","text":"MITgcm (the MIT General Circulation Model) can be built and run on Midway. For more information about MITgcm and how to acquire the source code, see http://mitgcm.org/ We recommend using the intel compiler suite with MITgcm. The relevant modules that need to be loaded when working with MITgcm are therefore: $ module load intelmpi/4.1+intel-12.1 $ module load netcdf/4.2+intel-12.1 The only Midway-specific customization you will need to use MITgcm on Midway is the following optfile for use with genmake2: linux_amd64_ifort+mpi_midway You should place this file in your instance of MITgcm\u2019s tools/build_options/ directory and point genmake2 to it with the -of= options. Otherwise, all of the standard MITgcm documentation can be followed verbatim.","title":"MITgcm"},{"location":"midway23/software/applications/namd/","text":"NAMD namd.sbatch is a submission script that can be used to submit the apoa1.namd calculation job to the queue. #!/bin/sh #SBATCH --job-name=namd #SBATCH --output=namd-%j.out #SBATCH --constraint=ib #SBATCH --exclusive #SBATCH --nodes=4 module load namd/2.9 mpirun namd2 apoa1.namd The script can be submitted with this command: sbatch namd.sbatch","title":"[NAMD](http://www.ks.uiuc.edu/Research/namd/)"},{"location":"midway23/software/applications/namd/#namd","text":"namd.sbatch is a submission script that can be used to submit the apoa1.namd calculation job to the queue. #!/bin/sh #SBATCH --job-name=namd #SBATCH --output=namd-%j.out #SBATCH --constraint=ib #SBATCH --exclusive #SBATCH --nodes=4 module load namd/2.9 mpirun namd2 apoa1.namd The script can be submitted with this command: sbatch namd.sbatch","title":"NAMD"},{"location":"midway23/software/applications/openfoam/","text":"OpenFOAM OpenFOAM is available through the module system via module load openfoam This loads /software/openfoam-2.1-el6-x86_64/OpenFOAM/OpenFOAM-2.1.x/etc/bashrc This is a bashrc file which adds environment variables Notable entries: WM_PROJECT_DIR WM_THIRD_PARTY_DIR FOAM_APPBIN FOAM_TUTORIALS FOAM_RUN If you use C-shell this will not work. There is an alternate C-shell file, to load this file enter source $WM_PROJECT_DIR/etc/cshrc The recommended starting procedure to verify proper operation is Load openfoam module module load openfoam Create personal OpenFOAM directory mkdir -p $FOAM_RUN (creates $HOME/OpenFOAM) Copy tutorials cp -ruv $FOAM_TUTORIALS $FOAM_RUN Test cd $FOAM_RUN /tutorials/incompressible/icoFoam/cavity blockMesh icoFoam paraFoam NOTE : paraFoam is a graphical program and requires a windowing system. See the RCC FAQ at rcc.uchicago.edu for instructions.","title":"[OpenFOAM](single:OpenFOAM)"},{"location":"midway23/software/applications/openfoam/#openfoam","text":"OpenFOAM is available through the module system via module load openfoam This loads /software/openfoam-2.1-el6-x86_64/OpenFOAM/OpenFOAM-2.1.x/etc/bashrc This is a bashrc file which adds environment variables Notable entries: WM_PROJECT_DIR WM_THIRD_PARTY_DIR FOAM_APPBIN FOAM_TUTORIALS FOAM_RUN If you use C-shell this will not work. There is an alternate C-shell file, to load this file enter source $WM_PROJECT_DIR/etc/cshrc The recommended starting procedure to verify proper operation is Load openfoam module module load openfoam Create personal OpenFOAM directory mkdir -p $FOAM_RUN (creates $HOME/OpenFOAM) Copy tutorials cp -ruv $FOAM_TUTORIALS $FOAM_RUN Test cd $FOAM_RUN /tutorials/incompressible/icoFoam/cavity blockMesh icoFoam paraFoam NOTE : paraFoam is a graphical program and requires a windowing system. See the RCC FAQ at rcc.uchicago.edu for instructions.","title":"OpenFOAM"},{"location":"midway23/software/compilers/GNU/","text":"GNU Compiler Suite The GNU compiler suite is available on Midway and is the system default compiler. By default, the GNU v4.4.7 are avaiable. Additionally, GNU compilers v4.8.2 are avaiable in the gcc/4.8 software module.","title":"GNU Compiler Suite"},{"location":"midway23/software/compilers/GNU/#gnu-compiler-suite","text":"The GNU compiler suite is available on Midway and is the system default compiler. By default, the GNU v4.4.7 are avaiable. Additionally, GNU compilers v4.8.2 are avaiable in the gcc/4.8 software module.","title":"GNU Compiler Suite"},{"location":"midway23/software/compilers/Intel/","text":"Intel Compiler Suite Multiple versions of the Intel compiler suite are available on Midway. At time of writing, the available versions are: intel/11.1 intel/12.1(default) intel/13.1 intel/14.0 intel/15.0 The official documentaiton for the Intel compiler suite is available here: https://software.intel.com/en-us/intel-compilers","title":"Intel Compiler Suite"},{"location":"midway23/software/compilers/Intel/#intel-compiler-suite","text":"Multiple versions of the Intel compiler suite are available on Midway. At time of writing, the available versions are: intel/11.1 intel/12.1(default) intel/13.1 intel/14.0 intel/15.0 The official documentaiton for the Intel compiler suite is available here: https://software.intel.com/en-us/intel-compilers","title":"Intel Compiler Suite"},{"location":"midway23/software/compilers/PGI/","text":"PGI Compiler Suite | Command | Language | Compiler | | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------- | --------------- | ---------- | | | | | | | | | | | | | | | pgf77 | FORTRAN 77 | PGF77 | | pgf95 | Fortran 90/95/F2003 | PGF95 | | pgfortran | PGI Fortran | PGFORTRAN | | pghpf | High Performance Fortran | PGHPF | | pgcc | ANSI C99 and K&R C | PGCC C | | pgCC | ANSI C++ with cfront features | PGC++ | | pgdbg | Source code debugger | PGDBG | | pgprof | Performance profiler | PGPROF | hello.f90 is a Fortran program. Compile and execute this program interactively by entering the following commands into the console: module load pgi pgf95 hello.f90 ./a.out","title":"PGI Compiler Suite"},{"location":"midway23/software/compilers/PGI/#pgi-compiler-suite","text":"| Command | Language | Compiler | | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------- | --------------- | ---------- | | | | | | | | | | | | | | | pgf77 | FORTRAN 77 | PGF77 | | pgf95 | Fortran 90/95/F2003 | PGF95 | | pgfortran | PGI Fortran | PGFORTRAN | | pghpf | High Performance Fortran | PGHPF | | pgcc | ANSI C99 and K&R C | PGCC C | | pgCC | ANSI C++ with cfront features | PGC++ | | pgdbg | Source code debugger | PGDBG | | pgprof | Performance profiler | PGPROF | hello.f90 is a Fortran program. Compile and execute this program interactively by entering the following commands into the console: module load pgi pgf95 hello.f90 ./a.out","title":"PGI Compiler Suite"},{"location":"midway23/software/compilers/nvidia/","text":"CUDA and OpenACC Compilers This page contains information about compiling GPU-based codes with NVidia\u2019s CUDA compiler and PGI\u2019s OpenACC compiler directives. For information on how to run GPU Computing jobs in Midway, see GPU jobs Compiling CUDA GPU code on Midway To view available CUDA versions on Midway, use the command: module avail cuda At time of writing, the available CUDA versions are: cuda/4.2(default) cuda/5.0 cuda/5.5 A very basic CUDA example code is provided below cudamemset.cu : #include <stdio.h> #include <cuda.h> int main (){ int n = 16 ; // host and device memory pointers int *h_a ; int *d_a ; // allocate host memory h_a = ( int* ) malloc ( n * sizeof ( int )) ; // allocate device memory cudaMalloc (( void** ) & d_a, n * sizeof ( int )) ; // set device memory to all zero ' s cudaMemset ( d_a, 0 , n * sizeof ( int )) ; // copy device memory back to host cudaMemcpy ( h_a, d_a, n * sizeof ( int ) , cudaMemcpyDeviceToHost ) ; // print host memory for ( int i = 0 ; i < n ; i++ ){ printf ( \"%d \" , h_a [ i ]) ; } printf ( \"\\n\" ) ; // free buffers free ( h_a ) ; cudaFree ( d_a ) ; return 0 ; } CUDA code must be compiled with Nvidia\u2019s nvcc compiler which is part of the cuda software module. To build a CUDA executable, first load the desired CUDA module and compile with: nvcc source_code.cu Compiling OpenACC GPU code on Midway OpenACC is supported on Midway through the PGI 2013 compiler suite. To load the OpenACC compiler, use the command: module load pgi/2013 A very basic OpenACC example code is provided below stencil.c : #include <stdio.h> #include <stdlib.h> int main (){ int i,j,it ; // set the size of our test arrays int numel = 2000 ; // allocate and initialize test arrays float A [ numel ][ numel ] ; float Anew [ numel ][ numel ] ; for ( i = 0 ; i < numel ; i++ ){ for ( j = 0 ; j < numel ; j++ ){ A [ i ][ j ] = drand48 () ; } } // apply stencil 1000 times #pragma acc data copy(A), create(Anew) for ( it = 0 ; it < 1000 ; it++ ){ #pragma acc parallel loop for ( i = 1 ; i < numel-1 ; i++ ){ for ( j = 1 ; j < numel-1 ; j++ ){ Anew [ i ][ j ] = 0 .25f * ( A [ i ][ j-1 ] + A [ i ][ j+1 ] + A [ i-1 ][ j ] + A [ i+1 ][ j ]) ; } } #pragma acc parallel loop for ( i = 1 ; i < numel-1 ; i++ ){ for ( j = 1 ; j < numel-1 ; j++ ){ A [ i ][ j ] = Anew [ i ][ j ] ; } } } // do something with A [][] return 0 ; } OpenACC code targeted at an Nvidia GPU must be compiled with the PGI compiler using at least the following options: pgcc source_code.c -ta = nvidia -acc","title":"CUDA and OpenACC Compilers"},{"location":"midway23/software/compilers/nvidia/#cuda-and-openacc-compilers","text":"This page contains information about compiling GPU-based codes with NVidia\u2019s CUDA compiler and PGI\u2019s OpenACC compiler directives. For information on how to run GPU Computing jobs in Midway, see GPU jobs","title":"CUDA and OpenACC Compilers"},{"location":"midway23/software/compilers/nvidia/#compiling-cuda-gpu-code-on-midway","text":"To view available CUDA versions on Midway, use the command: module avail cuda At time of writing, the available CUDA versions are: cuda/4.2(default) cuda/5.0 cuda/5.5 A very basic CUDA example code is provided below cudamemset.cu : #include <stdio.h> #include <cuda.h> int main (){ int n = 16 ; // host and device memory pointers int *h_a ; int *d_a ; // allocate host memory h_a = ( int* ) malloc ( n * sizeof ( int )) ; // allocate device memory cudaMalloc (( void** ) & d_a, n * sizeof ( int )) ; // set device memory to all zero ' s cudaMemset ( d_a, 0 , n * sizeof ( int )) ; // copy device memory back to host cudaMemcpy ( h_a, d_a, n * sizeof ( int ) , cudaMemcpyDeviceToHost ) ; // print host memory for ( int i = 0 ; i < n ; i++ ){ printf ( \"%d \" , h_a [ i ]) ; } printf ( \"\\n\" ) ; // free buffers free ( h_a ) ; cudaFree ( d_a ) ; return 0 ; } CUDA code must be compiled with Nvidia\u2019s nvcc compiler which is part of the cuda software module. To build a CUDA executable, first load the desired CUDA module and compile with: nvcc source_code.cu","title":"Compiling CUDA GPU code on Midway"},{"location":"midway23/software/compilers/nvidia/#compiling-openacc-gpu-code-on-midway","text":"OpenACC is supported on Midway through the PGI 2013 compiler suite. To load the OpenACC compiler, use the command: module load pgi/2013 A very basic OpenACC example code is provided below stencil.c : #include <stdio.h> #include <stdlib.h> int main (){ int i,j,it ; // set the size of our test arrays int numel = 2000 ; // allocate and initialize test arrays float A [ numel ][ numel ] ; float Anew [ numel ][ numel ] ; for ( i = 0 ; i < numel ; i++ ){ for ( j = 0 ; j < numel ; j++ ){ A [ i ][ j ] = drand48 () ; } } // apply stencil 1000 times #pragma acc data copy(A), create(Anew) for ( it = 0 ; it < 1000 ; it++ ){ #pragma acc parallel loop for ( i = 1 ; i < numel-1 ; i++ ){ for ( j = 1 ; j < numel-1 ; j++ ){ Anew [ i ][ j ] = 0 .25f * ( A [ i ][ j-1 ] + A [ i ][ j+1 ] + A [ i-1 ][ j ] + A [ i+1 ][ j ]) ; } } #pragma acc parallel loop for ( i = 1 ; i < numel-1 ; i++ ){ for ( j = 1 ; j < numel-1 ; j++ ){ A [ i ][ j ] = Anew [ i ][ j ] ; } } } // do something with A [][] return 0 ; } OpenACC code targeted at an Nvidia GPU must be compiled with the PGI compiler using at least the following options: pgcc source_code.c -ta = nvidia -acc","title":"Compiling OpenACC GPU code on Midway"},{"location":"midway23/software/debugging/ddt/","text":"Allinea DDT Allinea\u2019s DDT (Distributed Debugging Tool) is a powerful, commercial gui-based debugger used in many HPC environments for debugging large MPI and OpenMP parallel programs. The RCC has purchased licenses for 8 processes for use on Midway. This means a total of 8 processes or MPI ranks can be analyzed at one time, either by one user or multiple users. Threads do not count toward the license limit, so a job using up to 128 cores on Midway can be debugged, provided only one MPI rank is assigned to each 16-core node. To analyze an MPI job with more than 8 ranks, the user must attach DDT to a subset of the MPI ranks. See the Allinea DDT User Guide for more information. Usage DDT can be used to debug a wide variety of programs in a number of different ways, and can be a little complicated to configure. To start ddt, use the command: $ module load ddt $ ddt This will bring up the DDT wizard to guide you through the various configuration options: * Run: launch a new program through DDT. This can launch a serial or multithreaded program on the current node (please use an interactive session), or submit a serial, threaded, or MPI job to Slurm. In the latter case DDT will wait for the job to begin execution and automatically attach itself to the job processes. * Attach: Attach the DDT debugger to one or more existing processes. If you are running on a head node, you will need the output of **squeue** to tell DDT which nodes your processes are running on. * Core: load the process state from a core file generated by a now terminated process. Useful for analyzing a program The online help is very good, and should be the first place you look when things aren\u2019t working. RCC has configured DDT to work with all supported MPI environments on Midway, and to submit debugging jobs through the batch queues. You may need to alter the default configuration to suit your particular needs. We recommend leaving the MPI implementation option \u201cgeneric\u201d, as shown below, or \u201cAuto Detect\u201d if you plan to use only one MPI module. The job submission panel controls how and whether jobs are submitted to the SLURM batch system. If you run DDT from an interactive session, deselect the \u201csubmit job through queue\u201d option. Beware, if this is unchecked and you are running DDT on one of the head nodes, your job will run there, not on a compute node. DDT includes a powerful memory checking feature, however it can cause your code to run very slowly due to overhead involved in checking memory accesses and allocations. This memory debugger is not enabled by default, and when enabled can be configured with a variety of checks that affect the resulting overhead. Select \u201cMemory Debugging\u201d details from the job submission window to bring up the full set of options. Be sure to select the language \u201cC/Fortran, threads\u201d when debugging a multi-threaded program or \u201cno threads\u201d when debugging serially. The following sections describe the steps necessary to configure DDT in its various modes. Interactive When running DDT from a compute node interactive session, be sure to deselect the checkbox on the Options panel, \u201cJob Submission\u201d section. DDT will then execute the program to debug on the current machine. The job run window will look like the following. Configure the job options including arguments and working directory, OpenMP threading, and Memory Debugging. DDT batch submission When running MPI jobs on more than one node it is necessary to have DDT submit your job to the batch scheduler (this is also possible for OpenMP or serial codes, however in that case an interactive session will be easier). The run window is very similar to the interactive case, with the ability to alter Queue Submission Parameters. When you select Submit, DDT will bring up the Queue Submission Parameters, which allows you to configure SLURM options based on the following submission template script. If you need to further configure your job script, you can create your own template and use the option in the Job Submission options panel to point DDT there. Instructions on how to customize these templates can be found in the Allinea DDT User Guide , or in the sample script at /software/allinea/tools/templates/sample.qtf . #!/bin/bash # ACCOUNT_TAG: {type=text,label=\"Account\"} # PARTITION_TAG: {type=text,label=\"Partition\",default=\"sandyb\"} # QOS_TAG: {type=text,label=\"QOS\",default=\"debug\"} # CONSTRAINT_TAG: {type=text,label=\"Node Constraints (optional)\",default=\"ib\"} # WALL_CLOCK_LIMIT_TAG: {type=text,label=\"Wall Clock Limit\",default=\"15:00\",mask=\"09:09\"} # MODULES_TAG: {type=text,label=\"Modules to load (optional)\"} #SBATCH --partition=PARTITION_TAG #SBATCH --account=ACCOUNT_TAG #SBATCH --qos=QOS_TAG #SBATCH --nodes=NUM_NODES_TAG #SBATCH --constraint=CONSTRAINT_TAG #SBATCH --ntasks-per-node=PROCS_PER_NODE_TAG #SBATCH --cpus-per-task=NUM_THREADS_TAG #SBATCH --time=WALL_CLOCK_LIMIT_TAG #SBATCH --output=PROGRAM_TAG-ddt-%j.out #SBATCH --no-requeue #SBATCH --exclusive module load MODULES_TAG AUTO_LAUNCH_TAG The default template has mandatory fields for walltime, partition, account, and qos. Constraint and modules fields allow you to request nodes with a gpu or load necessary modules (although DDT will export your current environment, so this should not be necessary in general). NOTE : The maximum wallclock time for the debug qos is 15 minutes. DDT will resubmit your job to the queue as necessary, or you can select the normal qos for a longer walltime. You may request a Slurm reservation from RCC staff to ensure that nodes are available. DDT will continually refresh the output of squeue and wait until your job has started. Be sure that you have selected options that will allow your job to eventually start. Once the job has started and DDT has attached to all running processes you will be taken to the normal debug window for the number of MPI ranks and threads you chose. Some MPI implementations will include threads used by the MPI implementation, which can be safely ignored. Attach to running process In order to attach to a running process you will need to know the node(s) and PID(s) of the processes you wish to examine. The slurm command squeue can be used for the former, and the system call getpid or ps can be used for the latter. DDT can be run directly on the node your The following c code will insert a breakpoint for all processes in an MPI program, allowing you to attach to the correct process and continue from the specified point in the code: void mpi_breakpoint ( int proc ) { int rank , i = 0 ; char host [ 256 ]; MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); if ( rank == proc ) { gethostname ( host , 256 ); printf ( \"%u entering breakpoint from host %s, %s:%u \\n \" , getpid (), host , __FILE__ , __LINE__ + 1 ); while ( i == 0 ) { sleep ( 1 ); } } MPI_Barrier ( MPI_COMM_WORLD ); } Generally the attached process will be inside the sleep system call. Set a breakpoint at the specified line or MPI_Barrier, then set the value of the variable i to a non-zero value to allow the process to proceed. Once the code has returned from the mpi_breakpoint function (after the barrier), you can debug that process normally. Other processes will proceed as normally, only waiting on blocking communication with the attached process(es). Debugging The following images show the DDT debug window for several different program types. Serial OpenMP MPI","title":"Allinea DDT"},{"location":"midway23/software/debugging/ddt/#allinea-ddt","text":"Allinea\u2019s DDT (Distributed Debugging Tool) is a powerful, commercial gui-based debugger used in many HPC environments for debugging large MPI and OpenMP parallel programs. The RCC has purchased licenses for 8 processes for use on Midway. This means a total of 8 processes or MPI ranks can be analyzed at one time, either by one user or multiple users. Threads do not count toward the license limit, so a job using up to 128 cores on Midway can be debugged, provided only one MPI rank is assigned to each 16-core node. To analyze an MPI job with more than 8 ranks, the user must attach DDT to a subset of the MPI ranks. See the Allinea DDT User Guide for more information.","title":"Allinea DDT"},{"location":"midway23/software/debugging/ddt/#usage","text":"DDT can be used to debug a wide variety of programs in a number of different ways, and can be a little complicated to configure. To start ddt, use the command: $ module load ddt $ ddt This will bring up the DDT wizard to guide you through the various configuration options: * Run: launch a new program through DDT. This can launch a serial or multithreaded program on the current node (please use an interactive session), or submit a serial, threaded, or MPI job to Slurm. In the latter case DDT will wait for the job to begin execution and automatically attach itself to the job processes. * Attach: Attach the DDT debugger to one or more existing processes. If you are running on a head node, you will need the output of **squeue** to tell DDT which nodes your processes are running on. * Core: load the process state from a core file generated by a now terminated process. Useful for analyzing a program The online help is very good, and should be the first place you look when things aren\u2019t working. RCC has configured DDT to work with all supported MPI environments on Midway, and to submit debugging jobs through the batch queues. You may need to alter the default configuration to suit your particular needs. We recommend leaving the MPI implementation option \u201cgeneric\u201d, as shown below, or \u201cAuto Detect\u201d if you plan to use only one MPI module. The job submission panel controls how and whether jobs are submitted to the SLURM batch system. If you run DDT from an interactive session, deselect the \u201csubmit job through queue\u201d option. Beware, if this is unchecked and you are running DDT on one of the head nodes, your job will run there, not on a compute node. DDT includes a powerful memory checking feature, however it can cause your code to run very slowly due to overhead involved in checking memory accesses and allocations. This memory debugger is not enabled by default, and when enabled can be configured with a variety of checks that affect the resulting overhead. Select \u201cMemory Debugging\u201d details from the job submission window to bring up the full set of options. Be sure to select the language \u201cC/Fortran, threads\u201d when debugging a multi-threaded program or \u201cno threads\u201d when debugging serially. The following sections describe the steps necessary to configure DDT in its various modes.","title":"Usage"},{"location":"midway23/software/debugging/ddt/#interactive","text":"When running DDT from a compute node interactive session, be sure to deselect the checkbox on the Options panel, \u201cJob Submission\u201d section. DDT will then execute the program to debug on the current machine. The job run window will look like the following. Configure the job options including arguments and working directory, OpenMP threading, and Memory Debugging.","title":"Interactive"},{"location":"midway23/software/debugging/ddt/#ddt-batch-submission","text":"When running MPI jobs on more than one node it is necessary to have DDT submit your job to the batch scheduler (this is also possible for OpenMP or serial codes, however in that case an interactive session will be easier). The run window is very similar to the interactive case, with the ability to alter Queue Submission Parameters. When you select Submit, DDT will bring up the Queue Submission Parameters, which allows you to configure SLURM options based on the following submission template script. If you need to further configure your job script, you can create your own template and use the option in the Job Submission options panel to point DDT there. Instructions on how to customize these templates can be found in the Allinea DDT User Guide , or in the sample script at /software/allinea/tools/templates/sample.qtf . #!/bin/bash # ACCOUNT_TAG: {type=text,label=\"Account\"} # PARTITION_TAG: {type=text,label=\"Partition\",default=\"sandyb\"} # QOS_TAG: {type=text,label=\"QOS\",default=\"debug\"} # CONSTRAINT_TAG: {type=text,label=\"Node Constraints (optional)\",default=\"ib\"} # WALL_CLOCK_LIMIT_TAG: {type=text,label=\"Wall Clock Limit\",default=\"15:00\",mask=\"09:09\"} # MODULES_TAG: {type=text,label=\"Modules to load (optional)\"} #SBATCH --partition=PARTITION_TAG #SBATCH --account=ACCOUNT_TAG #SBATCH --qos=QOS_TAG #SBATCH --nodes=NUM_NODES_TAG #SBATCH --constraint=CONSTRAINT_TAG #SBATCH --ntasks-per-node=PROCS_PER_NODE_TAG #SBATCH --cpus-per-task=NUM_THREADS_TAG #SBATCH --time=WALL_CLOCK_LIMIT_TAG #SBATCH --output=PROGRAM_TAG-ddt-%j.out #SBATCH --no-requeue #SBATCH --exclusive module load MODULES_TAG AUTO_LAUNCH_TAG The default template has mandatory fields for walltime, partition, account, and qos. Constraint and modules fields allow you to request nodes with a gpu or load necessary modules (although DDT will export your current environment, so this should not be necessary in general). NOTE : The maximum wallclock time for the debug qos is 15 minutes. DDT will resubmit your job to the queue as necessary, or you can select the normal qos for a longer walltime. You may request a Slurm reservation from RCC staff to ensure that nodes are available. DDT will continually refresh the output of squeue and wait until your job has started. Be sure that you have selected options that will allow your job to eventually start. Once the job has started and DDT has attached to all running processes you will be taken to the normal debug window for the number of MPI ranks and threads you chose. Some MPI implementations will include threads used by the MPI implementation, which can be safely ignored.","title":"DDT batch submission"},{"location":"midway23/software/debugging/ddt/#attach-to-running-process","text":"In order to attach to a running process you will need to know the node(s) and PID(s) of the processes you wish to examine. The slurm command squeue can be used for the former, and the system call getpid or ps can be used for the latter. DDT can be run directly on the node your The following c code will insert a breakpoint for all processes in an MPI program, allowing you to attach to the correct process and continue from the specified point in the code: void mpi_breakpoint ( int proc ) { int rank , i = 0 ; char host [ 256 ]; MPI_Comm_rank ( MPI_COMM_WORLD , & rank ); if ( rank == proc ) { gethostname ( host , 256 ); printf ( \"%u entering breakpoint from host %s, %s:%u \\n \" , getpid (), host , __FILE__ , __LINE__ + 1 ); while ( i == 0 ) { sleep ( 1 ); } } MPI_Barrier ( MPI_COMM_WORLD ); } Generally the attached process will be inside the sleep system call. Set a breakpoint at the specified line or MPI_Barrier, then set the value of the variable i to a non-zero value to allow the process to proceed. Once the code has returned from the mpi_breakpoint function (after the barrier), you can debug that process normally. Other processes will proceed as normally, only waiting on blocking communication with the attached process(es).","title":"Attach to running process"},{"location":"midway23/software/debugging/ddt/#debugging","text":"The following images show the DDT debug window for several different program types.","title":"Debugging"},{"location":"midway23/software/debugging/ddt/#serial","text":"","title":"Serial"},{"location":"midway23/software/debugging/ddt/#openmp","text":"","title":"OpenMP"},{"location":"midway23/software/debugging/ddt/#mpi","text":"","title":"MPI"},{"location":"midway23/software/debugging/hpctoolkit/","text":"HPC Toolkit HPC Toolkit is an open source suite of profiling and performance analysis. It requires recompiling the code to be instrumented, but otherwise the code can remain unchanged. Compile Compile source files as normally using your normal compiler. Prepend the command hpclink to the linking stage and statically link: $ module load hpctoolkit $ hpclink gcc -static -g foo.c Or in the case of an MPI code: $ module load hpctoolkit/5.3+openmpi-1.6 $ hpclink mpicc -static -g <source>.c -o <executable> Currently HPC Toolkit modules are created only for GNU compiler based MPI environments. Gather Sampling Data The command hpcrun is used to run the instrumented program and collect sampling data. The option -l will list the available events that may be pofiled, including those defined by PAPI . The user can control which events are profiled and at what frequency using the option -e : $ hpcrun -e PAPI_L2_DCM@510011 <executable> or: $ mpirun hpcrun -e PAPI_L2_DCM@510011 <executable> Multiple events can be profiled in a single invokation of hpcrun , however not all events are compatible. It may also be necessary to run hpcrun multiple times to gather sufficient events to capture relatively rare events. Statially Instrument Code HPC Toolkit performs a static analysis of the programs original source in order to properly interpret the sampling data. Use the command hpcstruct : $ hpcstruct <executable> Correlate Source and Sampling Data The hpcprof command collects all available samples and correlates them with the static analysis produced by hpcstruct : $ hpcprof -I path-to-source -S <executable>.hpcstruct hpctoolkit-<executable>-measurements Analysis Once the database of measurements has been created, a separate module is available with a program to graphically visualize and explore the data: $ module load hpcviewer $ hpcviewer hpctoolkit-<executable>-database","title":"HPC Toolkit"},{"location":"midway23/software/debugging/hpctoolkit/#hpc-toolkit","text":"HPC Toolkit is an open source suite of profiling and performance analysis. It requires recompiling the code to be instrumented, but otherwise the code can remain unchanged.","title":"HPC Toolkit"},{"location":"midway23/software/debugging/hpctoolkit/#compile","text":"Compile source files as normally using your normal compiler. Prepend the command hpclink to the linking stage and statically link: $ module load hpctoolkit $ hpclink gcc -static -g foo.c Or in the case of an MPI code: $ module load hpctoolkit/5.3+openmpi-1.6 $ hpclink mpicc -static -g <source>.c -o <executable> Currently HPC Toolkit modules are created only for GNU compiler based MPI environments.","title":"Compile"},{"location":"midway23/software/debugging/hpctoolkit/#gather-sampling-data","text":"The command hpcrun is used to run the instrumented program and collect sampling data. The option -l will list the available events that may be pofiled, including those defined by PAPI . The user can control which events are profiled and at what frequency using the option -e : $ hpcrun -e PAPI_L2_DCM@510011 <executable> or: $ mpirun hpcrun -e PAPI_L2_DCM@510011 <executable> Multiple events can be profiled in a single invokation of hpcrun , however not all events are compatible. It may also be necessary to run hpcrun multiple times to gather sufficient events to capture relatively rare events.","title":"Gather Sampling Data"},{"location":"midway23/software/debugging/hpctoolkit/#statially-instrument-code","text":"HPC Toolkit performs a static analysis of the programs original source in order to properly interpret the sampling data. Use the command hpcstruct : $ hpcstruct <executable>","title":"Statially Instrument Code"},{"location":"midway23/software/debugging/hpctoolkit/#correlate-source-and-sampling-data","text":"The hpcprof command collects all available samples and correlates them with the static analysis produced by hpcstruct : $ hpcprof -I path-to-source -S <executable>.hpcstruct hpctoolkit-<executable>-measurements","title":"Correlate Source and Sampling Data"},{"location":"midway23/software/debugging/hpctoolkit/#analysis","text":"Once the database of measurements has been created, a separate module is available with a program to graphically visualize and explore the data: $ module load hpcviewer $ hpcviewer hpctoolkit-<executable>-database","title":"Analysis"},{"location":"midway23/software/debugging/papi/","text":"PAPI PAPI is a multi-platform library for portably accessing hardware counters for event profiling of software including flop counts, cache efficiency, and branch prediction rates. See the PAPI website for more information. Usage The user must add PAPI function calls to their code and link to the PAPI library in order to access the hardware counters. Often PAPI calls can be added to previously instrumented code through timing calls, otherwise the user will need to identify the subset of the code to be profiled. An example code that uses PAPI to identify poor cache performance is located below. Available Counters The command papi_avail will determine which PAPI counters are accessible on the current system. Some counters are supported natively, and others can be derived from other counters that are natively supported. PAPI also supports multiplexing, where a larger number of events can be profiled simultaneously using a sampling technique. See the PAPI documentation for more details. NOTE : The number of active counters is much less than the number of counters available on the system. Sandy Bridge nodes have 11 registers that can be used for hardware counters, but PAPI requires a few of those registers for internal functions. In practice, ~4 independent PAPI events can be instrumented at one time, and valid combinations of events must be found using trial-and-error. $ module load papi/5.1 $ papi_avail -a Available events and hardware information. -------------------------------------------------------------------------------- PAPI Version : 5 .1.0.2 Vendor string and code : GenuineIntel ( 1 ) Model string and code : Intel ( R ) Xeon ( R ) CPU E5-2670 0 @ 2 .60GHz ( 45 ) CPU Revision : 7 .000000 CPUID Info : Family: 6 Model: 45 Stepping: 7 CPU Max Megahertz : 2599 CPU Min Megahertz : 2599 Hdw Threads per core : 2 Cores per Socket : 8 NUMA Nodes : 2 CPUs per Node : 16 Total CPUs : 32 Running in a VM : no Number Hardware Counters : 11 Max Multiplex Counters : 64 -------------------------------------------------------------------------------- Name Code Deriv Description ( Note ) PAPI_L1_DCM 0x80000000 No Level 1 data cache misses PAPI_L1_ICM 0x80000001 No Level 1 instruction cache misses PAPI_L2_DCM 0x80000002 Yes Level 2 data cache misses PAPI_L2_ICM 0x80000003 No Level 2 instruction cache misses PAPI_L1_TCM 0x80000006 Yes Level 1 cache misses PAPI_L2_TCM 0x80000007 No Level 2 cache misses PAPI_L3_TCM 0x80000008 No Level 3 cache misses PAPI_TLB_DM 0x80000014 Yes Data translation lookaside buffer misses PAPI_TLB_IM 0x80000015 No Instruction translation lookaside buffer misses PAPI_L1_LDM 0x80000017 No Level 1 load misses PAPI_L1_STM 0x80000018 No Level 1 store misses PAPI_L2_STM 0x8000001a No Level 2 store misses PAPI_STL_ICY 0x80000025 No Cycles with no instruction issue PAPI_BR_UCN 0x8000002a Yes Unconditional branch instructions PAPI_BR_CN 0x8000002b No Conditional branch instructions PAPI_BR_TKN 0x8000002c Yes Conditional branch instructions taken PAPI_BR_NTK 0x8000002d No Conditional branch instructions not taken PAPI_BR_MSP 0x8000002e No Conditional branch instructions mispredicted PAPI_BR_PRC 0x8000002f Yes Conditional branch instructions correctly predicted PAPI_TOT_INS 0x80000032 No Instructions completed PAPI_FP_INS 0x80000034 Yes Floating point instructions PAPI_LD_INS 0x80000035 No Load instructions PAPI_SR_INS 0x80000036 No Store instructions PAPI_BR_INS 0x80000037 No Branch instructions PAPI_TOT_CYC 0x8000003b No Total cycles PAPI_L2_DCH 0x8000003f Yes Level 2 data cache hits PAPI_L2_DCA 0x80000041 No Level 2 data cache accesses PAPI_L3_DCA 0x80000042 Yes Level 3 data cache accesses PAPI_L2_DCR 0x80000044 No Level 2 data cache reads PAPI_L3_DCR 0x80000045 No Level 3 data cache reads PAPI_L2_DCW 0x80000047 No Level 2 data cache writes PAPI_L3_DCW 0x80000048 No Level 3 data cache writes PAPI_L2_ICH 0x8000004a No Level 2 instruction cache hits PAPI_L2_ICA 0x8000004d No Level 2 instruction cache accesses PAPI_L3_ICA 0x8000004e No Level 3 instruction cache accesses PAPI_L2_ICR 0x80000050 No Level 2 instruction cache reads PAPI_L3_ICR 0x80000051 No Level 3 instruction cache reads PAPI_L2_TCA 0x80000059 Yes Level 2 total cache accesses PAPI_L3_TCA 0x8000005a No Level 3 total cache accesses PAPI_L2_TCR 0x8000005c Yes Level 2 total cache reads PAPI_L3_TCR 0x8000005d Yes Level 3 total cache reads PAPI_L2_TCW 0x8000005f No Level 2 total cache writes PAPI_L3_TCW 0x80000060 No Level 3 total cache writes PAPI_FDV_INS 0x80000063 No Floating point divide instructions PAPI_FP_OPS 0x80000066 Yes Floating point operations PAPI_SP_OPS 0x80000067 Yes Floating point operations ; optimized to count scaled single precision vector operations PAPI_DP_OPS 0x80000068 Yes Floating point operations ; optimized to count scaled double precision vector operations PAPI_VEC_SP 0x80000069 Yes Single precision vector/SIMD instructions PAPI_VEC_DP 0x8000006a Yes Double precision vector/SIMD instructions PAPI_REF_CYC 0x8000006b No Reference clock cycles ------------------------------------------------------------------------- Of 50 available events, 17 are derived. avail.c PASSED Example Download matrixmult_papi.c for an example using PAPI to measure the L2 cache miss rate for a poorly-written matrix multiplication program: $ module load papi/5.1 $ gcc -O2 matrixmult_papi.c -lpapi $ ./a.out 322761027 L2 cache misses (0.744% misses) in 5740137120 cycles The precise output will vary with the system load. Reversing the order of the inner two loops should produce a significant improvement in cache efficiency and a corresponding speedup.","title":"PAPI"},{"location":"midway23/software/debugging/papi/#papi","text":"PAPI is a multi-platform library for portably accessing hardware counters for event profiling of software including flop counts, cache efficiency, and branch prediction rates. See the PAPI website for more information.","title":"PAPI"},{"location":"midway23/software/debugging/papi/#usage","text":"The user must add PAPI function calls to their code and link to the PAPI library in order to access the hardware counters. Often PAPI calls can be added to previously instrumented code through timing calls, otherwise the user will need to identify the subset of the code to be profiled. An example code that uses PAPI to identify poor cache performance is located below.","title":"Usage"},{"location":"midway23/software/debugging/papi/#available-counters","text":"The command papi_avail will determine which PAPI counters are accessible on the current system. Some counters are supported natively, and others can be derived from other counters that are natively supported. PAPI also supports multiplexing, where a larger number of events can be profiled simultaneously using a sampling technique. See the PAPI documentation for more details. NOTE : The number of active counters is much less than the number of counters available on the system. Sandy Bridge nodes have 11 registers that can be used for hardware counters, but PAPI requires a few of those registers for internal functions. In practice, ~4 independent PAPI events can be instrumented at one time, and valid combinations of events must be found using trial-and-error. $ module load papi/5.1 $ papi_avail -a Available events and hardware information. -------------------------------------------------------------------------------- PAPI Version : 5 .1.0.2 Vendor string and code : GenuineIntel ( 1 ) Model string and code : Intel ( R ) Xeon ( R ) CPU E5-2670 0 @ 2 .60GHz ( 45 ) CPU Revision : 7 .000000 CPUID Info : Family: 6 Model: 45 Stepping: 7 CPU Max Megahertz : 2599 CPU Min Megahertz : 2599 Hdw Threads per core : 2 Cores per Socket : 8 NUMA Nodes : 2 CPUs per Node : 16 Total CPUs : 32 Running in a VM : no Number Hardware Counters : 11 Max Multiplex Counters : 64 -------------------------------------------------------------------------------- Name Code Deriv Description ( Note ) PAPI_L1_DCM 0x80000000 No Level 1 data cache misses PAPI_L1_ICM 0x80000001 No Level 1 instruction cache misses PAPI_L2_DCM 0x80000002 Yes Level 2 data cache misses PAPI_L2_ICM 0x80000003 No Level 2 instruction cache misses PAPI_L1_TCM 0x80000006 Yes Level 1 cache misses PAPI_L2_TCM 0x80000007 No Level 2 cache misses PAPI_L3_TCM 0x80000008 No Level 3 cache misses PAPI_TLB_DM 0x80000014 Yes Data translation lookaside buffer misses PAPI_TLB_IM 0x80000015 No Instruction translation lookaside buffer misses PAPI_L1_LDM 0x80000017 No Level 1 load misses PAPI_L1_STM 0x80000018 No Level 1 store misses PAPI_L2_STM 0x8000001a No Level 2 store misses PAPI_STL_ICY 0x80000025 No Cycles with no instruction issue PAPI_BR_UCN 0x8000002a Yes Unconditional branch instructions PAPI_BR_CN 0x8000002b No Conditional branch instructions PAPI_BR_TKN 0x8000002c Yes Conditional branch instructions taken PAPI_BR_NTK 0x8000002d No Conditional branch instructions not taken PAPI_BR_MSP 0x8000002e No Conditional branch instructions mispredicted PAPI_BR_PRC 0x8000002f Yes Conditional branch instructions correctly predicted PAPI_TOT_INS 0x80000032 No Instructions completed PAPI_FP_INS 0x80000034 Yes Floating point instructions PAPI_LD_INS 0x80000035 No Load instructions PAPI_SR_INS 0x80000036 No Store instructions PAPI_BR_INS 0x80000037 No Branch instructions PAPI_TOT_CYC 0x8000003b No Total cycles PAPI_L2_DCH 0x8000003f Yes Level 2 data cache hits PAPI_L2_DCA 0x80000041 No Level 2 data cache accesses PAPI_L3_DCA 0x80000042 Yes Level 3 data cache accesses PAPI_L2_DCR 0x80000044 No Level 2 data cache reads PAPI_L3_DCR 0x80000045 No Level 3 data cache reads PAPI_L2_DCW 0x80000047 No Level 2 data cache writes PAPI_L3_DCW 0x80000048 No Level 3 data cache writes PAPI_L2_ICH 0x8000004a No Level 2 instruction cache hits PAPI_L2_ICA 0x8000004d No Level 2 instruction cache accesses PAPI_L3_ICA 0x8000004e No Level 3 instruction cache accesses PAPI_L2_ICR 0x80000050 No Level 2 instruction cache reads PAPI_L3_ICR 0x80000051 No Level 3 instruction cache reads PAPI_L2_TCA 0x80000059 Yes Level 2 total cache accesses PAPI_L3_TCA 0x8000005a No Level 3 total cache accesses PAPI_L2_TCR 0x8000005c Yes Level 2 total cache reads PAPI_L3_TCR 0x8000005d Yes Level 3 total cache reads PAPI_L2_TCW 0x8000005f No Level 2 total cache writes PAPI_L3_TCW 0x80000060 No Level 3 total cache writes PAPI_FDV_INS 0x80000063 No Floating point divide instructions PAPI_FP_OPS 0x80000066 Yes Floating point operations PAPI_SP_OPS 0x80000067 Yes Floating point operations ; optimized to count scaled single precision vector operations PAPI_DP_OPS 0x80000068 Yes Floating point operations ; optimized to count scaled double precision vector operations PAPI_VEC_SP 0x80000069 Yes Single precision vector/SIMD instructions PAPI_VEC_DP 0x8000006a Yes Double precision vector/SIMD instructions PAPI_REF_CYC 0x8000006b No Reference clock cycles ------------------------------------------------------------------------- Of 50 available events, 17 are derived. avail.c PASSED","title":"Available Counters"},{"location":"midway23/software/debugging/papi/#example","text":"Download matrixmult_papi.c for an example using PAPI to measure the L2 cache miss rate for a poorly-written matrix multiplication program: $ module load papi/5.1 $ gcc -O2 matrixmult_papi.c -lpapi $ ./a.out 322761027 L2 cache misses (0.744% misses) in 5740137120 cycles The precise output will vary with the system load. Reversing the order of the inner two loops should produce a significant improvement in cache efficiency and a corresponding speedup.","title":"Example"},{"location":"midway23/software/debugging/valgrind/","text":"Valgrind Valgrind is an open source set of debugging and profiling tools. It is most commonly used to locate memory errors, including leaks, but also can be used to debug threaded codes and profile cache efficiency. See the Valgrind Online Documentation for more information. Usage The following snippet shows how to load the valgrind module and use it to perform analysis on a c code. module load valgrind gcc -g source.c valgrind --tool =[ memcheck,cachgrind,helgrind ] ./a.out If no tool is specified, valgrind will default to the memory checker. Memcheck Memcheck is a tool to detect a wide range of memory errors including buffer over-runs, memory leaks and double-freeing of heap blocks, and uninitialized variables. Download memleak.c : for a simple example of using the cachegrind module to identify a memory leak: $ module load valgrind $ gcc -g memleak.c $ valgrind --tool=memcheck ./a.out ==3153== Memcheck, a memory error detector ==3153== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al. ==3153== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info ==3153== Command: ./a.out ==3153== ==3153== ==3153== HEAP SUMMARY: ==3153== in use at exit: 800 bytes in 10 blocks ==3153== total heap usage: 10 allocs, 0 frees, 800 bytes allocated ==3153== ==3153== LEAK SUMMARY: ==3153== definitely lost: 800 bytes in 10 blocks ==3153== indirectly lost: 0 bytes in 0 blocks ==3153== possibly lost: 0 bytes in 0 blocks ==3153== still reachable: 0 bytes in 0 blocks ==3153== suppressed: 0 bytes in 0 blocks ==3153== Rerun with --leak-check=full to see details of leaked memory ==3153== ==3153== For counts of detected and suppressed errors, rerun with: -v ==3153== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 6 from 6) Running memcheck without any options identifies that 800 bytes were not freed at the time the program terminated, and those were allocated in 10 distinct blocks. To get a better idea of where those blocks were allocated, use the option --leak-check=full : $ valgrind --tool=memcheck --leak-check=full ./a.out ==3154== Memcheck, a memory error detector ==3154== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al. ==3154== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info ==3154== Command: ./a.out ==3154== ==3154== ==3154== HEAP SUMMARY: ==3154== in use at exit: 800 bytes in 10 blocks ==3154== total heap usage: 10 allocs, 0 frees, 800 bytes allocated ==3154== ==3154== 800 bytes in 10 blocks are definitely lost in loss record 1 of 1 ==3154== at 0x4C278FE: malloc (vg_replace_malloc.c:270) ==3154== by 0x400575: main (memleak.c:i24) ==3154== ==3154== LEAK SUMMARY: ==3154== definitely lost: 800 bytes in 10 blocks ==3154== indirectly lost: 0 bytes in 0 blocks ==3154== possibly lost: 0 bytes in 0 blocks ==3154== still reachable: 0 bytes in 0 blocks ==3154== suppressed: 0 bytes in 0 blocks ==3154== ==3154== For counts of detected and suppressed errors, rerun with: -v ==3154== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 6 from 6) Now memcheck has identified that the 10 code blocks were allocated at memleak.c line 10, and the user can modify the code to free those allocations at the appropriate place. Cachegrind Cachegrind is a Valgrind tool that simulates (rather than measures) how a code interact with the multi-level caches found in modern computer architectures. It is very useful for identifying cache misses as a performance problem, as well as identifying parts of the code responsible. Cachegrind does have several limitations, and can dramatically increase the time it takes to execute a code. See the cachgrind manual for full details. Download matrixmult.c for a simple example using the cachegrind module to estimate cache efficiency: $ module load valgrind $ gcc -g matrixmult.c $ valgrind --tool=cachegrind ./a.out ==2548== Cachegrind, a cache and branch-prediction profiler ==2548== Copyright (C) 2002-2012, and GNU GPL'd, by Nicholas Nethercote et al. ==2548== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info ==2548== Command: ./a.out ==2548== --2548-- warning: L3 cache found, using its data for the LL simulation. ==2548== ==2548== I refs: 3,252,178,387 ==2548== I1 misses: 745 ==2548== LLi misses: 738 ==2548== I1 miss rate: 0.00% ==2548== LLi miss rate: 0.00% ==2548== ==2548== D refs: 1,082,643,679 (720,139,382 rd + 362,504,297 wr) ==2548== D1 misses: 406,465,246 (405,103,433 rd + 1,361,813 wr) ==2548== LLd misses: 313,706 ( 1,950 rd + 311,756 wr) ==2548== D1 miss rate: 37.5% ( 56.2% + 0.3% ) ==2548== LLd miss rate: 0.0% ( 0.0% + 0.0% ) ==2548== ==2548== LL refs: 406,465,991 (405,104,178 rd + 1,361,813 wr) ==2548== LL misses: 314,444 ( 2,688 rd + 311,756 wr) ==2548== LL miss rate: 0.0% ( 0.0% + 0.0% ) The above output shows that the example code has a greater than 50% read cache miss rate, which will significantly degrade performance. Since the code was compiled with the -g compiler flag, the cg_annotate tool can be used to parse cachgrind output and produce a line-by-line annotated report: $ cg_annotate --auto=yes cachegrind.out.2548 -------------------------------------------------------------------------------- I1 cache: 32768 B, 64 B, 8-way associative D1 cache: 32768 B, 64 B, 8-way associative LL cache: 20971520 B, 64 B, 20-way associative Command: ./a.out Data file: cachegrind.out.2548 Events recorded: Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw Events shown: Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw Event sort order: Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw Thresholds: 0.1 100 100 100 100 100 100 100 100 Include dirs: User annotated: Auto-annotation: on -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw -------------------------------------------------------------------------------- 3,252,178,387 745 738 720,139,382 405,103,433 1,950 362,504,297 1,361,813 311,756 PROGRAM TOTALS -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw file:function -------------------------------------------------------------------------------- 3,251,974,540 6 6 720,090,005 405,100,952 0 362,490,010 1,361,251 311,250 /home/drudd/debug/matrixmult.c:main -------------------------------------------------------------------------------- -- Auto-annotated source: /home/drudd/debug/matrixmult.c -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw -- line 12 ---------------------------------------- . . . . . . . . . *******************************************************************/ . . . . . . . . . #include <stdlib.h> . . . . . . . . . #include <stdio.h> . . . . . . . . . #include <math.h> . . . . . . . . . . . . . . . . . . #define N 300 . . . . . . . . . #define M 4000 . . . . . . . . . 909 0 0 0 0 0 3 0 0 int main( int argc, char *argv[] ) { . . . . . . . . . int i, j, k; . . . . . . . . . double *A, *B, *C; . . . . . . . . . double tmp; . . . . . . . . . 3 1 1 0 0 0 1 0 0 A = (double *)malloc(N*M*sizeof(double)); 3 0 0 0 0 0 1 0 0 B = (double *)malloc(N*M*sizeof(double)); 2 0 0 0 0 0 1 0 0 C = (double *)malloc(N*N*sizeof(double)); . . . . . . . . . 7 1 1 0 0 0 0 0 0 if ( A == NULL || B == NULL || C == NULL ) { . . . . . . . . . fprintf(stderr,\"Error allocating memory!\\n\"); . . . . . . . . . exit(1); . . . . . . . . . } . . . . . . . . . . . . . . . . . . /* initialize A & B */ 1,801 0 0 0 0 0 0 0 0 for ( i = 0; i < N; i++ ) { 6,000,600 1 1 0 0 0 0 0 0 for ( j = 0; j < M; j++ ) { 2,400,000 0 0 0 0 0 1,200,000 150,000 150,000 A[M*i+j] = 3.0; 2,400,000 0 0 0 0 0 1,200,000 1,199,999 150,000 B[N*j+i] = 2.0; . . . . . . . . . } . . . . . . . . . } . . . . . . . . . 180,001 0 0 0 0 0 0 0 0 for ( i = 0; i < N*N; i++ ) { 180,000 0 0 0 0 0 90,000 11,251 11,250 C[i] = 0.0; . . . . . . . . . } . . . . . . . . . 600 0 0 0 0 0 0 0 0 for ( i = 0; i < N; i++ ) { 630,600 2 2 90,000 11,251 0 0 0 0 for ( j = 0; j < N; j++ ) { 1,800,180,000 0 0 0 0 0 0 0 0 for ( k = 0; k < M; k++ ) { 1,440,000,000 0 0 720,000,000 405,089,701 0 360,000,000 0 0 C[N*i+j] += A[M*i+k]*B[N*k+j]; . . . . . . . . . } . . . . . . . . . } . . . . . . . . . } . . . . . . . . . 3 0 0 0 0 0 2 1 0 free(A); 2 0 0 0 0 0 1 0 0 free(B); 3 0 0 1 0 0 1 0 0 free(C); . . . . . . . . . . . . . . . . . . return 0; 6 1 1 4 0 0 0 0 0 } -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw -------------------------------------------------------------------------------- 100 1 1 100 100 0 100 100 100 percentage of events annotated Note that in this example the loop order causes very poor cache performance for the innermost line of the nested loop. Exchanging the k and j indexed loops will give significantly better performance. Still better performance can be obtained through blocking, or, as this is a standard linear algebra opperation, using LAPACK or the Intel Math Kernel Library, which has tuned routines for performing such calculations. Helgrind Helgrind is a thread error checking tool. Unfortunately it has poor interaction with gcc\u2019s OpenMP implementation, and can lead to a large number of distracting messages. Still, it can be useful in identifying races or unprotected critical sections within shared memory parallel code.","title":"Valgrind"},{"location":"midway23/software/debugging/valgrind/#valgrind","text":"Valgrind is an open source set of debugging and profiling tools. It is most commonly used to locate memory errors, including leaks, but also can be used to debug threaded codes and profile cache efficiency. See the Valgrind Online Documentation for more information.","title":"Valgrind"},{"location":"midway23/software/debugging/valgrind/#usage","text":"The following snippet shows how to load the valgrind module and use it to perform analysis on a c code. module load valgrind gcc -g source.c valgrind --tool =[ memcheck,cachgrind,helgrind ] ./a.out If no tool is specified, valgrind will default to the memory checker.","title":"Usage"},{"location":"midway23/software/debugging/valgrind/#memcheck","text":"Memcheck is a tool to detect a wide range of memory errors including buffer over-runs, memory leaks and double-freeing of heap blocks, and uninitialized variables. Download memleak.c : for a simple example of using the cachegrind module to identify a memory leak: $ module load valgrind $ gcc -g memleak.c $ valgrind --tool=memcheck ./a.out ==3153== Memcheck, a memory error detector ==3153== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al. ==3153== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info ==3153== Command: ./a.out ==3153== ==3153== ==3153== HEAP SUMMARY: ==3153== in use at exit: 800 bytes in 10 blocks ==3153== total heap usage: 10 allocs, 0 frees, 800 bytes allocated ==3153== ==3153== LEAK SUMMARY: ==3153== definitely lost: 800 bytes in 10 blocks ==3153== indirectly lost: 0 bytes in 0 blocks ==3153== possibly lost: 0 bytes in 0 blocks ==3153== still reachable: 0 bytes in 0 blocks ==3153== suppressed: 0 bytes in 0 blocks ==3153== Rerun with --leak-check=full to see details of leaked memory ==3153== ==3153== For counts of detected and suppressed errors, rerun with: -v ==3153== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 6 from 6) Running memcheck without any options identifies that 800 bytes were not freed at the time the program terminated, and those were allocated in 10 distinct blocks. To get a better idea of where those blocks were allocated, use the option --leak-check=full : $ valgrind --tool=memcheck --leak-check=full ./a.out ==3154== Memcheck, a memory error detector ==3154== Copyright (C) 2002-2012, and GNU GPL'd, by Julian Seward et al. ==3154== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info ==3154== Command: ./a.out ==3154== ==3154== ==3154== HEAP SUMMARY: ==3154== in use at exit: 800 bytes in 10 blocks ==3154== total heap usage: 10 allocs, 0 frees, 800 bytes allocated ==3154== ==3154== 800 bytes in 10 blocks are definitely lost in loss record 1 of 1 ==3154== at 0x4C278FE: malloc (vg_replace_malloc.c:270) ==3154== by 0x400575: main (memleak.c:i24) ==3154== ==3154== LEAK SUMMARY: ==3154== definitely lost: 800 bytes in 10 blocks ==3154== indirectly lost: 0 bytes in 0 blocks ==3154== possibly lost: 0 bytes in 0 blocks ==3154== still reachable: 0 bytes in 0 blocks ==3154== suppressed: 0 bytes in 0 blocks ==3154== ==3154== For counts of detected and suppressed errors, rerun with: -v ==3154== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 6 from 6) Now memcheck has identified that the 10 code blocks were allocated at memleak.c line 10, and the user can modify the code to free those allocations at the appropriate place.","title":"Memcheck"},{"location":"midway23/software/debugging/valgrind/#cachegrind","text":"Cachegrind is a Valgrind tool that simulates (rather than measures) how a code interact with the multi-level caches found in modern computer architectures. It is very useful for identifying cache misses as a performance problem, as well as identifying parts of the code responsible. Cachegrind does have several limitations, and can dramatically increase the time it takes to execute a code. See the cachgrind manual for full details. Download matrixmult.c for a simple example using the cachegrind module to estimate cache efficiency: $ module load valgrind $ gcc -g matrixmult.c $ valgrind --tool=cachegrind ./a.out ==2548== Cachegrind, a cache and branch-prediction profiler ==2548== Copyright (C) 2002-2012, and GNU GPL'd, by Nicholas Nethercote et al. ==2548== Using Valgrind-3.8.1 and LibVEX; rerun with -h for copyright info ==2548== Command: ./a.out ==2548== --2548-- warning: L3 cache found, using its data for the LL simulation. ==2548== ==2548== I refs: 3,252,178,387 ==2548== I1 misses: 745 ==2548== LLi misses: 738 ==2548== I1 miss rate: 0.00% ==2548== LLi miss rate: 0.00% ==2548== ==2548== D refs: 1,082,643,679 (720,139,382 rd + 362,504,297 wr) ==2548== D1 misses: 406,465,246 (405,103,433 rd + 1,361,813 wr) ==2548== LLd misses: 313,706 ( 1,950 rd + 311,756 wr) ==2548== D1 miss rate: 37.5% ( 56.2% + 0.3% ) ==2548== LLd miss rate: 0.0% ( 0.0% + 0.0% ) ==2548== ==2548== LL refs: 406,465,991 (405,104,178 rd + 1,361,813 wr) ==2548== LL misses: 314,444 ( 2,688 rd + 311,756 wr) ==2548== LL miss rate: 0.0% ( 0.0% + 0.0% ) The above output shows that the example code has a greater than 50% read cache miss rate, which will significantly degrade performance. Since the code was compiled with the -g compiler flag, the cg_annotate tool can be used to parse cachgrind output and produce a line-by-line annotated report: $ cg_annotate --auto=yes cachegrind.out.2548 -------------------------------------------------------------------------------- I1 cache: 32768 B, 64 B, 8-way associative D1 cache: 32768 B, 64 B, 8-way associative LL cache: 20971520 B, 64 B, 20-way associative Command: ./a.out Data file: cachegrind.out.2548 Events recorded: Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw Events shown: Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw Event sort order: Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw Thresholds: 0.1 100 100 100 100 100 100 100 100 Include dirs: User annotated: Auto-annotation: on -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw -------------------------------------------------------------------------------- 3,252,178,387 745 738 720,139,382 405,103,433 1,950 362,504,297 1,361,813 311,756 PROGRAM TOTALS -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw file:function -------------------------------------------------------------------------------- 3,251,974,540 6 6 720,090,005 405,100,952 0 362,490,010 1,361,251 311,250 /home/drudd/debug/matrixmult.c:main -------------------------------------------------------------------------------- -- Auto-annotated source: /home/drudd/debug/matrixmult.c -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw -- line 12 ---------------------------------------- . . . . . . . . . *******************************************************************/ . . . . . . . . . #include <stdlib.h> . . . . . . . . . #include <stdio.h> . . . . . . . . . #include <math.h> . . . . . . . . . . . . . . . . . . #define N 300 . . . . . . . . . #define M 4000 . . . . . . . . . 909 0 0 0 0 0 3 0 0 int main( int argc, char *argv[] ) { . . . . . . . . . int i, j, k; . . . . . . . . . double *A, *B, *C; . . . . . . . . . double tmp; . . . . . . . . . 3 1 1 0 0 0 1 0 0 A = (double *)malloc(N*M*sizeof(double)); 3 0 0 0 0 0 1 0 0 B = (double *)malloc(N*M*sizeof(double)); 2 0 0 0 0 0 1 0 0 C = (double *)malloc(N*N*sizeof(double)); . . . . . . . . . 7 1 1 0 0 0 0 0 0 if ( A == NULL || B == NULL || C == NULL ) { . . . . . . . . . fprintf(stderr,\"Error allocating memory!\\n\"); . . . . . . . . . exit(1); . . . . . . . . . } . . . . . . . . . . . . . . . . . . /* initialize A & B */ 1,801 0 0 0 0 0 0 0 0 for ( i = 0; i < N; i++ ) { 6,000,600 1 1 0 0 0 0 0 0 for ( j = 0; j < M; j++ ) { 2,400,000 0 0 0 0 0 1,200,000 150,000 150,000 A[M*i+j] = 3.0; 2,400,000 0 0 0 0 0 1,200,000 1,199,999 150,000 B[N*j+i] = 2.0; . . . . . . . . . } . . . . . . . . . } . . . . . . . . . 180,001 0 0 0 0 0 0 0 0 for ( i = 0; i < N*N; i++ ) { 180,000 0 0 0 0 0 90,000 11,251 11,250 C[i] = 0.0; . . . . . . . . . } . . . . . . . . . 600 0 0 0 0 0 0 0 0 for ( i = 0; i < N; i++ ) { 630,600 2 2 90,000 11,251 0 0 0 0 for ( j = 0; j < N; j++ ) { 1,800,180,000 0 0 0 0 0 0 0 0 for ( k = 0; k < M; k++ ) { 1,440,000,000 0 0 720,000,000 405,089,701 0 360,000,000 0 0 C[N*i+j] += A[M*i+k]*B[N*k+j]; . . . . . . . . . } . . . . . . . . . } . . . . . . . . . } . . . . . . . . . 3 0 0 0 0 0 2 1 0 free(A); 2 0 0 0 0 0 1 0 0 free(B); 3 0 0 1 0 0 1 0 0 free(C); . . . . . . . . . . . . . . . . . . return 0; 6 1 1 4 0 0 0 0 0 } -------------------------------------------------------------------------------- Ir I1mr ILmr Dr D1mr DLmr Dw D1mw DLmw -------------------------------------------------------------------------------- 100 1 1 100 100 0 100 100 100 percentage of events annotated Note that in this example the loop order causes very poor cache performance for the innermost line of the nested loop. Exchanging the k and j indexed loops will give significantly better performance. Still better performance can be obtained through blocking, or, as this is a standard linear algebra opperation, using LAPACK or the Intel Math Kernel Library, which has tuned routines for performing such calculations.","title":"Cachegrind"},{"location":"midway23/software/debugging/valgrind/#helgrind","text":"Helgrind is a thread error checking tool. Unfortunately it has poor interaction with gcc\u2019s OpenMP implementation, and can lead to a large number of distracting messages. Still, it can be useful in identifying races or unprotected critical sections within shared memory parallel code.","title":"Helgrind"},{"location":"midway23/software/environments/R/","text":"R R is an interactive environment for computing. To find the list of currently available R modules, run: $ module avail R The RStudio IDE is also available as the rstudio module. This provides a graphical interface for developing and running R. To use R in this mode, we recommend connecting to the RCC cluster using Connecting with ThinLinc . NOTE: Some of the examples below have not been revised to reflect updates to the software and hardware on the RCC cluster. If an example does not work, or if you have questions about an example, please contact the RCC Help Desk for guidance. Serial Examples Here is a simple \u201chello world\u201d example to submit an R job to the SLURM queue. This is appropriate for an R job that expects to use a single CPU. sbatch script Rhello.sbatch #!/bin/sh #SBATCH --partition=broadwl #SBATCH --tasks=1 # Load the default version of hello. module load R # Use R CMD BATCH to run Rhello.R. R CMD BATCH --no-save --no-restore Rhello.R R script Rhello.R : print(\"Hello World\") Parallel Examples For parallel computing there are several options depending on whether there should be parallel tasks on a single node only or multiple nodes and the level of flexibility required. There are other R packages available for parallel programming than what is covered here, but we\u2019ll cover some frequently used packages. Multicore On a single node, it is possible to use doParallel and foreach. sbatch script doParallel.sbatch : #!/bin/bash #SBATCH --partition=broadwl #SBATCH --nodes=1 #SBATCH --ntasks-per-node=16 # --ntasks-per-node will be used in doParallel.R to specify the number # of cores to use on the machine. Using 16 will allow us to use all # cores on a sandyb node module load R R CMD BATCH --no-save --no-restore doParallel.R R script doParallel.R : library(doParallel) # use the environment variable SLURM_NTASKS_PER_NODE to set the number of cores registerDoParallel(cores=(Sys.getenv(\"SLURM_NTASKS_PER_NODE\"))) # Bootstrapping iteration example x <- iris[which(iris[,5] != \"setosa\"), c(1,5)] iterations <- 10000 # Number of iterations to run # Parallel version of code # Note the '%dopar%' instruction parallel_time <- system.time({ r <- foreach(icount(iterations), .combine=cbind) %dopar% { ind <- sample(100, 100, replace=TRUE) result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit)) coefficients(result1) } }) # Shows the number of Parallel Workers to be used getDoParWorkers() # Prints the total compute time. parallel_time[\"elapsed\"] Output: Loading required package: foreach Loading required package: iterators Loading required package: parallel [1] \"16\" elapsed 5.157 SNOW For multiple nodes, you can use the SNOW package, which provides a select number of functions to simplify using multi-node clusters. (NOTE OF CAUTION: This example may not work as described. In particular, the workers may not actually run on 4 separate nodes.) sbatch script snow-test.sbatch : #!/bin/bash #SBATCH --job-name=snow-test #SBATCH --partition=broadwl #SBATCH --nodes=4 #SBATCH --time=10 #SBATCH --exclusive module load R # the openmpi module is not loaded by default with R module load openmpi/3.0.0 # Always use -n 1 for the snow package. It uses Rmpi internally to spawn # additional processes dynamically mpirun -np 1 R CMD BATCH --no-save --no-restore snow-test.R R script snow-test.R : ## # Source: http://www.umbc.edu/hpcf/resources-tara/how-to-run-R.html # filename: snow-test.R # # SNOW quick ref: http://www.sfu.ca/~sblay/R/snow.html # # Notes: # - Library loading order matters # - system.time([function]) is an easy way to test optimizations # - parApply is snow parallel version of 'apply' # ## library(Rmpi) library(snow) # Initialize SNOW using MPI communication. The first line will get the # number of MPI processes the scheduler assigned to us. Everything # else is standard SNOW np <- 4 cluster <- makeMPIcluster(np) # Print the hostname for each cluster member sayhello <- function() { info <- Sys.info()[c(\"nodename\", \"machine\")] paste(\"Hello from\", info[1], \"with CPU type\", info[2]) } names <- clusterCall(cluster, sayhello) print(unlist(names)) # Compute row sums in parallel using all processes, then a grand sum # at the end on the master process parallelSum <- function(m, n) { A <- matrix(rnorm(m*n), nrow = m, ncol = n) # Parallelize the summation row.sums <- parApply(cluster, A, 1, sum) print(sum(row.sums)) } # Run the operation over different size matricies system.time(parallelSum(5000, 5000)) # Always stop your cluster and exit MPI to ensure resources are # properly freed. stopCluster(cluster) mpi.exit() Output (trimmed for readability): 64 slaves are spawned successfully. 0 failed. [1] \"Hello from midway197 with CPU type x86_64\" [2] \"Hello from midway197 with CPU type x86_64\" [3] \"Hello from midway197 with CPU type x86_64\" ... [63] \"Hello from midway200 with CPU type x86_64\" [64] \"Hello from midway197 with CPU type x86_64\" [1] -9363.914 user system elapsed 3.988 0.443 5.553 [1] 1 [1] \"Detaching Rmpi. Rmpi cannot be used unless relaunching R.\" Rmpi For multiple nodes, you can also use Rmpi. This is what snow uses internally. It is less convenient than snow, but also more flexible. This page has a number of useful Rmpi examples: http://www.umbc.edu/hpcf/resources-tara-2010/how-to-run-R.php sbatch script Rmpi.sbatch : #!/bin/sh #SBATCH --partition=broadwl #SBATCH --nodes=4 #SBATCH --time=1 #SBATCH --exclusive module load R # The openmpi module is not loaded by default with R. module load openmpi/3.0.0 # Always use -n 1 for the Rmpi package. It spawns additional processes # dynamically mpirun -n 1 R CMD BATCH --no-save --no-restore Rmpi.R R script Rmpi.R : library(Rmpi) # initialize an Rmpi environment ns <- 4 mpi.spawn.Rslaves(nslaves=ns) # send these commands to the slaves mpi.bcast.cmd( id <- mpi.comm.rank() ) mpi.bcast.cmd( ns <- mpi.comm.size() ) mpi.bcast.cmd( host <- mpi.get.processor.name() ) # all slaves execute this command mpi.remote.exec(paste(\"I am\", id, \"of\", ns, \"running on\", host)) # close down the Rmpi environment mpi.close.Rslaves(dellog = FALSE) mpi.exit() Output (trimmed for readability): 64 slaves are spawned successfully. 0 failed. master (rank 0 , comm 1) of size 65 is running on: midway449 slave1 (rank 1 , comm 1) of size 65 is running on: midway449 slave2 (rank 2 , comm 1) of size 65 is running on: midway449 slave3 (rank 3 , comm 1) of size 65 is running on: midway449 ... ... ... slave63 (rank 63, comm 1) of size 65 is running on: midway452 slave64 (rank 64, comm 1) of size 65 is running on: midway449 $slave1 [1] \"I am 1 of 65\" $slave2 [1] \"I am 2 of 65\" ... $slave63 [1] \"I am 63 of 65\" $slave64 [1] \"I am 64 of 65\"","title":"R"},{"location":"midway23/software/environments/R/#r","text":"R is an interactive environment for computing. To find the list of currently available R modules, run: $ module avail R The RStudio IDE is also available as the rstudio module. This provides a graphical interface for developing and running R. To use R in this mode, we recommend connecting to the RCC cluster using Connecting with ThinLinc . NOTE: Some of the examples below have not been revised to reflect updates to the software and hardware on the RCC cluster. If an example does not work, or if you have questions about an example, please contact the RCC Help Desk for guidance.","title":"R"},{"location":"midway23/software/environments/R/#serial-examples","text":"Here is a simple \u201chello world\u201d example to submit an R job to the SLURM queue. This is appropriate for an R job that expects to use a single CPU. sbatch script Rhello.sbatch #!/bin/sh #SBATCH --partition=broadwl #SBATCH --tasks=1 # Load the default version of hello. module load R # Use R CMD BATCH to run Rhello.R. R CMD BATCH --no-save --no-restore Rhello.R R script Rhello.R : print(\"Hello World\")","title":"Serial Examples"},{"location":"midway23/software/environments/R/#parallel-examples","text":"For parallel computing there are several options depending on whether there should be parallel tasks on a single node only or multiple nodes and the level of flexibility required. There are other R packages available for parallel programming than what is covered here, but we\u2019ll cover some frequently used packages.","title":"Parallel Examples"},{"location":"midway23/software/environments/R/#multicore","text":"On a single node, it is possible to use doParallel and foreach. sbatch script doParallel.sbatch : #!/bin/bash #SBATCH --partition=broadwl #SBATCH --nodes=1 #SBATCH --ntasks-per-node=16 # --ntasks-per-node will be used in doParallel.R to specify the number # of cores to use on the machine. Using 16 will allow us to use all # cores on a sandyb node module load R R CMD BATCH --no-save --no-restore doParallel.R R script doParallel.R : library(doParallel) # use the environment variable SLURM_NTASKS_PER_NODE to set the number of cores registerDoParallel(cores=(Sys.getenv(\"SLURM_NTASKS_PER_NODE\"))) # Bootstrapping iteration example x <- iris[which(iris[,5] != \"setosa\"), c(1,5)] iterations <- 10000 # Number of iterations to run # Parallel version of code # Note the '%dopar%' instruction parallel_time <- system.time({ r <- foreach(icount(iterations), .combine=cbind) %dopar% { ind <- sample(100, 100, replace=TRUE) result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit)) coefficients(result1) } }) # Shows the number of Parallel Workers to be used getDoParWorkers() # Prints the total compute time. parallel_time[\"elapsed\"] Output: Loading required package: foreach Loading required package: iterators Loading required package: parallel [1] \"16\" elapsed 5.157","title":"Multicore"},{"location":"midway23/software/environments/R/#snow","text":"For multiple nodes, you can use the SNOW package, which provides a select number of functions to simplify using multi-node clusters. (NOTE OF CAUTION: This example may not work as described. In particular, the workers may not actually run on 4 separate nodes.) sbatch script snow-test.sbatch : #!/bin/bash #SBATCH --job-name=snow-test #SBATCH --partition=broadwl #SBATCH --nodes=4 #SBATCH --time=10 #SBATCH --exclusive module load R # the openmpi module is not loaded by default with R module load openmpi/3.0.0 # Always use -n 1 for the snow package. It uses Rmpi internally to spawn # additional processes dynamically mpirun -np 1 R CMD BATCH --no-save --no-restore snow-test.R R script snow-test.R : ## # Source: http://www.umbc.edu/hpcf/resources-tara/how-to-run-R.html # filename: snow-test.R # # SNOW quick ref: http://www.sfu.ca/~sblay/R/snow.html # # Notes: # - Library loading order matters # - system.time([function]) is an easy way to test optimizations # - parApply is snow parallel version of 'apply' # ## library(Rmpi) library(snow) # Initialize SNOW using MPI communication. The first line will get the # number of MPI processes the scheduler assigned to us. Everything # else is standard SNOW np <- 4 cluster <- makeMPIcluster(np) # Print the hostname for each cluster member sayhello <- function() { info <- Sys.info()[c(\"nodename\", \"machine\")] paste(\"Hello from\", info[1], \"with CPU type\", info[2]) } names <- clusterCall(cluster, sayhello) print(unlist(names)) # Compute row sums in parallel using all processes, then a grand sum # at the end on the master process parallelSum <- function(m, n) { A <- matrix(rnorm(m*n), nrow = m, ncol = n) # Parallelize the summation row.sums <- parApply(cluster, A, 1, sum) print(sum(row.sums)) } # Run the operation over different size matricies system.time(parallelSum(5000, 5000)) # Always stop your cluster and exit MPI to ensure resources are # properly freed. stopCluster(cluster) mpi.exit() Output (trimmed for readability): 64 slaves are spawned successfully. 0 failed. [1] \"Hello from midway197 with CPU type x86_64\" [2] \"Hello from midway197 with CPU type x86_64\" [3] \"Hello from midway197 with CPU type x86_64\" ... [63] \"Hello from midway200 with CPU type x86_64\" [64] \"Hello from midway197 with CPU type x86_64\" [1] -9363.914 user system elapsed 3.988 0.443 5.553 [1] 1 [1] \"Detaching Rmpi. Rmpi cannot be used unless relaunching R.\"","title":"SNOW"},{"location":"midway23/software/environments/R/#rmpi","text":"For multiple nodes, you can also use Rmpi. This is what snow uses internally. It is less convenient than snow, but also more flexible. This page has a number of useful Rmpi examples: http://www.umbc.edu/hpcf/resources-tara-2010/how-to-run-R.php sbatch script Rmpi.sbatch : #!/bin/sh #SBATCH --partition=broadwl #SBATCH --nodes=4 #SBATCH --time=1 #SBATCH --exclusive module load R # The openmpi module is not loaded by default with R. module load openmpi/3.0.0 # Always use -n 1 for the Rmpi package. It spawns additional processes # dynamically mpirun -n 1 R CMD BATCH --no-save --no-restore Rmpi.R R script Rmpi.R : library(Rmpi) # initialize an Rmpi environment ns <- 4 mpi.spawn.Rslaves(nslaves=ns) # send these commands to the slaves mpi.bcast.cmd( id <- mpi.comm.rank() ) mpi.bcast.cmd( ns <- mpi.comm.size() ) mpi.bcast.cmd( host <- mpi.get.processor.name() ) # all slaves execute this command mpi.remote.exec(paste(\"I am\", id, \"of\", ns, \"running on\", host)) # close down the Rmpi environment mpi.close.Rslaves(dellog = FALSE) mpi.exit() Output (trimmed for readability): 64 slaves are spawned successfully. 0 failed. master (rank 0 , comm 1) of size 65 is running on: midway449 slave1 (rank 1 , comm 1) of size 65 is running on: midway449 slave2 (rank 2 , comm 1) of size 65 is running on: midway449 slave3 (rank 3 , comm 1) of size 65 is running on: midway449 ... ... ... slave63 (rank 63, comm 1) of size 65 is running on: midway452 slave64 (rank 64, comm 1) of size 65 is running on: midway449 $slave1 [1] \"I am 1 of 65\" $slave2 [1] \"I am 2 of 65\" ... $slave63 [1] \"I am 63 of 65\" $slave64 [1] \"I am 64 of 65\"","title":"Rmpi"},{"location":"midway23/software/environments/mathematica/","text":"Mathematica Mathematica is powerful and intuitive computation software. It is capable of geometric, audio, graphical, and raw data analysis. The embedded Wolfram Language is an incredibly powerful scripting tool for doing sybolic math analysis and granting command line style access to the plethora of algorithms within the software. Mathematica has a GUI and CLI that can be used. Getting Started To gain access to Mathematica, a Mathematica module must be loaded with the command: module load mathematica A full list of available Mathematica versions can be obtained by calling the command: module avail mathematica Using Mathematica\u2019s Graphical Interface To use Mathematica\u2019s GUI interface on Midway, we reccomend connecting to Midway via ThinLinc. Information about how to use ThinLinc can be found in the Connecting with ThinLinc section of the user guide. Note that once connected via ThinLinc, you will be accessing a Midway login node. In order to run Mathematica with its GUI interface on a compute node, obtain a terminal in the ThinLinc desktop and issue the sinteractive command. This will deliver you to a compute node. From there, you can launch Mathematica with the commands: module load mathematica mathematica and have access to the GUI interface. Using Mathematica\u2019s Textual Interface Once a Mathematica software module has been loaded, Mathematica\u2019s command line interface can be started with the command math . $ module load mathematica $ math Mathematica 8 .0 for Linux x86 ( 64 -bit ) Copyright 1988 -2011 Wolfram Research, Inc. In [ 1 ] : = a = 1 ; In [ 2 ] : = b = 2 ; In [ 3 ] : = a+b Out [ 3 ]= 3 In [ 4 ] : = Quit [] ; More information about using the command line interface to Mathematica is available here: http://reference.wolfram.com/language/tutorial/UsingATextBasedInterface.html Running Mathematica Jobs with SLURM To submit Mathematica jobs to Midway\u2019s resource scheduler, SLURM, the Mathematica commands to be executed must be containined in a single .m script. The .m script will then be passed to the math command in an sbatch file. For example: math-simple.m is a basic Mathematica script that computes the sum of A and B: A = Sum [ i, { i,1,100 }] B = Mean [{ 25 , 36 , 22 , 16 , 8 , 42 }] Answer = A + B Quit [] ; This script can be submited to SLURM with math-simple.sbatch which will send the job to a compute node: #!/bin/bash #SBATCH --job-name=math-simple #SBATCH --output=math-simple.out #SBATCH --error=math-simple.err #SBATCH --partition=sandyb #SBATCH --time=00:05:00 #SBATCH --ntasks=1 module load mathematica math -run < math-simple.m To run this example, download both files to a directory on Midway. Then, enter the following command to submit the job to the scheduler: sbatch math-simple.sbatch Output from this example can be found in the file named math-simple.out which will be created in the same directory. Using Multiple CPUs in Mathematica Mathematica can be run in parallel using the built in Parallel commands or by utilizing parallel API. Parallel Mathematica jobs are limited to one node, but can utilize all CPU cores on the node if allocated. A parallel Mathematica script must either be submitted to a node that was requested with the exclusive flag or the script must specify the number of processors allocated. As an example of the latter, the following Mathematica script would be appropriate for a SLURM request of 1 node with 8 tasks per node. Sample Parallel Job Submission SBATCH script: Click here to download the script: sample-sbatch.sbatch #!/bin/bash #SBATCH --job-name=mathematica_example #SBATCH --output=mathematica_example.out #SBATCH --error=mathematica_example.err #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 module load mathematica math -run < ./sample-parallel.m Mathematica script: Click here to download the script: sample-parallel.m ( *Limits Mathematica to requested resources* ) Unprotect [ $ProcessorCount ] ; $ProcessorCount = 8 ; ( *Prints the machine name that each kernel is running on* ) Print [ ParallelEvaluate [ $MachineName ]] ; ( *Prints all Mersenne PRime numbers less than 2000 * ) Print [ Parallelize [ Select [ Range [ 2000 ] ,PrimeQ [ 2 ^#-1 ] & ]]] ;","title":"Mathematica"},{"location":"midway23/software/environments/mathematica/#mathematica","text":"Mathematica is powerful and intuitive computation software. It is capable of geometric, audio, graphical, and raw data analysis. The embedded Wolfram Language is an incredibly powerful scripting tool for doing sybolic math analysis and granting command line style access to the plethora of algorithms within the software. Mathematica has a GUI and CLI that can be used.","title":"Mathematica"},{"location":"midway23/software/environments/mathematica/#getting-started","text":"To gain access to Mathematica, a Mathematica module must be loaded with the command: module load mathematica A full list of available Mathematica versions can be obtained by calling the command: module avail mathematica","title":"Getting Started"},{"location":"midway23/software/environments/mathematica/#using-mathematicas-graphical-interface","text":"To use Mathematica\u2019s GUI interface on Midway, we reccomend connecting to Midway via ThinLinc. Information about how to use ThinLinc can be found in the Connecting with ThinLinc section of the user guide. Note that once connected via ThinLinc, you will be accessing a Midway login node. In order to run Mathematica with its GUI interface on a compute node, obtain a terminal in the ThinLinc desktop and issue the sinteractive command. This will deliver you to a compute node. From there, you can launch Mathematica with the commands: module load mathematica mathematica and have access to the GUI interface.","title":"Using Mathematica\u2019s Graphical Interface"},{"location":"midway23/software/environments/mathematica/#using-mathematicas-textual-interface","text":"Once a Mathematica software module has been loaded, Mathematica\u2019s command line interface can be started with the command math . $ module load mathematica $ math Mathematica 8 .0 for Linux x86 ( 64 -bit ) Copyright 1988 -2011 Wolfram Research, Inc. In [ 1 ] : = a = 1 ; In [ 2 ] : = b = 2 ; In [ 3 ] : = a+b Out [ 3 ]= 3 In [ 4 ] : = Quit [] ; More information about using the command line interface to Mathematica is available here: http://reference.wolfram.com/language/tutorial/UsingATextBasedInterface.html","title":"Using Mathematica\u2019s Textual Interface"},{"location":"midway23/software/environments/mathematica/#running-mathematica-jobs-with-slurm","text":"To submit Mathematica jobs to Midway\u2019s resource scheduler, SLURM, the Mathematica commands to be executed must be containined in a single .m script. The .m script will then be passed to the math command in an sbatch file. For example: math-simple.m is a basic Mathematica script that computes the sum of A and B: A = Sum [ i, { i,1,100 }] B = Mean [{ 25 , 36 , 22 , 16 , 8 , 42 }] Answer = A + B Quit [] ; This script can be submited to SLURM with math-simple.sbatch which will send the job to a compute node: #!/bin/bash #SBATCH --job-name=math-simple #SBATCH --output=math-simple.out #SBATCH --error=math-simple.err #SBATCH --partition=sandyb #SBATCH --time=00:05:00 #SBATCH --ntasks=1 module load mathematica math -run < math-simple.m To run this example, download both files to a directory on Midway. Then, enter the following command to submit the job to the scheduler: sbatch math-simple.sbatch Output from this example can be found in the file named math-simple.out which will be created in the same directory.","title":"Running Mathematica Jobs with SLURM"},{"location":"midway23/software/environments/mathematica/#using-multiple-cpus-in-mathematica","text":"Mathematica can be run in parallel using the built in Parallel commands or by utilizing parallel API. Parallel Mathematica jobs are limited to one node, but can utilize all CPU cores on the node if allocated. A parallel Mathematica script must either be submitted to a node that was requested with the exclusive flag or the script must specify the number of processors allocated. As an example of the latter, the following Mathematica script would be appropriate for a SLURM request of 1 node with 8 tasks per node.","title":"Using Multiple CPUs in Mathematica"},{"location":"midway23/software/environments/mathematica/#sample-parallel-job-submission","text":"SBATCH script: Click here to download the script: sample-sbatch.sbatch #!/bin/bash #SBATCH --job-name=mathematica_example #SBATCH --output=mathematica_example.out #SBATCH --error=mathematica_example.err #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 module load mathematica math -run < ./sample-parallel.m Mathematica script: Click here to download the script: sample-parallel.m ( *Limits Mathematica to requested resources* ) Unprotect [ $ProcessorCount ] ; $ProcessorCount = 8 ; ( *Prints the machine name that each kernel is running on* ) Print [ ParallelEvaluate [ $MachineName ]] ; ( *Prints all Mersenne PRime numbers less than 2000 * ) Print [ Parallelize [ Select [ Range [ 2000 ] ,PrimeQ [ 2 ^#-1 ] & ]]] ;","title":"Sample Parallel Job Submission"},{"location":"midway23/software/environments/matlab/","text":"MATLAB RCC provides the Matlab programming environment on all Midway compute resources. Most Matlab toolboxes are also available (see: https://itservices.uchicago.edu/page/matlab-tah-toolboxes ). When running compute- or memory-intensive Matlab jobs on Midway, it is important to run on compute nodes, and not on the login nodes. NOTE : Compute- and memory-intensive jobs running on the login nodes are subject to termination without warning by RCC system administrators as this impacts the performance of the login nodes and ability for other users to work. Getting Started To gain access to Matlab, a Matlab module must be loaded with the command: module load matlab A full list of the available Matlab versions can be obtained by issuing the command: module avail matlab Using Matlab\u2019s Textual Interface On Midway, Matlab can be launched at the terminal with the commands: module load matlab matlab This will launch Matlab\u2019s textual interface. We recommend running Matlab on a compute node as opposed to a login node. To obtain a shell on a compute node, use the sinteractive command (see Interactive Jobs for more information). Using Matlab\u2019s GUI Interface To use Matlab\u2019s GUI interface on Midway, we reccomend connecting to Midway via ThinLinc. Information about how to use ThinLinc can be found in the Connecting with ThinLinc section of the user guide. Note that once connected via ThinLinc, you will be accessing a Midway login node. In order to run Matlab with its GUI interface on a compute node, obtain a terminal in the ThinLinc desktop and issue the sinteractive command. This will deliver you to a compute node. From there, you can launch Matlab with the command: module load matlab matlab and have access to the GUI interface. Running Matlab Jobs with SLURM To submit Matlab jobs to Midway\u2019s resource scheduler, SLURM, the Matlab commands to be executed must be containined in a single .m script. matlab_simple.m is a basic Matlab script that computes and prints a 10x10 magic square matlab_simple.sbatch is a submission script that submits Matlab program to the default queue To run this example, download both files to a directory on Midway. Then, enter the following command to submit the program matlab_simple.m to the scheduler: sbatch matlab_simple.sbatch Output from this example can be found in the file named matlab.out which will be created in the same directory. Matlab Parallel To run MATLAB effectively using parallel computing techniques requires a few basic concepts which can be optimized and expanded upon. The MATLAB Parallel Computing Toolbox User\u2019s Guide is the official documentation and should be referred to for further details, examples and explanations. Here, we provide some Midway-specific considerations that RCC users should be aware of. RCC reccomends MATLAB 2014b for parallel matlab computing as it relaxes the restriction on number of workers available through the PCT. NOTE : At this time, RCC does not support the Matlab Distributed Compute Server (MDCS). As such, parallel Matlab jobs are limited to a single node with the \u201clocal\u201d pool through use of the Parallel Compute Toolbox (PCT). Matlab versions prior to 2014b have a limit of 12 workers. Matlab 2014b relaxes this restriction and a number of workers up to the number of CPUs in a compute node can be created. See Types of Compute Nodes for more information about avaiable compute node configurations. Basic PCT Operation The most basic level of parallelization in Matlab is achieved through use of a parfor loop in place of a for loop. The iterations of a parfor loop are distributed to the workers in the active matlabpool and computed concurrently. For this reason, care must be taken to ensure that each iteration of the parfor loop is independent of every other. The overall procedure for leveraging parfor in your Matlab script is as follows: Create a local matlabpool Call parfor in place of for in your Matlab scripts and functions A simple Matlab script that uses parfor can be downloaded here: matlab_parfor.m and is shown below. % start the matlabpool with maximum available workers % control how many workers by setting ntasks in your sbatch script pc = parcluster ( 'local' ) parpool ( pc, str2num ( getenv ( 'SLURM_CPUS_ON_NODE' ))) % run a parfor loop, distributing the iterations to the SLURM_CPUS_ON_NODE workers parfor i = 1 :100 ones ( 10 ,10 ) end Submitting a PCT Matlab Job to SLURM Compute intensive jobs that will consume non-trivial amounts of CPU and/or memory resources should not be run on Midway\u2019s login nodes. Instead, the job should be submitted to the scheduler and run on a compute node. A sample submission script for the above example Matlab sample is provided here: matlab_parfor.sbatch and is shown below. #!/bin/bash #SBATCH --job-name=whatever #SBATCH --output=matlab_parfor.out #SBATCH --error=matlab_parfor.err #SBATCH --partition=sandyb #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --ntasks=16 module load matlab/2014b matlab -nodisplay < matlab_parfor.m Running Multiple PCT Matlab Jobs Specific care must be taken when running multiple PCT jobs on Midway. When you submit multiple jobs that are all using PCT for parallelization, the multiple matlabpools that get created have the ability to interfere with one another which can lead to errors and early termination of your scripts. The Matlab PCT requires a temporary \u201cJob Storage Location\u201d where is stores information about the Matlab pool that is in use. This is simply a directory on the filesystem that Matlab writes various files to in order to coordinate the parallelization of the matlabpool. By default, this information is stored in /home/YourUsername/.matlab/ (the default \u201cJob Storage Location\u201d). When submitting multiple jobs to SLURM that will all use the PCT, all of the jobs will attempt to use this default location for storing job information thereby creating a race condition where one job modifies the files that were put in place by another. Clearly, this situation must be avoided. The solution is to have each of your jobs that will use the PCT set a unique location for storing job information. To do this, a temporary directory must be created before launching matlab in your submission script and then the matlabpool must be created to explicitly use this unique temporary directory. An example sbatch script matlab_multi.sbatch to do this is shown below: #!/bin/bash #SBATCH --job-name=whatever #SBATCH --output=matlab_parfor.out #SBATCH --error=matlab_parfor.err #SBATCH --partition=sandyb #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --ntasks=16 module load matlab/2014b # Create a temporary directory on scratch mkdir -p $SCRATCH / $SLURM_JOB_ID # Kick off matlab matlab -nodisplay < multi_parfor.m # Cleanup local work directory rm -rf $SCRATCH / $SLURM_JOB_ID And the corresponding Matlab script multi_parfor.m shown here: % create a local cluster object pc = parcluster ( 'local' ) % explicitly set the JobStorageLocation to the temp directory that was created in your sbatch script pc.JobStorageLocation = strcat ( getenv ( 'SCRATCH' ) , '/' , getenv ( 'SLURM_JOB_ID' )) % start the matlabpool with maximum available workers % control how many workers by setting ntasks in your sbatch script parpool ( pc, str2num ( getenv ( 'SLURM_CPUS_ON_NODE' ))) % run a parallel for loop parfor i = 1 :100 ones ( 10 ,10 ) end","title":"MATLAB"},{"location":"midway23/software/environments/matlab/#matlab","text":"RCC provides the Matlab programming environment on all Midway compute resources. Most Matlab toolboxes are also available (see: https://itservices.uchicago.edu/page/matlab-tah-toolboxes ). When running compute- or memory-intensive Matlab jobs on Midway, it is important to run on compute nodes, and not on the login nodes. NOTE : Compute- and memory-intensive jobs running on the login nodes are subject to termination without warning by RCC system administrators as this impacts the performance of the login nodes and ability for other users to work.","title":"MATLAB"},{"location":"midway23/software/environments/matlab/#getting-started","text":"To gain access to Matlab, a Matlab module must be loaded with the command: module load matlab A full list of the available Matlab versions can be obtained by issuing the command: module avail matlab","title":"Getting Started"},{"location":"midway23/software/environments/matlab/#using-matlabs-textual-interface","text":"On Midway, Matlab can be launched at the terminal with the commands: module load matlab matlab This will launch Matlab\u2019s textual interface. We recommend running Matlab on a compute node as opposed to a login node. To obtain a shell on a compute node, use the sinteractive command (see Interactive Jobs for more information).","title":"Using Matlab\u2019s Textual Interface"},{"location":"midway23/software/environments/matlab/#using-matlabs-gui-interface","text":"To use Matlab\u2019s GUI interface on Midway, we reccomend connecting to Midway via ThinLinc. Information about how to use ThinLinc can be found in the Connecting with ThinLinc section of the user guide. Note that once connected via ThinLinc, you will be accessing a Midway login node. In order to run Matlab with its GUI interface on a compute node, obtain a terminal in the ThinLinc desktop and issue the sinteractive command. This will deliver you to a compute node. From there, you can launch Matlab with the command: module load matlab matlab and have access to the GUI interface.","title":"Using Matlab\u2019s GUI Interface"},{"location":"midway23/software/environments/matlab/#running-matlab-jobs-with-slurm","text":"To submit Matlab jobs to Midway\u2019s resource scheduler, SLURM, the Matlab commands to be executed must be containined in a single .m script. matlab_simple.m is a basic Matlab script that computes and prints a 10x10 magic square matlab_simple.sbatch is a submission script that submits Matlab program to the default queue To run this example, download both files to a directory on Midway. Then, enter the following command to submit the program matlab_simple.m to the scheduler: sbatch matlab_simple.sbatch Output from this example can be found in the file named matlab.out which will be created in the same directory.","title":"Running Matlab Jobs with SLURM"},{"location":"midway23/software/environments/matlab/#matlab-parallel","text":"To run MATLAB effectively using parallel computing techniques requires a few basic concepts which can be optimized and expanded upon. The MATLAB Parallel Computing Toolbox User\u2019s Guide is the official documentation and should be referred to for further details, examples and explanations. Here, we provide some Midway-specific considerations that RCC users should be aware of. RCC reccomends MATLAB 2014b for parallel matlab computing as it relaxes the restriction on number of workers available through the PCT. NOTE : At this time, RCC does not support the Matlab Distributed Compute Server (MDCS). As such, parallel Matlab jobs are limited to a single node with the \u201clocal\u201d pool through use of the Parallel Compute Toolbox (PCT). Matlab versions prior to 2014b have a limit of 12 workers. Matlab 2014b relaxes this restriction and a number of workers up to the number of CPUs in a compute node can be created. See Types of Compute Nodes for more information about avaiable compute node configurations.","title":"Matlab Parallel"},{"location":"midway23/software/environments/matlab/#basic-pct-operation","text":"The most basic level of parallelization in Matlab is achieved through use of a parfor loop in place of a for loop. The iterations of a parfor loop are distributed to the workers in the active matlabpool and computed concurrently. For this reason, care must be taken to ensure that each iteration of the parfor loop is independent of every other. The overall procedure for leveraging parfor in your Matlab script is as follows: Create a local matlabpool Call parfor in place of for in your Matlab scripts and functions A simple Matlab script that uses parfor can be downloaded here: matlab_parfor.m and is shown below. % start the matlabpool with maximum available workers % control how many workers by setting ntasks in your sbatch script pc = parcluster ( 'local' ) parpool ( pc, str2num ( getenv ( 'SLURM_CPUS_ON_NODE' ))) % run a parfor loop, distributing the iterations to the SLURM_CPUS_ON_NODE workers parfor i = 1 :100 ones ( 10 ,10 ) end","title":"Basic PCT Operation"},{"location":"midway23/software/environments/matlab/#submitting-a-pct-matlab-job-to-slurm","text":"Compute intensive jobs that will consume non-trivial amounts of CPU and/or memory resources should not be run on Midway\u2019s login nodes. Instead, the job should be submitted to the scheduler and run on a compute node. A sample submission script for the above example Matlab sample is provided here: matlab_parfor.sbatch and is shown below. #!/bin/bash #SBATCH --job-name=whatever #SBATCH --output=matlab_parfor.out #SBATCH --error=matlab_parfor.err #SBATCH --partition=sandyb #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --ntasks=16 module load matlab/2014b matlab -nodisplay < matlab_parfor.m","title":"Submitting a PCT Matlab Job to SLURM"},{"location":"midway23/software/environments/matlab/#running-multiple-pct-matlab-jobs","text":"Specific care must be taken when running multiple PCT jobs on Midway. When you submit multiple jobs that are all using PCT for parallelization, the multiple matlabpools that get created have the ability to interfere with one another which can lead to errors and early termination of your scripts. The Matlab PCT requires a temporary \u201cJob Storage Location\u201d where is stores information about the Matlab pool that is in use. This is simply a directory on the filesystem that Matlab writes various files to in order to coordinate the parallelization of the matlabpool. By default, this information is stored in /home/YourUsername/.matlab/ (the default \u201cJob Storage Location\u201d). When submitting multiple jobs to SLURM that will all use the PCT, all of the jobs will attempt to use this default location for storing job information thereby creating a race condition where one job modifies the files that were put in place by another. Clearly, this situation must be avoided. The solution is to have each of your jobs that will use the PCT set a unique location for storing job information. To do this, a temporary directory must be created before launching matlab in your submission script and then the matlabpool must be created to explicitly use this unique temporary directory. An example sbatch script matlab_multi.sbatch to do this is shown below: #!/bin/bash #SBATCH --job-name=whatever #SBATCH --output=matlab_parfor.out #SBATCH --error=matlab_parfor.err #SBATCH --partition=sandyb #SBATCH --time=00:10:00 #SBATCH --nodes=1 #SBATCH --ntasks=16 module load matlab/2014b # Create a temporary directory on scratch mkdir -p $SCRATCH / $SLURM_JOB_ID # Kick off matlab matlab -nodisplay < multi_parfor.m # Cleanup local work directory rm -rf $SCRATCH / $SLURM_JOB_ID And the corresponding Matlab script multi_parfor.m shown here: % create a local cluster object pc = parcluster ( 'local' ) % explicitly set the JobStorageLocation to the temp directory that was created in your sbatch script pc.JobStorageLocation = strcat ( getenv ( 'SCRATCH' ) , '/' , getenv ( 'SLURM_JOB_ID' )) % start the matlabpool with maximum available workers % control how many workers by setting ntasks in your sbatch script parpool ( pc, str2num ( getenv ( 'SLURM_CPUS_ON_NODE' ))) % run a parallel for loop parfor i = 1 :100 ones ( 10 ,10 ) end","title":"Running Multiple PCT Matlab Jobs"},{"location":"midway23/software/environments/perl/","text":"Perl Perl is available as a software module. No additional perl modules have been installed. Installing local modules using local::lib with cpanm is the preferred method. Here is an example to install and test Math::CDF: module load perl eval $(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib) cpanm Math::CDF perl -e \"require Math::CDF\" To have the appropriate environment available during login, it is recommended to put these lines in the $HOME/.bashrc: module load perl [ $SHLVL -eq 1 ] && eval \"$(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib)\" Additional details on local::lib can be in the local::lib documentation .","title":"Perl"},{"location":"midway23/software/environments/perl/#perl","text":"Perl is available as a software module. No additional perl modules have been installed. Installing local modules using local::lib with cpanm is the preferred method. Here is an example to install and test Math::CDF: module load perl eval $(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib) cpanm Math::CDF perl -e \"require Math::CDF\" To have the appropriate environment available during login, it is recommended to put these lines in the $HOME/.bashrc: module load perl [ $SHLVL -eq 1 ] && eval \"$(perl -I$HOME/perl5/lib/perl5 -Mlocal::lib)\" Additional details on local::lib can be in the local::lib documentation .","title":"Perl"},{"location":"midway23/software/environments/python/","text":"Python Getting Started Different versions of Python on Midway2 are offered as modules. To check the full list of Python modules use the module avail python command. The default version of Python, if you do module load python , will be the latest Anaconda distribution of Python. If you load an Anaconda distribution of Python, you will have multiple environments available. You can list them with conda env list . To activate an environment, run source activate <ENV NAME> , where is the name of the environment for a public environment, or the full path to the environment, if you are using a personal one. You can deactivate an environment with conda deactivate . WARNING: Never run conda init ! Use source activate instead of conda activate . conda init has been known to break ThinLinc. Managing Packages In the Anaconda distributions of Python, you should generally be using a personal environment to manage packages. Once you activate your environment, you can install packages with conda install or pip install . As per the advice of the Anaconda software authors, any pip install packages should be installed after conda install packages. Managing Environments With each Anaconda distribution, we have a small selection of widely used environments. Many, such as Tensorflow or DeepLabCut should be loaded through their modules, which automate the loading of other relevant libraries that are available as modules. If you need packages not available in the global environment, you can make a personal environment for them. If you want to copy an existing environment to modify it, you can do that with conda create --prefix=/path/to/new/environment --clone <EXISTING ENVIRONMENT> . If you want to make a clean environment, you can do that with conda create --prefix=/path/to/new/environment python=<PYTHON VERSION NUMBER> . Once your environment is set up how you want, especially if it is in your scratch space, you may want to create a backup of the environment into a YAML file. You do that after activating the environment with conda env export > environment.yml . That YAML file can then be used to recreate the environment with conda env create --prefix=/path/to/new/environment -f environment.yml . Using Python On Midway, python can be launched, after loading a desired module, at the terminal with the command: python To leave the launched interactive shell, use: exit() If you already have a python script, use this command to run it: python your_script.py Python Interactive Plotting For interactive plotting, it is necessary to set the matplotlib backend to a graphical backend. Here is an example: #!/usr/bin/env python import matplotlib matplotlib.use('Qt4Agg') import matplotlib.pyplot as plt plt.plot([1,2,3,4]) plt.ylabel('some numbers') plt.show() If you are saving files and viewing them with the display command, you may experience rapid flickering. There seems to be an issue with image transparency, use a command like this to disable the transparency: display -alpha off <image> Running Jupyter Notebooks The Jupyter notebook is a useful tool for python users because it provides interactive computing. You can launch Jupyter on Midway, open it in the browser on your local machine and have all the computation work done on Midway. If you want to perform heavy compute, you will need to start an interactive session (please see Interactive Jobs on how to get an interactive session) before launching Jupyter notebook otherwise you may use one of the login nodes. NOTE : Compute nodes are only visible on internal UChicago network. If you want to launch Jupyter on a compute node (using an interactive session), you will need to either be on campus or use VPN. However, you may launch it on a login node anytime. The steps to launch Jupyter are as follows: Load the desired Python module Determine your ip address. Whether you are on a login node or a compute node, you can use this command to get your ip address: /sbin/ip route get 8.8.8.8 | awk '{print $NF;exit}' Launch Jupyter with: jupyter-notebook \u2013no-browser \u2013ip= or in Python 3.x with: jupyter-notebook --no-browser --ip=<ip address> which will give you a URL with a token. For example: http://10.50.221.192:8888/?token=9c9b7fb3885a5b6896c959c8a945773b8860c6e2e0bad629 By default, the above command listens on port 8888. If the port is already taken by another user, it will complain. In that case, please try the next available port with the option: --port=<port number> Open the returned URL in the browser on your local machine. Note that if you do not specify --no-browser --ip= , the web browser will be launched on the node and the URL returned cannot be used on your local machine. To kill Jupyter, press Ctrl+c and then confirm with y that you want to stop it","title":"Python"},{"location":"midway23/software/environments/python/#python","text":"","title":"Python"},{"location":"midway23/software/environments/python/#getting-started","text":"Different versions of Python on Midway2 are offered as modules. To check the full list of Python modules use the module avail python command. The default version of Python, if you do module load python , will be the latest Anaconda distribution of Python. If you load an Anaconda distribution of Python, you will have multiple environments available. You can list them with conda env list . To activate an environment, run source activate <ENV NAME> , where is the name of the environment for a public environment, or the full path to the environment, if you are using a personal one. You can deactivate an environment with conda deactivate . WARNING: Never run conda init ! Use source activate instead of conda activate . conda init has been known to break ThinLinc.","title":"Getting Started"},{"location":"midway23/software/environments/python/#managing-packages","text":"In the Anaconda distributions of Python, you should generally be using a personal environment to manage packages. Once you activate your environment, you can install packages with conda install or pip install . As per the advice of the Anaconda software authors, any pip install packages should be installed after conda install packages.","title":"Managing Packages"},{"location":"midway23/software/environments/python/#managing-environments","text":"With each Anaconda distribution, we have a small selection of widely used environments. Many, such as Tensorflow or DeepLabCut should be loaded through their modules, which automate the loading of other relevant libraries that are available as modules. If you need packages not available in the global environment, you can make a personal environment for them. If you want to copy an existing environment to modify it, you can do that with conda create --prefix=/path/to/new/environment --clone <EXISTING ENVIRONMENT> . If you want to make a clean environment, you can do that with conda create --prefix=/path/to/new/environment python=<PYTHON VERSION NUMBER> . Once your environment is set up how you want, especially if it is in your scratch space, you may want to create a backup of the environment into a YAML file. You do that after activating the environment with conda env export > environment.yml . That YAML file can then be used to recreate the environment with conda env create --prefix=/path/to/new/environment -f environment.yml .","title":"Managing Environments"},{"location":"midway23/software/environments/python/#using-python","text":"On Midway, python can be launched, after loading a desired module, at the terminal with the command: python To leave the launched interactive shell, use: exit() If you already have a python script, use this command to run it: python your_script.py","title":"Using Python"},{"location":"midway23/software/environments/python/#python-interactive-plotting","text":"For interactive plotting, it is necessary to set the matplotlib backend to a graphical backend. Here is an example: #!/usr/bin/env python import matplotlib matplotlib.use('Qt4Agg') import matplotlib.pyplot as plt plt.plot([1,2,3,4]) plt.ylabel('some numbers') plt.show() If you are saving files and viewing them with the display command, you may experience rapid flickering. There seems to be an issue with image transparency, use a command like this to disable the transparency: display -alpha off <image>","title":"Python Interactive Plotting"},{"location":"midway23/software/environments/python/#running-jupyter-notebooks","text":"The Jupyter notebook is a useful tool for python users because it provides interactive computing. You can launch Jupyter on Midway, open it in the browser on your local machine and have all the computation work done on Midway. If you want to perform heavy compute, you will need to start an interactive session (please see Interactive Jobs on how to get an interactive session) before launching Jupyter notebook otherwise you may use one of the login nodes. NOTE : Compute nodes are only visible on internal UChicago network. If you want to launch Jupyter on a compute node (using an interactive session), you will need to either be on campus or use VPN. However, you may launch it on a login node anytime. The steps to launch Jupyter are as follows: Load the desired Python module Determine your ip address. Whether you are on a login node or a compute node, you can use this command to get your ip address: /sbin/ip route get 8.8.8.8 | awk '{print $NF;exit}' Launch Jupyter with: jupyter-notebook \u2013no-browser \u2013ip= or in Python 3.x with: jupyter-notebook --no-browser --ip=<ip address> which will give you a URL with a token. For example: http://10.50.221.192:8888/?token=9c9b7fb3885a5b6896c959c8a945773b8860c6e2e0bad629 By default, the above command listens on port 8888. If the port is already taken by another user, it will complain. In that case, please try the next available port with the option: --port=<port number> Open the returned URL in the browser on your local machine. Note that if you do not specify --no-browser --ip= , the web browser will be launched on the node and the URL returned cannot be used on your local machine. To kill Jupyter, press Ctrl+c and then confirm with y that you want to stop it","title":"Running Jupyter Notebooks"},{"location":"midway23/software/environments/spark/","text":"Spark Apache Spark is a fast and general engine for large-scale data processing. It has a Scala, Java, and Python API and can be run either on either a single node or multi-node configuration. For both cases, it is recommended to have exclusive access of the node in Slurm. Single Node Examples Here is the SparkPi and pi.py examples from the Spark distribution running on a single node: sbatch script spark-single-node.sbatch #!/bin/bash #SBATCH --job-name=spark # Exclusive mode is recommended for all spark jobs #SBATCH --exclusive #SBATCH --nodes=1 #SBATCH --time=10 module load spark # This syntax tells spark to use all cpu cores on the node. export MASTER = \"local[*]\" # This is a scala example run-example SparkPi # This is a python example. # For production jobs, you'll probably want to have a python module loaded. # This will use the system python if you don't have a python module loaded. spark-submit --master $MASTER $SPARK_HOME /examples/src/main/python/pi.py Multi-node Examples For multi-node Spark jobs, a helper script was written to launch the master and work tasks in the slurm allocation. Here are the same examples as above, but with Spark running on multiple nodes: sbatch script spark-multi-node.sbatch #!/bin/bash #SBATCH --job-name=spark-multi-node # Exclusive mode is recommended for all spark jobs #SBATCH --exclusive #SBATCH --nodes=4 #SBATCH --time=10 module load spark # This command starts the spark workers on the allocated nodes start-spark-slurm.sh # This syntax tells the spark workers where the master is export MASTER = spark:// $HOSTNAME :7077 # This is a scala example run-example SparkPi # This is a python example. # For production jobs, you'll probably want to have a python module loaded. # This will use the system python if you don't have a python module loaded. spark-submit --master $MASTER $SPARK_HOME /examples/src/main/python/pi.py","title":"Spark"},{"location":"midway23/software/environments/spark/#spark","text":"Apache Spark is a fast and general engine for large-scale data processing. It has a Scala, Java, and Python API and can be run either on either a single node or multi-node configuration. For both cases, it is recommended to have exclusive access of the node in Slurm.","title":"Spark"},{"location":"midway23/software/environments/spark/#single-node-examples","text":"Here is the SparkPi and pi.py examples from the Spark distribution running on a single node: sbatch script spark-single-node.sbatch #!/bin/bash #SBATCH --job-name=spark # Exclusive mode is recommended for all spark jobs #SBATCH --exclusive #SBATCH --nodes=1 #SBATCH --time=10 module load spark # This syntax tells spark to use all cpu cores on the node. export MASTER = \"local[*]\" # This is a scala example run-example SparkPi # This is a python example. # For production jobs, you'll probably want to have a python module loaded. # This will use the system python if you don't have a python module loaded. spark-submit --master $MASTER $SPARK_HOME /examples/src/main/python/pi.py","title":"Single Node Examples"},{"location":"midway23/software/environments/spark/#multi-node-examples","text":"For multi-node Spark jobs, a helper script was written to launch the master and work tasks in the slurm allocation. Here are the same examples as above, but with Spark running on multiple nodes: sbatch script spark-multi-node.sbatch #!/bin/bash #SBATCH --job-name=spark-multi-node # Exclusive mode is recommended for all spark jobs #SBATCH --exclusive #SBATCH --nodes=4 #SBATCH --time=10 module load spark # This command starts the spark workers on the allocated nodes start-spark-slurm.sh # This syntax tells the spark workers where the master is export MASTER = spark:// $HOSTNAME :7077 # This is a scala example run-example SparkPi # This is a python example. # For production jobs, you'll probably want to have a python module loaded. # This will use the system python if you don't have a python module loaded. spark-submit --master $MASTER $SPARK_HOME /examples/src/main/python/pi.py","title":"Multi-node Examples"},{"location":"midway23/software/environments/stata/","text":"Stata Stata is a powerful statistical software package that is widely used in scientific computing. RCC users are licensed to use Stata on all RCC resources. Stata can be used interactively or as a submitted script. Please note that if you would like to run it interactively, you must still run it on a compute node, in order to keep the login nodes free for other users. Stata can be run in parallel on up to 16 nodes. NOTE : Stata examples in this document are adapted from a Princeton tutorial . You may find it useful if you are new to Stata or want a refresher. Getting Started If you need to use the Stata GUI, connect to Midway with Connecting with ThinLinc . Obtain an interactive session on a compute node. This is necessary so that your computation doesn\u2019t interrupt other users on the login node. Now, load Stata: sinteractive module load stata xstata This will open up a Stata window. The middle pane has a text box to enter commands at the bottom, and a box for command results on top. On the left there\u2019s a box called \u201cReview\u201d that shows your command history. The right-hand box contains information about variables in the currently-loaded data set. One way Stata can be used is as a fancy desktop calculator. Type the following code into the command box: display 2+2 Stata can do much more if data is loaded into it. The following code loads census data that ships with Stata, prints a description of the data, then creates a graph of life expectancy over GNP: sysuse lifeexp describe graph twoway scatter lexp gnppc Running Stata from the command line This is very similar to running graphically; the command-line interface is equivalent to the \u201cResults\u201d pane in the graphical interface. Again, please use a compute node if you are running computationally-intensive calculations: sinteractive module load stata stata Running Stata Jobs with SLURM You can also submit Stata jobs to SLURM, the scheduler. A Stata script is called a \u201cdo-file,\u201d which contains a list of Stata commands that the interpreter will execute. You can write a do-file in any text editor, or in the Stata GUI\u2019s do-file editor: click \u201cDo-File Editor\u201d\u201d in the \u201cWindow\u201d menu. If your do-file is named \u201cexample.do,\u201d you can run it with either of the following commands: stata < example.do stata -b do example.do Here is a very simple do-file, which computes a regression on the sample data set from above: version 13 // current version of Stata, this is optional but recommended. sysuse lifeexp gen loggnppc = log(gnppc) regress lexp loggnppc Here is a submission script that submits the Stata program to the default queue on Midway: #!/bin/bash #SBATCH --job-name=stataEx #SBATCH --output=stata_example.out #SBATCH --error=stata_example.err #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 module load stata stata -b stata_example.do stata_example.do is our example do-file, and stata_example.sbatch is the submission script. To run this example, download both files to a directory on Midway. Enter the following command to submit the program to the scheduler: sbatch stata_example.sbatch Output from this example can be found in the file named stata_example.log , which will be created automatically in your current directory. Running Parallel Stata Jobs The parallel version of Stata, Stata/MP, can speed up computations and make effective use of RCC\u2019s resources. When running Stata/MP, you are limited to 16 cores and 5000 variables. Run an interactive Stata/MP session: sinteractive module load stata stata-mp # or, for the graphical interface: xstata-mp Here is a sample do-file that would benefit from parallelization. It runs bootstrap estimation on another data set that ships with Stata. version 13 sysuse auto expand 10000 bootstrap: logistic foreign price-gear_ratio Here is a submission script that will run the above do-file with Stata/MP: #!/bin/bash #SBATCH --job-name=stataMP #SBATCH --output=stata_parallel.out #SBATCH --error=stata_parallel.err #SBATCH --nodes=1 #SBATCH --tasks-per-node=16 module load stata stata-mp -b stata_parallel.do Download stata_parallel.do and stata_parallel.sbatch to Midway, then run the program with: sbatch stata_parallel.sbatch","title":"Stata"},{"location":"midway23/software/environments/stata/#stata","text":"Stata is a powerful statistical software package that is widely used in scientific computing. RCC users are licensed to use Stata on all RCC resources. Stata can be used interactively or as a submitted script. Please note that if you would like to run it interactively, you must still run it on a compute node, in order to keep the login nodes free for other users. Stata can be run in parallel on up to 16 nodes. NOTE : Stata examples in this document are adapted from a Princeton tutorial . You may find it useful if you are new to Stata or want a refresher.","title":"Stata"},{"location":"midway23/software/environments/stata/#getting-started","text":"If you need to use the Stata GUI, connect to Midway with Connecting with ThinLinc . Obtain an interactive session on a compute node. This is necessary so that your computation doesn\u2019t interrupt other users on the login node. Now, load Stata: sinteractive module load stata xstata This will open up a Stata window. The middle pane has a text box to enter commands at the bottom, and a box for command results on top. On the left there\u2019s a box called \u201cReview\u201d that shows your command history. The right-hand box contains information about variables in the currently-loaded data set. One way Stata can be used is as a fancy desktop calculator. Type the following code into the command box: display 2+2 Stata can do much more if data is loaded into it. The following code loads census data that ships with Stata, prints a description of the data, then creates a graph of life expectancy over GNP: sysuse lifeexp describe graph twoway scatter lexp gnppc","title":"Getting Started"},{"location":"midway23/software/environments/stata/#running-stata-from-the-command-line","text":"This is very similar to running graphically; the command-line interface is equivalent to the \u201cResults\u201d pane in the graphical interface. Again, please use a compute node if you are running computationally-intensive calculations: sinteractive module load stata stata","title":"Running Stata from the command line"},{"location":"midway23/software/environments/stata/#running-stata-jobs-with-slurm","text":"You can also submit Stata jobs to SLURM, the scheduler. A Stata script is called a \u201cdo-file,\u201d which contains a list of Stata commands that the interpreter will execute. You can write a do-file in any text editor, or in the Stata GUI\u2019s do-file editor: click \u201cDo-File Editor\u201d\u201d in the \u201cWindow\u201d menu. If your do-file is named \u201cexample.do,\u201d you can run it with either of the following commands: stata < example.do stata -b do example.do Here is a very simple do-file, which computes a regression on the sample data set from above: version 13 // current version of Stata, this is optional but recommended. sysuse lifeexp gen loggnppc = log(gnppc) regress lexp loggnppc Here is a submission script that submits the Stata program to the default queue on Midway: #!/bin/bash #SBATCH --job-name=stataEx #SBATCH --output=stata_example.out #SBATCH --error=stata_example.err #SBATCH --nodes=1 #SBATCH --tasks-per-node=1 module load stata stata -b stata_example.do stata_example.do is our example do-file, and stata_example.sbatch is the submission script. To run this example, download both files to a directory on Midway. Enter the following command to submit the program to the scheduler: sbatch stata_example.sbatch Output from this example can be found in the file named stata_example.log , which will be created automatically in your current directory.","title":"Running Stata Jobs with SLURM"},{"location":"midway23/software/environments/stata/#running-parallel-stata-jobs","text":"The parallel version of Stata, Stata/MP, can speed up computations and make effective use of RCC\u2019s resources. When running Stata/MP, you are limited to 16 cores and 5000 variables. Run an interactive Stata/MP session: sinteractive module load stata stata-mp # or, for the graphical interface: xstata-mp Here is a sample do-file that would benefit from parallelization. It runs bootstrap estimation on another data set that ships with Stata. version 13 sysuse auto expand 10000 bootstrap: logistic foreign price-gear_ratio Here is a submission script that will run the above do-file with Stata/MP: #!/bin/bash #SBATCH --job-name=stataMP #SBATCH --output=stata_parallel.out #SBATCH --error=stata_parallel.err #SBATCH --nodes=1 #SBATCH --tasks-per-node=16 module load stata stata-mp -b stata_parallel.do Download stata_parallel.do and stata_parallel.sbatch to Midway, then run the program with: sbatch stata_parallel.sbatch","title":"Running Parallel Stata Jobs"},{"location":"midway23/software/libraries/fftw/","text":"FFTW FFTW, \u201cthe Fastest Fourier Transform in the West,\u201d is a popular open-source library for computing discrete Fourier transforms. It supports both real and complex transformations, in both single and double precision. RCC has configured and installed a large variety of fftw modules including different versions, compiler choices, and parallelization strategy. Please contact RCC if a different version or configuration is necessary for your work. FFTW2 FFTW 2.1.5 is an older, now unsupported version, but still commonly used by codes as the API has changed in later versions. The versions compiled by the RCC support shared memory parallelism through OpenMP and distributed parallelism through MPI. See FFTW2 Documentation for complete documentation of this version. The non-MPI modules include both single and double precision versions, as well as openMP support. These have been built for all three RCC supported compilers (gcc, intel, and pgi). The library defaults to double-precision, however single-precision can be used by adding the prefix s to all filenames and library calls (see the following ) for more details). The MPI modules are compiled for each MPI library and compiler (leading to a large number of available combinations). These should all be interchangeable, simply ensure you match the correct module to the MPI library and compiler you are using (the module system will complain otherwise). Each module adds the fftw2 library to your includes path, but for codes that prefer to self-configure, the environment variable FFTW2_DIR points to the currently loaded version. The RCC help system includes a sample code to perform 1-dimensional complex transformations in serial or in parallel. To compile and run each sample code, use the following commands: fftw2_serial.c : module load fftw2/2.1.5 gcc fftw2_serial.c -lm -lfftw ./a.out 512 fftw2_openmp.c : module load fftw2/2.1.5 gcc -fopenmp fftw2_openmp.c -lm -lfftw -lfftw_threads OMP_NUM_THREADS = 8 ./a.out 512 8 fftw2_mpi.c : module load openmpi/1.6 module load fftw2/2.1.5+openmpi-1.6 mpicc fftw2_mpi.c -lfftw_mpi -lfftw mpirun -np 4 ./a.out 512 FFTW3 The API for FFTW has significantly changed in the 3.X branch of FFTW. MPI support has only recently been re-included as a stable feature (3.3.X), and it is this version the RCC supports. As FFTW2 is no longer supported, we recommend users upgrade to the newest version if their code allows. Documentation for this version can be viewed at: FFTW3 Documentation Single precision support is included by post-fixing \u2018f\u2019 to commands and filenames, see this document Sample codes for serial, shared memory (openMP), and distributed (MPI) have been included in the RCC help system. Use the following to compile and run each sample: fftw3_serial.c : module load fftw3/3.3 gcc fftw3_serial.c -lm -lfftw ./a.out 512 fftw3_openmp.c : module load fftw3/3.3 gcc -fopenmp fftw3_openmp.c -lfftw3_omp -lfftw3 -lm OMP_NUM_THREADS = 8 ./a.out 512 fftw3_threads.c : (this is a pthread enabled version) module load fftw3/3.3 gcc fftw3_threads.c -lpthread -lfftw3_threads -lfftw3 -lm ./a.out 512 8 fftw3_mpi.c : (note, only 2d or higher dimensional transforms supported in 3.3) module load fftw3/3.3+openmpi-1.6 mpicc fftw3_mpi.c -lfftw3_mpi -lfftw3 mpirun -np 4 ./a.out 512","title":"FFTW"},{"location":"midway23/software/libraries/fftw/#fftw","text":"FFTW, \u201cthe Fastest Fourier Transform in the West,\u201d is a popular open-source library for computing discrete Fourier transforms. It supports both real and complex transformations, in both single and double precision. RCC has configured and installed a large variety of fftw modules including different versions, compiler choices, and parallelization strategy. Please contact RCC if a different version or configuration is necessary for your work.","title":"FFTW"},{"location":"midway23/software/libraries/fftw/#fftw2","text":"FFTW 2.1.5 is an older, now unsupported version, but still commonly used by codes as the API has changed in later versions. The versions compiled by the RCC support shared memory parallelism through OpenMP and distributed parallelism through MPI. See FFTW2 Documentation for complete documentation of this version. The non-MPI modules include both single and double precision versions, as well as openMP support. These have been built for all three RCC supported compilers (gcc, intel, and pgi). The library defaults to double-precision, however single-precision can be used by adding the prefix s to all filenames and library calls (see the following ) for more details). The MPI modules are compiled for each MPI library and compiler (leading to a large number of available combinations). These should all be interchangeable, simply ensure you match the correct module to the MPI library and compiler you are using (the module system will complain otherwise). Each module adds the fftw2 library to your includes path, but for codes that prefer to self-configure, the environment variable FFTW2_DIR points to the currently loaded version. The RCC help system includes a sample code to perform 1-dimensional complex transformations in serial or in parallel. To compile and run each sample code, use the following commands: fftw2_serial.c : module load fftw2/2.1.5 gcc fftw2_serial.c -lm -lfftw ./a.out 512 fftw2_openmp.c : module load fftw2/2.1.5 gcc -fopenmp fftw2_openmp.c -lm -lfftw -lfftw_threads OMP_NUM_THREADS = 8 ./a.out 512 8 fftw2_mpi.c : module load openmpi/1.6 module load fftw2/2.1.5+openmpi-1.6 mpicc fftw2_mpi.c -lfftw_mpi -lfftw mpirun -np 4 ./a.out 512","title":"FFTW2"},{"location":"midway23/software/libraries/fftw/#fftw3","text":"The API for FFTW has significantly changed in the 3.X branch of FFTW. MPI support has only recently been re-included as a stable feature (3.3.X), and it is this version the RCC supports. As FFTW2 is no longer supported, we recommend users upgrade to the newest version if their code allows. Documentation for this version can be viewed at: FFTW3 Documentation Single precision support is included by post-fixing \u2018f\u2019 to commands and filenames, see this document Sample codes for serial, shared memory (openMP), and distributed (MPI) have been included in the RCC help system. Use the following to compile and run each sample: fftw3_serial.c : module load fftw3/3.3 gcc fftw3_serial.c -lm -lfftw ./a.out 512 fftw3_openmp.c : module load fftw3/3.3 gcc -fopenmp fftw3_openmp.c -lfftw3_omp -lfftw3 -lm OMP_NUM_THREADS = 8 ./a.out 512 fftw3_threads.c : (this is a pthread enabled version) module load fftw3/3.3 gcc fftw3_threads.c -lpthread -lfftw3_threads -lfftw3 -lm ./a.out 512 8 fftw3_mpi.c : (note, only 2d or higher dimensional transforms supported in 3.3) module load fftw3/3.3+openmpi-1.6 mpicc fftw3_mpi.c -lfftw3_mpi -lfftw3 mpirun -np 4 ./a.out 512","title":"FFTW3"},{"location":"midway23/software/libraries/mpi/","text":"Message Passing Interface (MPI) For more information on how to run MPI jobs on Midway see MPI jobs RCC supports the following MPI implementations: IntelMPI MVAPICH2 OpenMPI Each MPI implementation usually has a module available for use with GCC, the Intel Compiler Suite, and PGI. Please see module_tag_high performance computing for the list of available MPI modules. MPI Implementation Notes The different MPI implementations have different options and features. Any notable differences are noted here. IntelMPI IntelMPI uses an environment variable to affect the network communication fabric it uses: I_MPI_FABRICS() During job launch the Slurm TaskProlog detects the network hardware and sets this variable approately. This will typically be set to shm:ofa , which makes IntelMPI use shared memory communication followed by ibverbs. If a job is run on a node without Infiniband this will be set to shm which uses shared memory only and limits IntelMPI to a single node job. This is usually what is wanted on nodes without a high speed interconnect. This variable can be overridden if desired in the submission script. MVAPICH2 MVAPICH2 is compiled with the OFA-IB-CH3 interface. There is no support for running programs compiled with MVAPICH2 on loosely coupled nodes. GPUDirect builds of MVAPICH2 with CUDA enabled are available for use on the GPU nodes. These builds are otherwise identical to the standard MVAPICH2 build. OpenMPI Nothing at this time.","title":"Message Passing Interface (MPI)"},{"location":"midway23/software/libraries/mpi/#message-passing-interface-mpi","text":"For more information on how to run MPI jobs on Midway see MPI jobs RCC supports the following MPI implementations: IntelMPI MVAPICH2 OpenMPI Each MPI implementation usually has a module available for use with GCC, the Intel Compiler Suite, and PGI. Please see module_tag_high performance computing for the list of available MPI modules.","title":"Message Passing Interface (MPI)"},{"location":"midway23/software/libraries/mpi/#mpi-implementation-notes","text":"The different MPI implementations have different options and features. Any notable differences are noted here.","title":"MPI Implementation Notes"},{"location":"midway23/software/libraries/mpi/#intelmpi","text":"IntelMPI uses an environment variable to affect the network communication fabric it uses:","title":"IntelMPI"},{"location":"midway23/software/libraries/mpi/#i_mpi_fabrics","text":"During job launch the Slurm TaskProlog detects the network hardware and sets this variable approately. This will typically be set to shm:ofa , which makes IntelMPI use shared memory communication followed by ibverbs. If a job is run on a node without Infiniband this will be set to shm which uses shared memory only and limits IntelMPI to a single node job. This is usually what is wanted on nodes without a high speed interconnect. This variable can be overridden if desired in the submission script.","title":"I_MPI_FABRICS()"},{"location":"midway23/software/libraries/mpi/#mvapich2","text":"MVAPICH2 is compiled with the OFA-IB-CH3 interface. There is no support for running programs compiled with MVAPICH2 on loosely coupled nodes. GPUDirect builds of MVAPICH2 with CUDA enabled are available for use on the GPU nodes. These builds are otherwise identical to the standard MVAPICH2 build.","title":"MVAPICH2"},{"location":"midway23/software/libraries/mpi/#openmpi","text":"Nothing at this time.","title":"OpenMPI"},{"location":"midway23/software/libraries/netcdf/","text":"NetCDF To run NetCDF codes they must be compiled first, usually from a C or Fortran file The module system has two versions of NetCDF, 3.6.3 and 4.2 (4.2 may be updated) The reason for this is due to incompatibilities and between them - particularly with the PGI family of compilers. To help run fortran code simply there are several files that go along with this help file. Copy and run those files to test version compatibility. A sample file is also provided to verify proper functionality. Example: ./pgf90-netcdf-3.6.3 simple_xy_wr.f90 output_file ./output_file *** SUCCESS writing example file simple_xy.nc!","title":"NetCDF"},{"location":"midway23/software/libraries/netcdf/#netcdf","text":"To run NetCDF codes they must be compiled first, usually from a C or Fortran file The module system has two versions of NetCDF, 3.6.3 and 4.2 (4.2 may be updated) The reason for this is due to incompatibilities and between them - particularly with the PGI family of compilers. To help run fortran code simply there are several files that go along with this help file. Copy and run those files to test version compatibility. A sample file is also provided to verify proper functionality. Example: ./pgf90-netcdf-3.6.3 simple_xy_wr.f90 output_file ./output_file *** SUCCESS writing example file simple_xy.nc!","title":"NetCDF"},{"location":"midway23/software/modules/","text":"Software Modules All software available via the modules system on Midway2 are listed on this page. To load any particular software module shown below use the following syntax from the command line on Midway2: [jbravo@midway2 ~]$ module load <modulename> where <modulename> is the name of the software module. See the page Using software modules on Midway for more information.","title":"Software Modules"},{"location":"midway23/software/modules/#software-modules","text":"All software available via the modules system on Midway2 are listed on this page. To load any particular software module shown below use the following syntax from the command line on Midway2: [jbravo@midway2 ~]$ module load <modulename> where <modulename> is the name of the software module. See the page Using software modules on Midway for more information.","title":"Software Modules"},{"location":"midwaybooth/","text":"GM4 User Guide The user guide is built witht he following: MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs. Instructions Once you have installed MkDocs, render the webpages and view them as follows: cd midwayr/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"GM4 User Guide"},{"location":"midwaybooth/#gm4-user-guide","text":"The user guide is built witht he following: MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs.","title":"GM4 User Guide"},{"location":"midwaybooth/#instructions","text":"Once you have installed MkDocs, render the webpages and view them as follows: cd midwayr/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"Instructions"},{"location":"midwaybooth/docs/","text":"The MidwayBooth Cluster User Guide MidwayBooth is a high performance computing cluster within Midway ecosystem dedicated to computationally intensive Social Science research and educational excellence. This user guide provides an overview of the cluster setup and information on how users can access and utilize this resource. The following pages provide information on commonly referenced topics for using MidwayBooth. For anything not addressed in this User Guide, please direct your questions to help@rcc.uchicago.edu","title":"The MidwayBooth Cluster User Guide"},{"location":"midwaybooth/docs/#the-midwaybooth-cluster-user-guide","text":"MidwayBooth is a high performance computing cluster within Midway ecosystem dedicated to computationally intensive Social Science research and educational excellence. This user guide provides an overview of the cluster setup and information on how users can access and utilize this resource. The following pages provide information on commonly referenced topics for using MidwayBooth. For anything not addressed in this User Guide, please direct your questions to help@rcc.uchicago.edu","title":"The MidwayBooth Cluster User Guide"},{"location":"midwaybooth/docs/cluster/","text":"MidwayBooth Cluster Overview This section of the documentation provides an overview of how the MidwayBooth cluster is organized. Partitions The MidwayBooth cluster consists of 21 Intel Caslake based nodes. The nodes are accessible thourgh the partition ssd . This partition includes all 21 nodes and is accessible to PI groups affiliated with the Division of Social Sciences (SSD). Three login nodes are designated for interactive sessions. The remaining 18 nodes are compute nodes. Slurm Quality of Service (QOS) The fair-share use of the MidwayBooth resources is managed through the Slurm scheduler Quality of Service (QOS) settings. The quality of service defines the type of resources that a job can request and whether the job is a production or debug run. The resource settings for each QOS are defined as follows: QOS Name: MidwayBooth (default if no QOS specified) Per User Settings Max Wall Time QOS Priority Max Running Jobs Max Jobs Submit Max CPUs 1-36:00:00 10000 28 28 320","title":"Index"},{"location":"midwaybooth/docs/cluster/#midwaybooth-cluster-overview","text":"This section of the documentation provides an overview of how the MidwayBooth cluster is organized.","title":"MidwayBooth Cluster Overview"},{"location":"midwaybooth/docs/cluster/#partitions","text":"The MidwayBooth cluster consists of 21 Intel Caslake based nodes. The nodes are accessible thourgh the partition ssd . This partition includes all 21 nodes and is accessible to PI groups affiliated with the Division of Social Sciences (SSD). Three login nodes are designated for interactive sessions. The remaining 18 nodes are compute nodes.","title":"Partitions"},{"location":"midwaybooth/docs/cluster/#slurm-quality-of-service-qos","text":"The fair-share use of the MidwayBooth resources is managed through the Slurm scheduler Quality of Service (QOS) settings. The quality of service defines the type of resources that a job can request and whether the job is a production or debug run. The resource settings for each QOS are defined as follows:","title":"Slurm Quality of Service (QOS)"},{"location":"midwaybooth/docs/connecting/","text":"Accessing MidwayBooth To use MidwayBooth resources, you will need to have a Midway user account. If you do not have a Midway user account, please see the Getting Started section on how to apply. Note that you will either register for a PI or general user account. Please note that you must have enabled Two Factor Authentication for your CNetID before connecting to MidwayBooth. External collaborators will require a CNetID and should apply as a general user under their designated PI. Connecting to MidwayBooth There are two alternative ways to connect to MidwayBooth. You may use a graphical user interface known as ThinLinc. This is accessed via your web browser and will give you a familar desktop-like environment for accessing your data and software. The second option uses an SSH connection in your terminal for command line interaction with MidwayBooth. Connecting with ThinLinc ThinLinc is a remote desktop server application. We recommend using ThinLinc when you run software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). For your convenience, the ThinLinc interface has been modified to give you a comfortable, familar experience comparable to your local machine, while you interact with the cluster. To use ThinLinc to connect to MidwayBooth, please take the following steps: First connect to the UChicago VPN. You may use the Cisco AnyConnect client. Connect to cvpn.uchicago.edu . You will need to follow the two factor authentication prompts. Once connected to the VPN, open a browser (Chrome or Firefox) and enter https://ssd.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. This is the ThinLinc GUI, indicating that you are now working on MidwayBooth: In order to access the software applications or a terminal window via ThinLinc, please select the Activities menu in the upper left corner or hover over and click on the icon of nine dots in a square along the left side as indicated below: The resulting view from opening the Applications window should look like the picture below. From here, you can click on any of the software applications and run as you would on a local machine. Please refer to the subsequent section on running applications in ThinLinc for more details: To disconnect and exit ThinLinc, close your selected application and click on the power button symbol in the upper right corner of the desktop as indicated below. Navigate to your name and select log out. Please note that closing the window tab will not end your interactive session on the login node. It is essential to disconnect when you have completed your work on MidwayBooth prior to closing the browser window. When successful, you should see the following confirmation of your disconnected session. Connecting with an SSH client If you are more comfortable accessing MidwayBooth via SSH in your terminal, you can explore your file structure, submit jobs and move files without the ThinLinc graphical user interface. In this case, please open your terminal window on your local machine. Enter ssh cnetid@ssd.rcc.uchicago.edu an hit return: Next, enter your CnetID and password. (There should be an additional two-factor authentication at this step.) At this point, if you successfully passed the password authentication step, you should be connected to one of the MidwayBooth login nodes, either ssd001 , ssd002 or ssd003 . To disconnect and close your MidwayBooth connection in the terminal, type exit and hit return.","title":"Index"},{"location":"midwaybooth/docs/connecting/#accessing-midwaybooth","text":"To use MidwayBooth resources, you will need to have a Midway user account. If you do not have a Midway user account, please see the Getting Started section on how to apply. Note that you will either register for a PI or general user account. Please note that you must have enabled Two Factor Authentication for your CNetID before connecting to MidwayBooth. External collaborators will require a CNetID and should apply as a general user under their designated PI.","title":"Accessing MidwayBooth"},{"location":"midwaybooth/docs/connecting/#connecting-to-midwaybooth","text":"There are two alternative ways to connect to MidwayBooth. You may use a graphical user interface known as ThinLinc. This is accessed via your web browser and will give you a familar desktop-like environment for accessing your data and software. The second option uses an SSH connection in your terminal for command line interaction with MidwayBooth.","title":"Connecting to MidwayBooth"},{"location":"midwaybooth/docs/connecting/#connecting-with-thinlinc","text":"ThinLinc is a remote desktop server application. We recommend using ThinLinc when you run software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). For your convenience, the ThinLinc interface has been modified to give you a comfortable, familar experience comparable to your local machine, while you interact with the cluster. To use ThinLinc to connect to MidwayBooth, please take the following steps: First connect to the UChicago VPN. You may use the Cisco AnyConnect client. Connect to cvpn.uchicago.edu . You will need to follow the two factor authentication prompts. Once connected to the VPN, open a browser (Chrome or Firefox) and enter https://ssd.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. This is the ThinLinc GUI, indicating that you are now working on MidwayBooth: In order to access the software applications or a terminal window via ThinLinc, please select the Activities menu in the upper left corner or hover over and click on the icon of nine dots in a square along the left side as indicated below: The resulting view from opening the Applications window should look like the picture below. From here, you can click on any of the software applications and run as you would on a local machine. Please refer to the subsequent section on running applications in ThinLinc for more details: To disconnect and exit ThinLinc, close your selected application and click on the power button symbol in the upper right corner of the desktop as indicated below. Navigate to your name and select log out. Please note that closing the window tab will not end your interactive session on the login node. It is essential to disconnect when you have completed your work on MidwayBooth prior to closing the browser window. When successful, you should see the following confirmation of your disconnected session.","title":"Connecting with ThinLinc"},{"location":"midwaybooth/docs/connecting/#connecting-with-an-ssh-client","text":"If you are more comfortable accessing MidwayBooth via SSH in your terminal, you can explore your file structure, submit jobs and move files without the ThinLinc graphical user interface. In this case, please open your terminal window on your local machine. Enter ssh cnetid@ssd.rcc.uchicago.edu an hit return: Next, enter your CnetID and password. (There should be an additional two-factor authentication at this step.) At this point, if you successfully passed the password authentication step, you should be connected to one of the MidwayBooth login nodes, either ssd001 , ssd002 or ssd003 . To disconnect and close your MidwayBooth connection in the terminal, type exit and hit return.","title":"Connecting with an SSH client"},{"location":"midwaybooth/docs/datatransfer/","text":"MidwayBooth Data Transfer This section of the documentation provides an overview of data transfer mechanisms for MidwayBooth. Please remember that MidwayBooth is a partition on the Midway3 cluster. The SSD has purchased storage for SSD faculty and student researchers; however, your storage is allocated on the Midway3 cluster. Therefore, the data transfer process described below is identical to the description for the Midway3 Intel and AMD partitions. Your storage is mounted to the greater cluster, allowing you to specify which partition you would like to use for your compute needs. RCC provides a number of methods for accessing and transferring data in/out of our systems. We recommend the SCP protocol for transferring files to/from RCC systems. RCC hosts a managed Globus Online endpoint that can be used for moving very large amounts of data. SSH (SCP or SFTP) Secure copy or SCP is a means of securely transferring computer files between a local host and a remote host. It is based on the Secure Shell (SSH) and Secure File Transfer (SFTP) protocols. To transfer files or directories from your local computer to your home directory on Midway3, open a terminal window and issue the command: $ scp <some file> <CNetID>@midway3.rcc.uchicago.edu: Or for directories: $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu: Or to connect to a directory on Midway3 (/project, for example): $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu:/project SAMBA SAMBA allows uses to connect to (or \u201cmount\u201d) their home and project directories on their local computer so that the file system on Midway3 appears as if it were directly connected to the local machine. This method of accessing your RCC home and project space is only available from within the UChicago campus network. From off-campus you will need to first connect through the UChicago VPN. Your SAMBA account credentials are your CNetID and password: Username: ADLOCAL\\CNetID Password: CNet password Hostname: midway3smb.rcc.uchicago.edu Note: Make sure to prefix your username with ADLOCAL\\ Connecting from Windows On a Windows computer, use the \u201cMap Network Drive\u201d option: * Enter one of the following UNC paths depending on which location on Midway you wish to connect to: home: \\\\midway3smb.rcc.uchicago.edu\\homes project: \\\\midway3smb.rcc.uchicago.edu\\project scratch: \\\\midway3smb.rcc.uchicago.edu\\midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password. Connecting from Mac OS X To connect on a Mac OS X computer follow these steps: Open the Connect to Server utility in Finder Enter one of the following URLs in the input box for Server Address depending on which location on Midway you wish to connect to: home: smb://midway3smb.rcc.uchicago.edu/homes project: smb://midway3smb.rcc.uchicago.edu/project scratch/midway: smb://midway3smb.rcc.uchicago.edu/midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password. Connecting from Ubuntu From Ubuntu, follow the steps here: https://wiki.ubuntu.com/MountWindowsSharesPermanently. Specifically, Install cifs sudo apt-get install cifs-utils . Create a folder to contain the mounted filesystem, e.g. mkdir /media/midway Put your RCC credentials in a text file. Create a file .smbcredentials in your home directory containing: username=[midway username] password=[midway password] Change permissions so only you can read the file chmod 600 ~/.smbcredentials . Edit the file /etc/fstab as root and add the following on one line at the end (Note: You can change \u201cproject\u201d in the below line to \u201chomes\u201d, \u201cproject2\u201d, or \u201cmidway3-scratch\u201d depending on which location you wish to mount): //midway3smb.rcc.uchicago.edu/project2 /media/midway cifs credentials=/home/[username]/.smbcredentials,domain=ADLOCAL,iocharset=utf8,sec=ntlm,vers=2.0 0 0 Remount everything in /etc/fstab with the command sudo mount -a . Your Midway folders should now be accessible at /media/midway . HTTP (web access) RCC provides web access to data on their storage system via public_html directories in users\u2019 home directories. Be sure your home directories and public_html have the execute bit set, and that public_html has read permissions if you would like to allow indexing (directory listings and automatic selection of index.html). For example, these are the commands for setting up Web access to your home directory, where $HOME is the environment variable specifying the location of your home directory: chmod o+x $HOME mkdir -p $HOME/public_html chmod o+x $HOME/public_html chmod o+r $HOME/public_html The last line is optional and only needed if you would like to allow directory listing. Files in public_html must also be readable by the web user (other), but should not be made executable, e.g., chmod o+r $HOME/public_html/research.dat Note: Use of these directories must conform with the RCC usage policy ( https://rcc.uchicago.edu/about-rcc/rcc-user-policy ). Please notify RCC if you expect a large number of people to access data hosted here. Globus Online Globus Online is a robust tool for transferring large data files to and from Midway3. The RCC has a customized Globus Online login site at https://globus.rcc.uchicago.edu and uses the Single Sign On capabilities of CILogon. Once you have signed up, here is the connection information for Midway3: URL: https://globus.rcc.uchicago.edu End Point: ucrcc#midway3 For full instructions, please see Globus Online Data Transfer .","title":"MidwayBooth Data Transfer"},{"location":"midwaybooth/docs/datatransfer/#midwaybooth-data-transfer","text":"This section of the documentation provides an overview of data transfer mechanisms for MidwayBooth. Please remember that MidwayBooth is a partition on the Midway3 cluster. The SSD has purchased storage for SSD faculty and student researchers; however, your storage is allocated on the Midway3 cluster. Therefore, the data transfer process described below is identical to the description for the Midway3 Intel and AMD partitions. Your storage is mounted to the greater cluster, allowing you to specify which partition you would like to use for your compute needs. RCC provides a number of methods for accessing and transferring data in/out of our systems. We recommend the SCP protocol for transferring files to/from RCC systems. RCC hosts a managed Globus Online endpoint that can be used for moving very large amounts of data.","title":"MidwayBooth Data Transfer"},{"location":"midwaybooth/docs/datatransfer/#ssh-scp-or-sftp","text":"Secure copy or SCP is a means of securely transferring computer files between a local host and a remote host. It is based on the Secure Shell (SSH) and Secure File Transfer (SFTP) protocols. To transfer files or directories from your local computer to your home directory on Midway3, open a terminal window and issue the command: $ scp <some file> <CNetID>@midway3.rcc.uchicago.edu: Or for directories: $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu: Or to connect to a directory on Midway3 (/project, for example): $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu:/project","title":"SSH (SCP or SFTP)"},{"location":"midwaybooth/docs/datatransfer/#samba","text":"SAMBA allows uses to connect to (or \u201cmount\u201d) their home and project directories on their local computer so that the file system on Midway3 appears as if it were directly connected to the local machine. This method of accessing your RCC home and project space is only available from within the UChicago campus network. From off-campus you will need to first connect through the UChicago VPN. Your SAMBA account credentials are your CNetID and password: Username: ADLOCAL\\CNetID Password: CNet password Hostname: midway3smb.rcc.uchicago.edu Note: Make sure to prefix your username with ADLOCAL\\","title":"SAMBA"},{"location":"midwaybooth/docs/datatransfer/#connecting-from-windows","text":"On a Windows computer, use the \u201cMap Network Drive\u201d option: * Enter one of the following UNC paths depending on which location on Midway you wish to connect to: home: \\\\midway3smb.rcc.uchicago.edu\\homes project: \\\\midway3smb.rcc.uchicago.edu\\project scratch: \\\\midway3smb.rcc.uchicago.edu\\midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password.","title":"Connecting from Windows"},{"location":"midwaybooth/docs/datatransfer/#connecting-from-mac-os-x","text":"To connect on a Mac OS X computer follow these steps: Open the Connect to Server utility in Finder Enter one of the following URLs in the input box for Server Address depending on which location on Midway you wish to connect to: home: smb://midway3smb.rcc.uchicago.edu/homes project: smb://midway3smb.rcc.uchicago.edu/project scratch/midway: smb://midway3smb.rcc.uchicago.edu/midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password.","title":"Connecting from Mac OS X"},{"location":"midwaybooth/docs/datatransfer/#connecting-from-ubuntu","text":"From Ubuntu, follow the steps here: https://wiki.ubuntu.com/MountWindowsSharesPermanently. Specifically, Install cifs sudo apt-get install cifs-utils . Create a folder to contain the mounted filesystem, e.g. mkdir /media/midway Put your RCC credentials in a text file. Create a file .smbcredentials in your home directory containing: username=[midway username] password=[midway password] Change permissions so only you can read the file chmod 600 ~/.smbcredentials . Edit the file /etc/fstab as root and add the following on one line at the end (Note: You can change \u201cproject\u201d in the below line to \u201chomes\u201d, \u201cproject2\u201d, or \u201cmidway3-scratch\u201d depending on which location you wish to mount): //midway3smb.rcc.uchicago.edu/project2 /media/midway cifs credentials=/home/[username]/.smbcredentials,domain=ADLOCAL,iocharset=utf8,sec=ntlm,vers=2.0 0 0 Remount everything in /etc/fstab with the command sudo mount -a . Your Midway folders should now be accessible at /media/midway .","title":"Connecting from Ubuntu"},{"location":"midwaybooth/docs/datatransfer/#http-web-access","text":"RCC provides web access to data on their storage system via public_html directories in users\u2019 home directories. Be sure your home directories and public_html have the execute bit set, and that public_html has read permissions if you would like to allow indexing (directory listings and automatic selection of index.html). For example, these are the commands for setting up Web access to your home directory, where $HOME is the environment variable specifying the location of your home directory: chmod o+x $HOME mkdir -p $HOME/public_html chmod o+x $HOME/public_html chmod o+r $HOME/public_html The last line is optional and only needed if you would like to allow directory listing. Files in public_html must also be readable by the web user (other), but should not be made executable, e.g., chmod o+r $HOME/public_html/research.dat Note: Use of these directories must conform with the RCC usage policy ( https://rcc.uchicago.edu/about-rcc/rcc-user-policy ). Please notify RCC if you expect a large number of people to access data hosted here.","title":"HTTP (web access)"},{"location":"midwaybooth/docs/datatransfer/#globus-online","text":"Globus Online is a robust tool for transferring large data files to and from Midway3. The RCC has a customized Globus Online login site at https://globus.rcc.uchicago.edu and uses the Single Sign On capabilities of CILogon. Once you have signed up, here is the connection information for Midway3: URL: https://globus.rcc.uchicago.edu End Point: ucrcc#midway3 For full instructions, please see Globus Online Data Transfer .","title":"Globus Online"},{"location":"midwaybooth/docs/interactive-jobs/","text":"Running Jobs Interactively on MidwayBooth This section of the documentation describes how to use the MidwayBooth cluster to run an interactive session with either ThinLinc or an ssh connection. Remember that you must connect to MidwayBooth first and can refer to the previous section in this User Guide for more information. An interactive session is run directly on one of the login nodes. This can be helpful when you are debugging code, unit testing to determine your resource allocation needs and completing tasks such as module installation or web scraping that require internet connectivity. The alternative to an interactive session is submission of a batch job to one or more of the compute nodes. This is done using a short batch script and will be explained in the next section of the User Guide. An Interactive Session in ThinLinc You open an interactive session as soon as you have logged into MidwayBooth via ThinLinc. You can click on the software that you require and run directly on that login node. If you close the ThinLinc tab or your browser window, your interactive session on MidwayBooth will continue to run while you use your local machine. You can access your results, or return to your session by logging in again with your CNetID. Computations run on the login node are not counted in terms of your service hour usage. It should be noted that there are only three login nodes. Jobs that computationally intensive or long running should be completed on a compute node and may be killed by the system monitor. You must remember to disconnect from ThinLinc and end your interactive session when your computation is complete. Failing to do so can result in future login failures. Example: Launching Stata Interactively in ThinLinc Once you have logged into MidwayBooth and are connected via ThinLinc, you can select the Stata icon from the desktop. Your screen should look like the image below. You are ready to begin your interactive session in Stata. When you finish your calculations, please exit as you usually would, by selecting exit under the file menu and then closing the window. You will still need to disconnect from the ThinLinc session.","title":"Running Jobs Interactively on MidwayBooth"},{"location":"midwaybooth/docs/interactive-jobs/#running-jobs-interactively-on-midwaybooth","text":"This section of the documentation describes how to use the MidwayBooth cluster to run an interactive session with either ThinLinc or an ssh connection. Remember that you must connect to MidwayBooth first and can refer to the previous section in this User Guide for more information. An interactive session is run directly on one of the login nodes. This can be helpful when you are debugging code, unit testing to determine your resource allocation needs and completing tasks such as module installation or web scraping that require internet connectivity. The alternative to an interactive session is submission of a batch job to one or more of the compute nodes. This is done using a short batch script and will be explained in the next section of the User Guide.","title":"Running Jobs Interactively on MidwayBooth"},{"location":"midwaybooth/docs/interactive-jobs/#an-interactive-session-in-thinlinc","text":"You open an interactive session as soon as you have logged into MidwayBooth via ThinLinc. You can click on the software that you require and run directly on that login node. If you close the ThinLinc tab or your browser window, your interactive session on MidwayBooth will continue to run while you use your local machine. You can access your results, or return to your session by logging in again with your CNetID. Computations run on the login node are not counted in terms of your service hour usage. It should be noted that there are only three login nodes. Jobs that computationally intensive or long running should be completed on a compute node and may be killed by the system monitor. You must remember to disconnect from ThinLinc and end your interactive session when your computation is complete. Failing to do so can result in future login failures.","title":"An Interactive Session in ThinLinc"},{"location":"midwaybooth/docs/interactive-jobs/#example-launching-stata-interactively-in-thinlinc","text":"Once you have logged into MidwayBooth and are connected via ThinLinc, you can select the Stata icon from the desktop. Your screen should look like the image below. You are ready to begin your interactive session in Stata. When you finish your calculations, please exit as you usually would, by selecting exit under the file menu and then closing the window. You will still need to disconnect from the ThinLinc session.","title":"Example: Launching Stata Interactively in ThinLinc"},{"location":"midwaybooth/docs/storage/","text":"Storage Allocations for MidwayBooth A Principal Investigator (PI) account for PI eligible faculty members and staff will be assigned a standard storage allocation of 500GB. This allocation can be increased to 2GB upon request at no added cost to the faculty member. Additional storage can be purchased by contacting help@rcc.uchicago.edu . General user accounts are assigned to PI groups and will have access to share storage with the PI and other users within the PI group. Folder read and write access must be set by the PI. Student accounts for select degree programs will be alloted minimal storage for the entire period of active degree enrollment.","title":"Index"},{"location":"midwaybooth/docs/storage/#storage-allocations-for-midwaybooth","text":"A Principal Investigator (PI) account for PI eligible faculty members and staff will be assigned a standard storage allocation of 500GB. This allocation can be increased to 2GB upon request at no added cost to the faculty member. Additional storage can be purchased by contacting help@rcc.uchicago.edu . General user accounts are assigned to PI groups and will have access to share storage with the PI and other users within the PI group. Folder read and write access must be set by the PI. Student accounts for select degree programs will be alloted minimal storage for the entire period of active degree enrollment.","title":"Storage Allocations for MidwayBooth"},{"location":"midwaybooth/docs/submit-jobs/","text":"Submitting Jobs on MidwayBooth using the Slurm Batch Scheduler A batch job is a compute task that you would like to schedule to run on MidwayBooth without your active participation in the session. Often these are tasks designed to utilize multiple cores in a parallel fashion to help enhance the compute efficiency of your job. Running a scheduled job on the compute nodes means that you request resources on MidwayBooth and the Slurm job scheduler allocates your requested resources while balancing the job requests of other authorized users. This ensures the efficient and fair utilization for all MidwayBooth users. Please remember that you will not be able to run jobs that require internet connectivity on the compute nodes. Running Jobs on a Compute Node You may request resources to run your job on a compute node through either ThinLinc or SSH. You will select the resources that you require and be notified upon job completion by an automated systems message to your email. If you choose to submit a job via ThinLinc, you will do this by selecting the Slurm Icon and following the prompts in the dialog box to choose the resources that you require and account under which you are currently working. If you prefer to submit a batch script via SSH, this is done from the terminal prompt on your local machine. Please connect first via ssh and follow the recommendations for writing this script as indicated below. Requesting Resources for Your Job If no wall time is specified in the resource request it will default to the 36 hour max wall time limit. An example sbatch resource request script for a 2 node, 80 core, mpi parallel job that uses the MidwayBooth partition is shown below. #!/bin/bash #SBATCH --time=1-12:00:00 #SBATCH --partition=ssd #SBATCH --account=pi-name # You must specify the pi-account you are running under #SBATCH --nodes=2 # Number of nodes #SBATCH --ntasks-per-node=40 # Number of tasks #SBATCH --cpus-per-task=1 # Number of threads per task #SBATCH --qos=MidwayBooth # QOS # # SET NUM TASKS NTASKS=$(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) # # SET NUMBER OF THREADS export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK #LOAD MODULES (e.g. intelmpi) module load intelmpi/2018.2.199+intel-18.0 # EXECUTE JOB mpirun -np $NTASKS myjob.x # # EOF","title":"Submitting Jobs on MidwayBooth using the Slurm Batch Scheduler"},{"location":"midwaybooth/docs/submit-jobs/#submitting-jobs-on-midwaybooth-using-the-slurm-batch-scheduler","text":"A batch job is a compute task that you would like to schedule to run on MidwayBooth without your active participation in the session. Often these are tasks designed to utilize multiple cores in a parallel fashion to help enhance the compute efficiency of your job. Running a scheduled job on the compute nodes means that you request resources on MidwayBooth and the Slurm job scheduler allocates your requested resources while balancing the job requests of other authorized users. This ensures the efficient and fair utilization for all MidwayBooth users. Please remember that you will not be able to run jobs that require internet connectivity on the compute nodes.","title":"Submitting Jobs on MidwayBooth using the Slurm Batch Scheduler"},{"location":"midwaybooth/docs/submit-jobs/#running-jobs-on-a-compute-node","text":"You may request resources to run your job on a compute node through either ThinLinc or SSH. You will select the resources that you require and be notified upon job completion by an automated systems message to your email. If you choose to submit a job via ThinLinc, you will do this by selecting the Slurm Icon and following the prompts in the dialog box to choose the resources that you require and account under which you are currently working. If you prefer to submit a batch script via SSH, this is done from the terminal prompt on your local machine. Please connect first via ssh and follow the recommendations for writing this script as indicated below.","title":"Running Jobs on a Compute Node"},{"location":"midwaybooth/docs/submit-jobs/#requesting-resources-for-your-job","text":"If no wall time is specified in the resource request it will default to the 36 hour max wall time limit. An example sbatch resource request script for a 2 node, 80 core, mpi parallel job that uses the MidwayBooth partition is shown below. #!/bin/bash #SBATCH --time=1-12:00:00 #SBATCH --partition=ssd #SBATCH --account=pi-name # You must specify the pi-account you are running under #SBATCH --nodes=2 # Number of nodes #SBATCH --ntasks-per-node=40 # Number of tasks #SBATCH --cpus-per-task=1 # Number of threads per task #SBATCH --qos=MidwayBooth # QOS # # SET NUM TASKS NTASKS=$(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) # # SET NUMBER OF THREADS export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK #LOAD MODULES (e.g. intelmpi) module load intelmpi/2018.2.199+intel-18.0 # EXECUTE JOB mpirun -np $NTASKS myjob.x # # EOF","title":"Requesting Resources for Your Job"},{"location":"midwaybooth/docs/system/","text":"System Overview MidwayBooth is composed of 21 decicated compute nodes within the Midway high-performance ecosystem. MidwayBooth features Intel Cascade Lake nodes, HDR InfiniBand interconnect, and SSD storage controllers for more performant small file I/O. The total installed storage on MidwayBooth is 400TB, whereby 100TB is reserved for the secure data enclave, MidwayR. The cluster uses Slurm as its workload manger and the software environment module to manage installed software. Intel Hardware Key Features: MidwayBooth hosts three login nodes: midwaybooth-login1 , midwaybooth-login2 and midwaybooth-login3 as well as 18 compute nodes CPUs: 21 nodes total All nodes have HDR InfiniBand (100 Gbps) network cards Each node has 960 GB SSD local disk Operating System throughout cluster is CentOS 8 File Systems: There is a total of 300TB of raw parallel file storage that is part of the MidwayBooth cluster. The new storage system is designed such that any file smaller than 4KB in size is bundled with the file metadata and stored on the pool of dedicated metadata SSDs that are part of the parallel storage system. This can lead to considerable improvement in performance for small file size (<4KB) I/O. This feature was not available with Midway2, where the metadata and data were stored on slower mechanical hard drives. Using MidwayBooth: MidwayBooth nodes run CentOS 8. Its job scheduler is the Slurm . Slurm commands enable you to submit, manage, monitor, and control your jobs. Software: If using ThinLinc, available software is visible as icons on the desktop. Organized in modules Use module avail , to see what is available To load a particular available package, for example, gcc version 8.2.0, do module load gcc/8.2.0 If you do not specify a version of the package, the default one is loaded To see what environmental variables are modified when gcc/8.2.0 is loaded, do module show gcc/8.2.0 To unload gcc/8.2.0 , do module unload gcc/8.2.0 Definition of Terms: When reading the User Guide, please keep in mind the following: A core is the basic computational unit of a multiprocessor CPU. A node is a single machine with memory, cores, operating system and network connection. A partition is a collection of nodes with similar technical specifications (memory, cores, etc.) The cluster is the complete collection of nodes with networking and file storage facilities.","title":"System Overview"},{"location":"midwaybooth/docs/system/#system-overview","text":"MidwayBooth is composed of 21 decicated compute nodes within the Midway high-performance ecosystem. MidwayBooth features Intel Cascade Lake nodes, HDR InfiniBand interconnect, and SSD storage controllers for more performant small file I/O. The total installed storage on MidwayBooth is 400TB, whereby 100TB is reserved for the secure data enclave, MidwayR. The cluster uses Slurm as its workload manger and the software environment module to manage installed software. Intel Hardware Key Features: MidwayBooth hosts three login nodes: midwaybooth-login1 , midwaybooth-login2 and midwaybooth-login3 as well as 18 compute nodes CPUs: 21 nodes total All nodes have HDR InfiniBand (100 Gbps) network cards Each node has 960 GB SSD local disk Operating System throughout cluster is CentOS 8 File Systems: There is a total of 300TB of raw parallel file storage that is part of the MidwayBooth cluster. The new storage system is designed such that any file smaller than 4KB in size is bundled with the file metadata and stored on the pool of dedicated metadata SSDs that are part of the parallel storage system. This can lead to considerable improvement in performance for small file size (<4KB) I/O. This feature was not available with Midway2, where the metadata and data were stored on slower mechanical hard drives. Using MidwayBooth: MidwayBooth nodes run CentOS 8. Its job scheduler is the Slurm . Slurm commands enable you to submit, manage, monitor, and control your jobs. Software: If using ThinLinc, available software is visible as icons on the desktop. Organized in modules Use module avail , to see what is available To load a particular available package, for example, gcc version 8.2.0, do module load gcc/8.2.0 If you do not specify a version of the package, the default one is loaded To see what environmental variables are modified when gcc/8.2.0 is loaded, do module show gcc/8.2.0 To unload gcc/8.2.0 , do module unload gcc/8.2.0 Definition of Terms: When reading the User Guide, please keep in mind the following: A core is the basic computational unit of a multiprocessor CPU. A node is a single machine with memory, cores, operating system and network connection. A partition is a collection of nodes with similar technical specifications (memory, cores, etc.) The cluster is the complete collection of nodes with networking and file storage facilities.","title":"System Overview"},{"location":"midwayr/","text":"MidwayR User Guide The user guide will be built using MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs. Instructions Once you have installed MkDocs, it is easy to render the webpages and view them: cd midwayr/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"MidwayR User Guide"},{"location":"midwayr/#midwayr-user-guide","text":"The user guide will be built using MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs.","title":"MidwayR User Guide"},{"location":"midwayr/#instructions","text":"Once you have installed MkDocs, it is easy to render the webpages and view them: cd midwayr/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"Instructions"},{"location":"midwayr/midwayR_overview/","text":"MidwayR","title":"MidwayR"},{"location":"midwayr/midwayR_overview/#midwayr","text":"","title":"MidwayR"},{"location":"midwayr/docs/","text":"Welcome to MidwayR user guide MidwayR is the RCC's secure cluster that provides a secure computing environment to support research with higher security standard requirements. If you have any questions about MidwayR, please send an email to midwayr@rcc.uchicago.edu.","title":"Welcome to MidwayR user guide"},{"location":"midwayr/docs/#welcome-to-midwayr-user-guide","text":"MidwayR is the RCC's secure cluster that provides a secure computing environment to support research with higher security standard requirements. If you have any questions about MidwayR, please send an email to midwayr@rcc.uchicago.edu.","title":"Welcome to MidwayR user guide"},{"location":"midwayr/docs/connecting/","text":"Accessing MidwayR To use MidwayR resources, you will need to have a MidwayR user account. If you do not have a MidwayR user account, please see the Getting Started section for how to apply for an account. Note Although both Midway and MidwayR use CNetID for authentication, they do not share the accounts. In other words, if you already have an account on Midway, you will still need to apply for an account on MidwayR to be able to use MidwayR resources. Please also note that you must have enabled Two Factor Authentication for your CNetID before connecting to MidwayR. Accessing MidwayR is a two-step process: Login to Secure Data Enclave (SDE) Desktop using the Virtual Desktop Infrastructure (VDI) client. Once you are connected to the SDE Desktop, login to MidwayR using ThinLinc or an SSH client. Connecting to the SDE Desktop To connect to the SDE Desktop, you will need to download and install the VDI client on your computer. The VDI client, \"VMware Horizon\", can be downloaded from here . Note that you will need to be connected to the University of Chicago VPN (see below) to access this software. Once you have installed the VDI client on your computer, connect to the SDE Desktop by taking the following steps: Login to the University of Chicago VPN (see here for details). ( Note: you need to use VPN regardless of whether you are connecting from the campus network or not.) Start up the VDI client on your computer. If you are connecting to the SDE Desktop for the first time, add a new server by double-clicking on the \"Add Server\" or \"New Server\" button, which should appear on the screen as a button with a plus sign (+). Enter vdesk-sde.uchicago.edu as the name of the Connection Server, then click the \"Connect\" button (note that your screen may look slightly different than what is shown here): Enter your user name (your CNetID), enter your password, set the domain to \"ADLOCAL\", then clik the \"Login\" button: Double-click on the \"Secure Data Enclave\" icon and you will enter to the SDE environment: At this point, you should be logged in to the SDE environment. Connecting to MidwayR Once you are connected to the SDE environment using the VDI client following the steps given above, please follow one of the methods below to connect to MidwayR from the SDE environment. Connecting with ThinLinc ThinLinc is a remote desktop server application. It is recommended to use ThinLinc when you run a software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). To use ThinLinc to connect to MidwayR, please take the following steps: Open a browser (Chrome or Firefox) and enter https://sde.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. To access the command-line shell, select the Applications menu, then the Terminal icon: After selecting the Terminal icon, you should see a Terminal window appear. Typically this will give a console prompt showing which login node you are connected to, either sde-login1 or sde-login2 : To exit ThinLinc, type exit in any Terminal window, select the top-right icon, then select the \"Log Out\" menu item and follow the instructions. Finally, close the browser window. Connecting with an SSH client Launch PuTTY from the Windows Start Menu: Enter midwayr.rcc.uchicago.edu as the server name and click on the \"Open\" button: If you are connecting to MidwayR for the first time, you will be asked to create a new SSH key. Click the \"Yes\" button, which will save the server host key: Next, enter your CnetID and password. (There should be no additional two-factor authentication at this step.) At this point, if you successfully passed the password authentication step, you should be connected to one of the MidwayR login nodes, either sde-login1 or sde-login2 . Please read the other sections of the MidwayR User Guide to learn more about how to use the MidwayR computing environment. To close your MidwayR connection in PuTTy, type exit . Disconnecting from MidwayR Once you are done using MidwayR, be sure to logout of your session. Like connecting, disconnecting is a two-step process: first, exit from ThinLinc or PuTTY (see above); second, logout from the SDE environment by selecting \"Sign out\" from the Windows Start Menu:","title":"Index"},{"location":"midwayr/docs/connecting/#accessing-midwayr","text":"To use MidwayR resources, you will need to have a MidwayR user account. If you do not have a MidwayR user account, please see the Getting Started section for how to apply for an account. Note Although both Midway and MidwayR use CNetID for authentication, they do not share the accounts. In other words, if you already have an account on Midway, you will still need to apply for an account on MidwayR to be able to use MidwayR resources. Please also note that you must have enabled Two Factor Authentication for your CNetID before connecting to MidwayR. Accessing MidwayR is a two-step process: Login to Secure Data Enclave (SDE) Desktop using the Virtual Desktop Infrastructure (VDI) client. Once you are connected to the SDE Desktop, login to MidwayR using ThinLinc or an SSH client.","title":"Accessing MidwayR"},{"location":"midwayr/docs/connecting/#connecting-to-the-sde-desktop","text":"To connect to the SDE Desktop, you will need to download and install the VDI client on your computer. The VDI client, \"VMware Horizon\", can be downloaded from here . Note that you will need to be connected to the University of Chicago VPN (see below) to access this software. Once you have installed the VDI client on your computer, connect to the SDE Desktop by taking the following steps: Login to the University of Chicago VPN (see here for details). ( Note: you need to use VPN regardless of whether you are connecting from the campus network or not.) Start up the VDI client on your computer. If you are connecting to the SDE Desktop for the first time, add a new server by double-clicking on the \"Add Server\" or \"New Server\" button, which should appear on the screen as a button with a plus sign (+). Enter vdesk-sde.uchicago.edu as the name of the Connection Server, then click the \"Connect\" button (note that your screen may look slightly different than what is shown here): Enter your user name (your CNetID), enter your password, set the domain to \"ADLOCAL\", then clik the \"Login\" button: Double-click on the \"Secure Data Enclave\" icon and you will enter to the SDE environment: At this point, you should be logged in to the SDE environment.","title":"Connecting to the SDE Desktop"},{"location":"midwayr/docs/connecting/#connecting-to-midwayr","text":"Once you are connected to the SDE environment using the VDI client following the steps given above, please follow one of the methods below to connect to MidwayR from the SDE environment.","title":"Connecting to MidwayR"},{"location":"midwayr/docs/connecting/#connecting-with-thinlinc","text":"ThinLinc is a remote desktop server application. It is recommended to use ThinLinc when you run a software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). To use ThinLinc to connect to MidwayR, please take the following steps: Open a browser (Chrome or Firefox) and enter https://sde.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. To access the command-line shell, select the Applications menu, then the Terminal icon: After selecting the Terminal icon, you should see a Terminal window appear. Typically this will give a console prompt showing which login node you are connected to, either sde-login1 or sde-login2 : To exit ThinLinc, type exit in any Terminal window, select the top-right icon, then select the \"Log Out\" menu item and follow the instructions. Finally, close the browser window.","title":"Connecting with ThinLinc"},{"location":"midwayr/docs/connecting/#connecting-with-an-ssh-client","text":"Launch PuTTY from the Windows Start Menu: Enter midwayr.rcc.uchicago.edu as the server name and click on the \"Open\" button: If you are connecting to MidwayR for the first time, you will be asked to create a new SSH key. Click the \"Yes\" button, which will save the server host key: Next, enter your CnetID and password. (There should be no additional two-factor authentication at this step.) At this point, if you successfully passed the password authentication step, you should be connected to one of the MidwayR login nodes, either sde-login1 or sde-login2 . Please read the other sections of the MidwayR User Guide to learn more about how to use the MidwayR computing environment. To close your MidwayR connection in PuTTy, type exit .","title":"Connecting with an SSH client"},{"location":"midwayr/docs/connecting/#disconnecting-from-midwayr","text":"Once you are done using MidwayR, be sure to logout of your session. Like connecting, disconnecting is a two-step process: first, exit from ThinLinc or PuTTY (see above); second, logout from the SDE environment by selecting \"Sign out\" from the Windows Start Menu:","title":"Disconnecting from MidwayR"},{"location":"midwayr/docs/connecting_avd/","text":"Introducing the New Azure Virtual Desktop The new Azure Virtual Desktop (AVD) can be accessed from your local computer's web browser at this address: https://rdweb.wvd.microsoft.com/arm/webclient If you currently have access to MidwayR through Virtual Desktop Interface (VDI) using VMWare, then you will now have access through Azure. Quick Overview: Differences Between AVD and Previous VDI The AVD should feel familiar to you if you have used the previous VDI to connect to MidwayR. There are a few noteworthy differences: You no longer need to be connected to the University VPN before connecting to MidwayR. Azure takes care of encrypting and securing all communications between your local computer and the Virtual Desktop. You can now connect from your web browser, in addition to the Microsoft Remote Desktop application. Your local data storage limit in the SDE desktop is 30GB. Any data stored will be purged when your login session ends. It is no longer possible to copy/paste between your local computer and the Virtual Desktop. You can still copy/paste inside the AVD environment. Connecting to MidwayR Accessing MidwayR remains a two-step process: 1. Login to the Secure Data Enclave (SDE) Desktop using the AVD. 1. Once you are connected to the SDE Desktop, login to MidwayR using ThinLinc or an SSH client. Connecting To The AVD From The Web Browser Navigate to https://rdweb.wvd.microsoft.com/arm/webclient on your computer's web browser. Select \"AVD Host\" to launch the Virtual Desktop: You will be prompted for your username (cnetID@uchicago.edu) and password: After logging in, you will arrive at the Desktop where you can launch applications: Connecting To The AVD From Microsoft Remote Desktop You can also connect from the Microsoft Remote Desktop App, available for download on the Windows or MacOS app store. After launching the app, click on the \"+\" symbol and select \"Add Workspace\": In the dialog box, put the URL \"https://rdweb.wvd.microsoft.com\": You should then be able to launch the AVD from within the App. Connecting To MidwayR Once you are connected to the SDE environment using the AVD client following the steps given above, please follow one of the methods below to connect to MidwayR from the SDE environment. Connecting with ThinLinc ThinLinc is a remote desktop server application. It is recommended to use ThinLinc when you run a software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). To use ThinLinc to connect to MidwayR, please take the following steps: Open a browser (Chrome or Firefox) and enter https://sde.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. To access the command-line shell, select the Applications menu, then the Terminal icon: After selecting the Terminal icon, you should see a Terminal window appear. Typically this will give a console prompt showing which login node you are connected to, either sde-login1 or sde-login2 : To exit ThinLinc, type exit in any Terminal window, select the top-right icon, then select the \"Log Out\" menu item and follow the instructions. Finally, close the browser window. AVD Data Storage and Data Transfer Once connected to the Azure Desktop, you can open a web browser and download data from, e.g., UChicago Box, placing the data in the Desktop or Documents folder. There is a 30GB limit per-user for storing files in the AVD. These files can then be permanently transferred to MidwayR using WinSCP: Once your login session ends with Azure, all data that was downloaded will be purged from the AVD. In special cases where you need to transfer more than 30GB, please contact us at midwayr-help@rcc.uchicago.edu . Logging Out You can log out of the AVD by clicking the \"Log off\" application on the Desktop. Once logged off, any data stored in your AVD user-space will be purged.","title":"Index"},{"location":"midwayr/docs/connecting_avd/#introducing-the-new-azure-virtual-desktop","text":"The new Azure Virtual Desktop (AVD) can be accessed from your local computer's web browser at this address: https://rdweb.wvd.microsoft.com/arm/webclient If you currently have access to MidwayR through Virtual Desktop Interface (VDI) using VMWare, then you will now have access through Azure.","title":"Introducing the New Azure Virtual Desktop"},{"location":"midwayr/docs/connecting_avd/#quick-overview-differences-between-avd-and-previous-vdi","text":"The AVD should feel familiar to you if you have used the previous VDI to connect to MidwayR. There are a few noteworthy differences: You no longer need to be connected to the University VPN before connecting to MidwayR. Azure takes care of encrypting and securing all communications between your local computer and the Virtual Desktop. You can now connect from your web browser, in addition to the Microsoft Remote Desktop application. Your local data storage limit in the SDE desktop is 30GB. Any data stored will be purged when your login session ends. It is no longer possible to copy/paste between your local computer and the Virtual Desktop. You can still copy/paste inside the AVD environment.","title":"Quick Overview: Differences Between AVD and Previous VDI"},{"location":"midwayr/docs/connecting_avd/#connecting-to-midwayr","text":"Accessing MidwayR remains a two-step process: 1. Login to the Secure Data Enclave (SDE) Desktop using the AVD. 1. Once you are connected to the SDE Desktop, login to MidwayR using ThinLinc or an SSH client.","title":"Connecting to MidwayR"},{"location":"midwayr/docs/connecting_avd/#connecting-to-the-avd-from-the-web-browser","text":"Navigate to https://rdweb.wvd.microsoft.com/arm/webclient on your computer's web browser. Select \"AVD Host\" to launch the Virtual Desktop: You will be prompted for your username (cnetID@uchicago.edu) and password: After logging in, you will arrive at the Desktop where you can launch applications:","title":"Connecting To The AVD From The Web Browser"},{"location":"midwayr/docs/connecting_avd/#connecting-to-the-avd-from-microsoft-remote-desktop","text":"You can also connect from the Microsoft Remote Desktop App, available for download on the Windows or MacOS app store. After launching the app, click on the \"+\" symbol and select \"Add Workspace\": In the dialog box, put the URL \"https://rdweb.wvd.microsoft.com\": You should then be able to launch the AVD from within the App.","title":"Connecting To The AVD From Microsoft Remote Desktop"},{"location":"midwayr/docs/connecting_avd/#connecting-to-midwayr_1","text":"Once you are connected to the SDE environment using the AVD client following the steps given above, please follow one of the methods below to connect to MidwayR from the SDE environment.","title":"Connecting To MidwayR"},{"location":"midwayr/docs/connecting_avd/#connecting-with-thinlinc","text":"ThinLinc is a remote desktop server application. It is recommended to use ThinLinc when you run a software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). To use ThinLinc to connect to MidwayR, please take the following steps: Open a browser (Chrome or Firefox) and enter https://sde.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. To access the command-line shell, select the Applications menu, then the Terminal icon: After selecting the Terminal icon, you should see a Terminal window appear. Typically this will give a console prompt showing which login node you are connected to, either sde-login1 or sde-login2 : To exit ThinLinc, type exit in any Terminal window, select the top-right icon, then select the \"Log Out\" menu item and follow the instructions. Finally, close the browser window.","title":"Connecting with ThinLinc"},{"location":"midwayr/docs/connecting_avd/#avd-data-storage-and-data-transfer","text":"Once connected to the Azure Desktop, you can open a web browser and download data from, e.g., UChicago Box, placing the data in the Desktop or Documents folder. There is a 30GB limit per-user for storing files in the AVD. These files can then be permanently transferred to MidwayR using WinSCP: Once your login session ends with Azure, all data that was downloaded will be purged from the AVD. In special cases where you need to transfer more than 30GB, please contact us at midwayr-help@rcc.uchicago.edu .","title":"AVD Data Storage and Data Transfer"},{"location":"midwayr/docs/connecting_avd/#logging-out","text":"You can log out of the AVD by clicking the \"Log off\" application on the Desktop. Once logged off, any data stored in your AVD user-space will be purged.","title":"Logging Out"},{"location":"midwayr/docs/data-transfer/","text":"Transferring Data to the VDI and then WinSCP to MidwayR Method #1 (recommended) : Transferring Using UChicago Box from within a VDI browser Once you have started the MidwayR Desktop (see Connecting to the SDE Desktop ), you can open a browser and log in to a Box account at https://uchicago.account.box.com : Download files into the VDI using the built-in Box Download function : By default it will copy the files into your Downloads folder, e.g. C:\\Users[CNetID]\\Downloads : When you've transferred all your files from Box to the VDI, open the WinSCP application : Connect to midwayr.rcc.uchicago.edu with your CNet ID and password : (Add the host key if this is your first time using WinSCP to move files) : Move the folders or files you wish to MidwayR using the Upload function -- please do remember there is a 30GB quota on the home directories and a 500GB quota on project, so we strongly recommend keeping your data in project: You can use PuTTy to connect to MidwayR to confirm the files are there : Please note that any files remaining in the VDI will be removed; please be sure to move all your files to MidwayR. Method #2 : Transferring Using a Hardware-Encrypted USB Device Hardware-encrypted USB drives (available from IT Services at the @TechBar in the Regenstein Library) are recognized by the VDI client, and data can be transferred directly (uploading and downloading) using one of those devices. Method #3 : Transferring Using Secure Connections from within VDI We also envision the possibility that users with the ability to log in to secure systems via a web browser will be able to transfer data directly from a remote secure system.","title":"Index"},{"location":"midwayr/docs/data-transfer/#transferring-data-to-the-vdi-and-then-winscp-to-midwayr","text":"","title":"Transferring Data to the VDI and then WinSCP to MidwayR"},{"location":"midwayr/docs/data-transfer/#method-1-recommended-transferring-using-uchicago-box-from-within-a-vdi-browser","text":"","title":"Method #1 (recommended) : Transferring Using UChicago Box from within a VDI browser"},{"location":"midwayr/docs/data-transfer/#once-you-have-started-the-midwayr-desktop-see-connecting-to-the-sde-desktop-you-can-open-a-browser-and-log-in-to-a-box-account-at-httpsuchicagoaccountboxcom","text":"","title":"Once you have started the MidwayR Desktop (see Connecting to the SDE Desktop), you can open a browser and log in to a Box account at https://uchicago.account.box.com:"},{"location":"midwayr/docs/data-transfer/#download-files-into-the-vdi-using-the-built-in-box-download-function","text":"","title":"Download files into the VDI using the built-in Box Download function :"},{"location":"midwayr/docs/data-transfer/#by-default-it-will-copy-the-files-into-your-downloads-folder-eg-cuserscnetiddownloads","text":"","title":"By default it will copy the files into your Downloads folder, e.g. C:\\Users[CNetID]\\Downloads :"},{"location":"midwayr/docs/data-transfer/#when-youve-transferred-all-your-files-from-box-to-the-vdi-open-the-winscp-application","text":"","title":"When you've transferred all your files from Box to the VDI, open the WinSCP application : "},{"location":"midwayr/docs/data-transfer/#connect-to-midwayrrccuchicagoedu-with-your-cnet-id-and-password","text":"","title":"Connect to midwayr.rcc.uchicago.edu with your CNet ID and password :"},{"location":"midwayr/docs/data-transfer/#add-the-host-key-if-this-is-your-first-time-using-winscp-to-move-files","text":"","title":"(Add the host key if this is your first time using WinSCP to move files) :"},{"location":"midwayr/docs/data-transfer/#move-the-folders-or-files-you-wish-to-midwayr-using-the-upload-function-please-do-remember-there-is-a-30gb-quota-on-the-home-directories-and-a-500gb-quota-on-project-so-we-strongly-recommend-keeping-your-data-in-project","text":"","title":"Move the folders or files you wish to MidwayR using the Upload function -- please do remember there is a 30GB quota on the home directories and a 500GB quota on project, so we strongly recommend keeping your data in project:"},{"location":"midwayr/docs/data-transfer/#you-can-use-putty-to-connect-to-midwayr-to-confirm-the-files-are-there","text":"","title":"You can use PuTTy to connect to MidwayR to confirm the files are there : "},{"location":"midwayr/docs/data-transfer/#please-note-that-any-files-remaining-in-the-vdi-will-be-removed-please-be-sure-to-move-all-your-files-to-midwayr","text":"","title":"Please note that any files remaining in the VDI will be removed; please be sure to move all your files to MidwayR."},{"location":"midwayr/docs/data-transfer/#method-2-transferring-using-a-hardware-encrypted-usb-device","text":"Hardware-encrypted USB drives (available from IT Services at the @TechBar in the Regenstein Library) are recognized by the VDI client, and data can be transferred directly (uploading and downloading) using one of those devices.","title":"Method #2 : Transferring Using a Hardware-Encrypted USB Device"},{"location":"midwayr/docs/data-transfer/#method-3-transferring-using-secure-connections-from-within-vdi","text":"We also envision the possibility that users with the ability to log in to secure systems via a web browser will be able to transfer data directly from a remote secure system.","title":"Method #3 : Transferring Using Secure Connections from within VDI"},{"location":"midwayr/docs/running-jobs/","text":"Running jobs on midwayR This section of the documentation describes how to run your jobs on the compute nodes of MidwayR. Login nodes vs. compute nodes Upon connecting to midwayR, you will be located on one of the two midwayR login nodes. Login nodes may be used for compiling and debugging code, installing software, editing and managing files, submitting jobs, or any other work that is not long-running or computationally intensive. Login nodes should not be used for computionally intensive work. All intensive computations should be performed on compute nodes. Access to compute nodes can be obtained by submitting a job through the Slurm scheduler, or by requesting an interactive session. If you are unsure whether your computations will be intensive, please request an interactive session and continue your work once you have connected to the compute node. Note Running computationally intensive jobs on the midwayR login nodes prevents other users from efficiently using the cluster. RCC System Administrators may terminate your processes without warning if your processes disrupt other users\u2019 work on the RCC cluster. Requesting Compute Resources for your Job The MidwayR compute resources rely on a job scheduler to manage requests for access to the compute resources. These requests are called jobs. In particular, we use the Slurm resource manager to schedule both interactive access and batch job submission to the compute nodes. Here, we give the minimal information you will need to know to start computing on MidwayR. The compute resources are managed by the Slurm job scheduler. There are two ways of making use of the compute resources. Your jobs can make use of the compute ressources of MidwayR either through an interactive session or by a non-interactive batch job submission. Both of these two ways of utilizing the compute resources are described below. Interactive jobs An interactive job is a job that allows users to interact with requested resources in real time. Users can run their analysis, execute their software, or run other commands directly on a compute node. The command sinteractive is the recommended Slurm command for requesting an interactive session. As soon as the requested resources become available, sinteractive will do the following: Log in to the compute node. Change into the directory you were working in. Set up X11 forwarding for displaying graphics if this is enabled. Transfer your current shell environment, including any modules you have previously loaded. To get started (with the default interactive settings), simply enter sinteractive in the command line: $ sinteractive If you are a MidwayR researcher affiliated with the Booth School of Business, you are entitled to Booth purchased harware resources. To use Booth dedicated MidwayR resources, please specify partition 'booth', as the following, $ sinteractive --partition = booth By default, an interactive session times out after 2 hours. If you would like more than 2 hours, be sure to include a --time=HH:MM:SS flag to specify the necessary amount of time. For example, to request an interactive session for 6 hours, run the following command: $ sinteractive --time = 06 :00:00 There are many additional options for the sinteractive command, including options to select the number of nodes, the number of cores per node, the amount of memory, and so on. For example, to request exclusive use of two compute nodes equipped with the Infiniband interconnect on the Midway1 sandyb partition for 8 hours, enter the following: $ sinteractive --exclusive --nodes = 2 --time = 08 :00:00 For more details about these and other useful options, see below about the sbatch command, and see the illustrative sbatch examples below. Note that all options available in the sbatch command are also available for the sinteractive command. srun An alternative to the sinteractive command is the srun command: $ srun --pty bash Unlike sinteractive , this command does not set up X11 forwarding, which means you cannot display graphics using srun . Both the srun and sinteractive commands have the same command options. Batch Jobs The sbatch command is the command most commonly used by RCC users to request computing resources on the Midway cluster. Rather than specify all the options in the command line, users typically write an \"sbatch script\" that contains all the commands and parameters neccessary to run the program on the cluster. In an sbatch script, all Slurm parameters are declared with #SBATCH , followed by additional definitions. Here is an example of an sbatch script: #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:05:00 #SBATCH --partition=skylake #SBATCH --nodes=2 #SBATCH --ntasks-per-node=40 #SBATCH --mem-per-cpu=2000 module load Anaconda3 python myjob.py Here is an explanation of what each of these options does: Option Description #SBATCH --job-name=example_sbatch Assigns label example_sbatch to the job. #SBATCH --output=example_sbatch.out Writes console output to file example_sbatch.out . #SBATCH --error=example_sbatch.err Writes an error messages to file example_sbatch.err . #SBATCH --time=00:05:00 Reserves the computing resources for 5 minutes (or less if program completes before 5 min). #SBATCH --partition=skylake Requests compute nodes from the skylake partition on the MidwayR cluster. (Noted, If you are a MidwayR researcher affiliated with the Booth School of Business, you have access to Booth partition in addition to skylake . Booth researchers can use #SBATCH --partition=booth in this line for better computational performance.) #SBATCH --nodes=2 Requests 2 compute nodes. #SBATCH --ntasks-per-node=40 Requests 40 cores (CPUs) per node, for a total of 40 * 2 = 80 cores. #SBATCH --mem-per-cpu=2000 Requests 2000 MB (2 GB) of memory (RAM) per core, for a total of 2 * 40 = 80 GB per node. To use Booth dedicated MidwayR resources, please specify partition 'booth', as the following, In this example, we have requested 2 compute nodes with 40 CPU cores each. Therefore, we have requested a total of 80 CPU cores for running our program. The last two lines of the script load the Anaconda3 module and launch the python executable that we have called myjob.py Note If the --partition option is not specified, the request will automatically be directed to the skylake partition on MidwayR. Continuing the example above, suppose that this script is saved in the current directory into a file called example.sbatch . This script is submitted to the cluster using the following command: $ sbatch ./example.sbatch Many other options are available for submitting jobs using the sbatch command. For more specialized computational needs, see :ref: running-jobs . Additionally, for a complete list of the available options, see the Official SBATCH Documentation . Illustrative sbatch job examples Below are a number of example job submission scripts for different types of job use cases. You can adapt the particular example script that meets your particular use case to run your job(s) on MidwayR. Single core/thread jobs -- The simplest example (for startup, 1-core job with all default settings) Multithreaded jobs -- Request multiple threads, (e.g. via OpenMP, etc.) Job arrays -- Submit a group of jobs by one script (job arrays) Parallel software -- Multiple concurrent tasks in a single job (using \"parallel\" command)","title":"Running jobs on midwayR"},{"location":"midwayr/docs/running-jobs/#running-jobs-on-midwayr","text":"This section of the documentation describes how to run your jobs on the compute nodes of MidwayR.","title":"Running jobs on midwayR"},{"location":"midwayr/docs/running-jobs/#login-nodes-vs-compute-nodes","text":"Upon connecting to midwayR, you will be located on one of the two midwayR login nodes. Login nodes may be used for compiling and debugging code, installing software, editing and managing files, submitting jobs, or any other work that is not long-running or computationally intensive. Login nodes should not be used for computionally intensive work. All intensive computations should be performed on compute nodes. Access to compute nodes can be obtained by submitting a job through the Slurm scheduler, or by requesting an interactive session. If you are unsure whether your computations will be intensive, please request an interactive session and continue your work once you have connected to the compute node. Note Running computationally intensive jobs on the midwayR login nodes prevents other users from efficiently using the cluster. RCC System Administrators may terminate your processes without warning if your processes disrupt other users\u2019 work on the RCC cluster.","title":"Login nodes vs. compute nodes"},{"location":"midwayr/docs/running-jobs/#requesting-compute-resources-for-your-job","text":"The MidwayR compute resources rely on a job scheduler to manage requests for access to the compute resources. These requests are called jobs. In particular, we use the Slurm resource manager to schedule both interactive access and batch job submission to the compute nodes. Here, we give the minimal information you will need to know to start computing on MidwayR. The compute resources are managed by the Slurm job scheduler. There are two ways of making use of the compute resources. Your jobs can make use of the compute ressources of MidwayR either through an interactive session or by a non-interactive batch job submission. Both of these two ways of utilizing the compute resources are described below.","title":"Requesting Compute Resources for your Job"},{"location":"midwayr/docs/running-jobs/#interactive-jobs","text":"An interactive job is a job that allows users to interact with requested resources in real time. Users can run their analysis, execute their software, or run other commands directly on a compute node. The command sinteractive is the recommended Slurm command for requesting an interactive session. As soon as the requested resources become available, sinteractive will do the following: Log in to the compute node. Change into the directory you were working in. Set up X11 forwarding for displaying graphics if this is enabled. Transfer your current shell environment, including any modules you have previously loaded. To get started (with the default interactive settings), simply enter sinteractive in the command line: $ sinteractive If you are a MidwayR researcher affiliated with the Booth School of Business, you are entitled to Booth purchased harware resources. To use Booth dedicated MidwayR resources, please specify partition 'booth', as the following, $ sinteractive --partition = booth By default, an interactive session times out after 2 hours. If you would like more than 2 hours, be sure to include a --time=HH:MM:SS flag to specify the necessary amount of time. For example, to request an interactive session for 6 hours, run the following command: $ sinteractive --time = 06 :00:00 There are many additional options for the sinteractive command, including options to select the number of nodes, the number of cores per node, the amount of memory, and so on. For example, to request exclusive use of two compute nodes equipped with the Infiniband interconnect on the Midway1 sandyb partition for 8 hours, enter the following: $ sinteractive --exclusive --nodes = 2 --time = 08 :00:00 For more details about these and other useful options, see below about the sbatch command, and see the illustrative sbatch examples below. Note that all options available in the sbatch command are also available for the sinteractive command.","title":"Interactive jobs"},{"location":"midwayr/docs/running-jobs/#srun","text":"An alternative to the sinteractive command is the srun command: $ srun --pty bash Unlike sinteractive , this command does not set up X11 forwarding, which means you cannot display graphics using srun . Both the srun and sinteractive commands have the same command options.","title":"srun"},{"location":"midwayr/docs/running-jobs/#batch-jobs","text":"The sbatch command is the command most commonly used by RCC users to request computing resources on the Midway cluster. Rather than specify all the options in the command line, users typically write an \"sbatch script\" that contains all the commands and parameters neccessary to run the program on the cluster. In an sbatch script, all Slurm parameters are declared with #SBATCH , followed by additional definitions. Here is an example of an sbatch script: #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:05:00 #SBATCH --partition=skylake #SBATCH --nodes=2 #SBATCH --ntasks-per-node=40 #SBATCH --mem-per-cpu=2000 module load Anaconda3 python myjob.py Here is an explanation of what each of these options does: Option Description #SBATCH --job-name=example_sbatch Assigns label example_sbatch to the job. #SBATCH --output=example_sbatch.out Writes console output to file example_sbatch.out . #SBATCH --error=example_sbatch.err Writes an error messages to file example_sbatch.err . #SBATCH --time=00:05:00 Reserves the computing resources for 5 minutes (or less if program completes before 5 min). #SBATCH --partition=skylake Requests compute nodes from the skylake partition on the MidwayR cluster. (Noted, If you are a MidwayR researcher affiliated with the Booth School of Business, you have access to Booth partition in addition to skylake . Booth researchers can use #SBATCH --partition=booth in this line for better computational performance.) #SBATCH --nodes=2 Requests 2 compute nodes. #SBATCH --ntasks-per-node=40 Requests 40 cores (CPUs) per node, for a total of 40 * 2 = 80 cores. #SBATCH --mem-per-cpu=2000 Requests 2000 MB (2 GB) of memory (RAM) per core, for a total of 2 * 40 = 80 GB per node. To use Booth dedicated MidwayR resources, please specify partition 'booth', as the following, In this example, we have requested 2 compute nodes with 40 CPU cores each. Therefore, we have requested a total of 80 CPU cores for running our program. The last two lines of the script load the Anaconda3 module and launch the python executable that we have called myjob.py Note If the --partition option is not specified, the request will automatically be directed to the skylake partition on MidwayR. Continuing the example above, suppose that this script is saved in the current directory into a file called example.sbatch . This script is submitted to the cluster using the following command: $ sbatch ./example.sbatch Many other options are available for submitting jobs using the sbatch command. For more specialized computational needs, see :ref: running-jobs . Additionally, for a complete list of the available options, see the Official SBATCH Documentation .","title":"Batch Jobs"},{"location":"midwayr/docs/running-jobs/#illustrative-sbatch-job-examples","text":"Below are a number of example job submission scripts for different types of job use cases. You can adapt the particular example script that meets your particular use case to run your job(s) on MidwayR. Single core/thread jobs -- The simplest example (for startup, 1-core job with all default settings) Multithreaded jobs -- Request multiple threads, (e.g. via OpenMP, etc.) Job arrays -- Submit a group of jobs by one script (job arrays) Parallel software -- Multiple concurrent tasks in a single job (using \"parallel\" command)","title":"Illustrative sbatch job examples"},{"location":"midwayr/docs/running-jobs/cron/","text":"","title":"Cron"},{"location":"midwayr/docs/running-jobs/job_arrays/","text":"Job Arrays According to the Slurm Job Array Documentation , \u201cjob arrays offer a mechanism for submitting and managing collections of similar jobs quickly and easily.\u201d In general, job arrays are useful for applying the same processing routine to a collection of multiple input data files. Job arrays offer a very simple way to submit a large number of independent processing jobs. By submitting a single job array sbatch script, a specified number of \u201carray-tasks\u201d will be created based on this \u201cmaster\u201d sbatch script. An example job array script is given below array.sbatch : #!/bin/bash #SBATCH --job-name=arrayJob #SBATCH --output=arrayJob_%A_%a.out #SBATCH --error=arrayJob_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=01:00:00 #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000 ###################### # Begin work section # ###################### # Print this sub-job's task ID echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Do some work based on the SLURM_ARRAY_TASK_ID # For example: # ./my_process $SLURM_ARRAY_TASK_ID # # where my_process is you executable In the above example, The --array=1-16 option will cause 16 array-tasks (numbered 1, 2, ..., 16) to be spawned when this master job script is submitted. The \u201carray-tasks\u201d are simply copies of this master script that are automatically submitted to the scheduler on your behalf. However, in each array-tasks an environment variable called SLURM_ARRAY_TASK_ID will be set to a unique value (in this example, a number in the range 1, 2, ..., 16). In your script, you can use this value to select, for example, a specific data file that each array-tasks will be responsible for processing. Job array indices can be specified in a number of ways. For example: #A job array with index values between 0 and 31: #SBATCH --array=0-31 #A job array with index values of 1, 2, 5, 19, 27: #SBATCH --array=1,2,5,19,27 #A job array with index values between 1 and 7 with a step size of 2 (i.e. 1, 3, 5, 7): #SBATCH --array=1-7:2 The %A_%a construct in the output and error file names is used to generate unique output and error files based on the master job ID ( %A ) and the array-tasks ID ( %a ). In this fashion, each array-tasks will be able to write to its own output and error file. The remaining #SBATCH options are used to configure each array-tasks. All of the standard #SBATCH options are available here. In the array.sbatch example, we are requesting that each array-task be allocated 1 CPU core ( --ntasks=1 ) and 4 GB (4000 MB) of memory ( --mem-per-cpu=4000 ) in the sandyb partition ( --partition=sandyb ), and be allowed to run for up to 1 hour ( --time=01:00:00 ). To be clear, the overall collection of 16 array-tasks will be allowed to take more than 1 hour to complete, but we have specified that each individual array-task will run for no more than 1 hour. The total number of array-tasks that are allowed to run in parallel will be governed by the QoS of the partition to which you are submitting. In most cases, this will limit users to a maximum of 64 concurrently running array-tasks. To achieve a higher throughput of array-tasks, see Parallel Batch Jobs More information about Slurm job arrays can be found in the Slurm Job Array Documentation .","title":"Job Arrays"},{"location":"midwayr/docs/running-jobs/job_arrays/#job-arrays","text":"According to the Slurm Job Array Documentation , \u201cjob arrays offer a mechanism for submitting and managing collections of similar jobs quickly and easily.\u201d In general, job arrays are useful for applying the same processing routine to a collection of multiple input data files. Job arrays offer a very simple way to submit a large number of independent processing jobs. By submitting a single job array sbatch script, a specified number of \u201carray-tasks\u201d will be created based on this \u201cmaster\u201d sbatch script. An example job array script is given below array.sbatch : #!/bin/bash #SBATCH --job-name=arrayJob #SBATCH --output=arrayJob_%A_%a.out #SBATCH --error=arrayJob_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=01:00:00 #SBATCH --ntasks=1 #SBATCH --mem-per-cpu=4000 ###################### # Begin work section # ###################### # Print this sub-job's task ID echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Do some work based on the SLURM_ARRAY_TASK_ID # For example: # ./my_process $SLURM_ARRAY_TASK_ID # # where my_process is you executable In the above example, The --array=1-16 option will cause 16 array-tasks (numbered 1, 2, ..., 16) to be spawned when this master job script is submitted. The \u201carray-tasks\u201d are simply copies of this master script that are automatically submitted to the scheduler on your behalf. However, in each array-tasks an environment variable called SLURM_ARRAY_TASK_ID will be set to a unique value (in this example, a number in the range 1, 2, ..., 16). In your script, you can use this value to select, for example, a specific data file that each array-tasks will be responsible for processing. Job array indices can be specified in a number of ways. For example: #A job array with index values between 0 and 31: #SBATCH --array=0-31 #A job array with index values of 1, 2, 5, 19, 27: #SBATCH --array=1,2,5,19,27 #A job array with index values between 1 and 7 with a step size of 2 (i.e. 1, 3, 5, 7): #SBATCH --array=1-7:2 The %A_%a construct in the output and error file names is used to generate unique output and error files based on the master job ID ( %A ) and the array-tasks ID ( %a ). In this fashion, each array-tasks will be able to write to its own output and error file. The remaining #SBATCH options are used to configure each array-tasks. All of the standard #SBATCH options are available here. In the array.sbatch example, we are requesting that each array-task be allocated 1 CPU core ( --ntasks=1 ) and 4 GB (4000 MB) of memory ( --mem-per-cpu=4000 ) in the sandyb partition ( --partition=sandyb ), and be allowed to run for up to 1 hour ( --time=01:00:00 ). To be clear, the overall collection of 16 array-tasks will be allowed to take more than 1 hour to complete, but we have specified that each individual array-task will run for no more than 1 hour. The total number of array-tasks that are allowed to run in parallel will be governed by the QoS of the partition to which you are submitting. In most cases, this will limit users to a maximum of 64 concurrently running array-tasks. To achieve a higher throughput of array-tasks, see Parallel Batch Jobs More information about Slurm job arrays can be found in the Slurm Job Array Documentation .","title":"Job Arrays"},{"location":"midwayr/docs/running-jobs/multithread/","text":"Multi-thread jobs This section describes how to request resources to run multi-threaded applications. Using tasks Requesting multiple threads for MKL libraries in python using the --ntasks-per-node flag. #!/bin/bash #SBATCH --job-name=test_sbatch #SBATCH --output=test_sbatch.out #SBATCH --error=test_sbatch.err ##SBATCH --time=12:00:00 #SBATCH --nodes=1 #SBATCH --ntasks-per-node=16 # # LOAD REQUIRED MODULES FOR JOB module load Anaconda3 # # SET NUMBER OF THREADS export OMP_NUM_THREADS = $SLURM_NTASKS_PER_NODE # RUN JOB python dotprod.py","title":"Multi-thread jobs"},{"location":"midwayr/docs/running-jobs/multithread/#multi-thread-jobs","text":"This section describes how to request resources to run multi-threaded applications.","title":"Multi-thread jobs"},{"location":"midwayr/docs/running-jobs/multithread/#using-tasks","text":"Requesting multiple threads for MKL libraries in python using the --ntasks-per-node flag. #!/bin/bash #SBATCH --job-name=test_sbatch #SBATCH --output=test_sbatch.out #SBATCH --error=test_sbatch.err ##SBATCH --time=12:00:00 #SBATCH --nodes=1 #SBATCH --ntasks-per-node=16 # # LOAD REQUIRED MODULES FOR JOB module load Anaconda3 # # SET NUMBER OF THREADS export OMP_NUM_THREADS = $SLURM_NTASKS_PER_NODE # RUN JOB python dotprod.py","title":"Using tasks"},{"location":"midwayr/docs/running-jobs/parallel/","text":"Parallel Batch Jobs Batch jobs that consist of large number of serial jobs should most likely be combined in some way to optimize the job submission and reduce the number of submitted jobs to a more manageable number. RCC has developed a method to handle this type of job submissions using GNU Parallel and srun . For this submission method, a single job is submitted with a chosen number of CPU cores allocated (using the Slurm option --ntasks=X ) and the parallel command is used to run that number of tasks simultaneously until all tasks have been completed. An example submit file is parallel.sbatch : #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --ntasks=32 module load parallel # for a large number of tasks, the controlling node will have a large number # of processes, so it will be necessary to change the user process limit ulimit -u 10000 # the --exclusive to srun make srun use distinct CPUs for each job step # -N1 -n1 allocates a single core to each task srun = \"srun --exclusive -N1 -n1\" # --delay .2 prevents overloading the controlling node # -j is the number of tasks parallel runs so we set it to $SLURM_NTASKS # --joblog makes parallel create a log of tasks that it has already run # --resume makes parallel use the joblog to resume from where it has left off # the combination of --joblog and --resume allow jobs to be resubmitted if # necessary and continue from where they left off # NOTE: if you want to run your job again, you will need to delete the runtask.log file parallel = \"parallel --delay .2 -j $SLURM_NTASKS --joblog runtask.log --resume\" # this runs the parallel command we want # in this case, we are running a script named runtask # parallel uses ::: to separate options. Here {1..128} is a shell expansion # so parallel will run the runtask script for the numbers 1 through 128 # {1} is the first argument # as an example, the first job will run like this: # srun --exclusive -N1 -n1 ./runtask arg1:1 > runtask.1 $parallel \" $srun ./runtask arg1:{1} > runtask.{1}\" ::: { 1 ..128 } # if your program does not take any input, use -n0 option to call the parallel # command as follows: # $parallel -n0 \"$srun ./runnoinptask > output.{1}\" ::: {1..128} In this submit file we want to run the runtask script 128 times. The --ntasks option is set to 32, so we are allocating 1/4th the number of CPU cores as tasks that we want to run. parallel will run the first 32 tasks on 32 cores and then as CPU cores become available it will run the rest of tasks. Parallel is very flexible in what can be used as the command line arguments. In this case we are using a simple shell expansion of the numbers 1 through 128, but arguments can be piped into the parallel command, arguments could be file names instead of numbers, replacements can be made, and much more. Reading the Parallel Manual will provide full details on its capabilities. Here is the runtask script that parallel runs. #!/bin/sh # this script echoes some useful outputs so we can see what parallel # and srun are doing sleepsecs = $ [ ( $RANDOM % 10 ) + 10 ] s # $1 is arg1:{1} from parallel. # $PARALLEL_SEQ is a special variable from parallel. It is the actual sequence # number of the job regardless of the arguments given # We print the sleep time, hostname, and date for more info echo task $1 seq: $PARALLEL_SEQ sleep: $sleepsecs host: $( hostname ) date: $( date ) # sleep a random amount of time sleep $sleepsecs To submit this job, download both the parallel.sbatch and runtask scripts and put them in the same directory. Run the chmod +x runtask command to make sure runtask is executatble. Then, the job can be submitted like this: sbatch parallel.sbatch When this job completes, there should be 128 runtask.N output files where N is a number between 1 and 128. The content of the first output file (i.e., runtask.1 ) will look similar to this (numbers and the host name will likely be different): task arg1:1 seq:1 sleep:14s host:midway2-0002 date:Thu Jan 10 09:17:36 CST 2017 A file named runtask.log will also be created that lists the completed jobs. NOTE: If this job is resubmitted, nothing will be run until the runtask.log file is removed. It is also possible to use this technique to run tasks that are either multi-threaded or can otherwise use more than one CPU at a time. Here is an example submit file: #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=28 #SBATCH --exclusive module load parallel # For this mode add -c (--cpus-per-task) to the srun command srun = \"srun --exclusive -N1 -n1 -c $SLURM_CPUS_PER_TASK \" # Instead of $SLURM_NTASKS we want to use $SLURM_NNODES to tell # parallel how many jobs to start parallel = \"parallel --delay .2 -j $SLURM_NNODES --joblog runtask.log --resume\" # run the parallel command again. The runtask command should be able to use the # 28 cpus we requested with -c28 by itself $parallel \" $srun ./runtask arg1:{1} > runtask.{1}\" ::: { 1 ..6 } In this case, the runtask script itself will use 28 CPUs and parallel distributes 6 of these jobs to the 2 requested nodes.","title":"Parallel Batch Jobs"},{"location":"midwayr/docs/running-jobs/parallel/#parallel-batch-jobs","text":"Batch jobs that consist of large number of serial jobs should most likely be combined in some way to optimize the job submission and reduce the number of submitted jobs to a more manageable number. RCC has developed a method to handle this type of job submissions using GNU Parallel and srun . For this submission method, a single job is submitted with a chosen number of CPU cores allocated (using the Slurm option --ntasks=X ) and the parallel command is used to run that number of tasks simultaneously until all tasks have been completed. An example submit file is parallel.sbatch : #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --ntasks=32 module load parallel # for a large number of tasks, the controlling node will have a large number # of processes, so it will be necessary to change the user process limit ulimit -u 10000 # the --exclusive to srun make srun use distinct CPUs for each job step # -N1 -n1 allocates a single core to each task srun = \"srun --exclusive -N1 -n1\" # --delay .2 prevents overloading the controlling node # -j is the number of tasks parallel runs so we set it to $SLURM_NTASKS # --joblog makes parallel create a log of tasks that it has already run # --resume makes parallel use the joblog to resume from where it has left off # the combination of --joblog and --resume allow jobs to be resubmitted if # necessary and continue from where they left off # NOTE: if you want to run your job again, you will need to delete the runtask.log file parallel = \"parallel --delay .2 -j $SLURM_NTASKS --joblog runtask.log --resume\" # this runs the parallel command we want # in this case, we are running a script named runtask # parallel uses ::: to separate options. Here {1..128} is a shell expansion # so parallel will run the runtask script for the numbers 1 through 128 # {1} is the first argument # as an example, the first job will run like this: # srun --exclusive -N1 -n1 ./runtask arg1:1 > runtask.1 $parallel \" $srun ./runtask arg1:{1} > runtask.{1}\" ::: { 1 ..128 } # if your program does not take any input, use -n0 option to call the parallel # command as follows: # $parallel -n0 \"$srun ./runnoinptask > output.{1}\" ::: {1..128} In this submit file we want to run the runtask script 128 times. The --ntasks option is set to 32, so we are allocating 1/4th the number of CPU cores as tasks that we want to run. parallel will run the first 32 tasks on 32 cores and then as CPU cores become available it will run the rest of tasks. Parallel is very flexible in what can be used as the command line arguments. In this case we are using a simple shell expansion of the numbers 1 through 128, but arguments can be piped into the parallel command, arguments could be file names instead of numbers, replacements can be made, and much more. Reading the Parallel Manual will provide full details on its capabilities. Here is the runtask script that parallel runs. #!/bin/sh # this script echoes some useful outputs so we can see what parallel # and srun are doing sleepsecs = $ [ ( $RANDOM % 10 ) + 10 ] s # $1 is arg1:{1} from parallel. # $PARALLEL_SEQ is a special variable from parallel. It is the actual sequence # number of the job regardless of the arguments given # We print the sleep time, hostname, and date for more info echo task $1 seq: $PARALLEL_SEQ sleep: $sleepsecs host: $( hostname ) date: $( date ) # sleep a random amount of time sleep $sleepsecs To submit this job, download both the parallel.sbatch and runtask scripts and put them in the same directory. Run the chmod +x runtask command to make sure runtask is executatble. Then, the job can be submitted like this: sbatch parallel.sbatch When this job completes, there should be 128 runtask.N output files where N is a number between 1 and 128. The content of the first output file (i.e., runtask.1 ) will look similar to this (numbers and the host name will likely be different): task arg1:1 seq:1 sleep:14s host:midway2-0002 date:Thu Jan 10 09:17:36 CST 2017 A file named runtask.log will also be created that lists the completed jobs. NOTE: If this job is resubmitted, nothing will be run until the runtask.log file is removed. It is also possible to use this technique to run tasks that are either multi-threaded or can otherwise use more than one CPU at a time. Here is an example submit file: #!/bin/sh #SBATCH --time=01:00:00 #SBATCH --nodes=2 #SBATCH --ntasks-per-node=1 #SBATCH --cpus-per-task=28 #SBATCH --exclusive module load parallel # For this mode add -c (--cpus-per-task) to the srun command srun = \"srun --exclusive -N1 -n1 -c $SLURM_CPUS_PER_TASK \" # Instead of $SLURM_NTASKS we want to use $SLURM_NNODES to tell # parallel how many jobs to start parallel = \"parallel --delay .2 -j $SLURM_NNODES --joblog runtask.log --resume\" # run the parallel command again. The runtask command should be able to use the # 28 cpus we requested with -c28 by itself $parallel \" $srun ./runtask arg1:{1} > runtask.{1}\" ::: { 1 ..6 } In this case, the runtask script itself will use 28 CPUs and parallel distributes 6 of these jobs to the 2 requested nodes.","title":"Parallel Batch Jobs"},{"location":"midwayr/docs/running-jobs/singlecore/","text":"Single core/thread sbatch job To submit a single core/thread job to the scheduler, you will need to create a sbatch file that requests this resource be scheduled and the workflow to be executed. The following sbatch script provides a minimal example of requesting a single node, single core job, which is the default when no other parameters are specified. #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:30:00 # LOAD THE MODULES REQUIRED FOR YOUR JOB module load Anaconda3 # RUN JOB python myjob.py In this minimal example sbatch script, only the wall time is passed to the scheduler. Since the quantity of nodes and cores/threads as well as memory were not specified, the default values for each are provisioned by the scheduler. Once a core is available and is allocated to the job, the myjob.py python code will then be executed on the allocated single core on one of the compute nodes. Default resources allocated when not specified: Default Memory: 2000MB (2GB) Cores: 1 Walltime: 36 hours","title":"Single core/thread sbatch job"},{"location":"midwayr/docs/running-jobs/singlecore/#single-corethread-sbatch-job","text":"To submit a single core/thread job to the scheduler, you will need to create a sbatch file that requests this resource be scheduled and the workflow to be executed. The following sbatch script provides a minimal example of requesting a single node, single core job, which is the default when no other parameters are specified. #!/bin/bash #SBATCH --job-name=example_sbatch #SBATCH --output=example_sbatch.out #SBATCH --error=example_sbatch.err #SBATCH --time=00:30:00 # LOAD THE MODULES REQUIRED FOR YOUR JOB module load Anaconda3 # RUN JOB python myjob.py In this minimal example sbatch script, only the wall time is passed to the scheduler. Since the quantity of nodes and cores/threads as well as memory were not specified, the default values for each are provisioned by the scheduler. Once a core is available and is allocated to the job, the myjob.py python code will then be executed on the allocated single core on one of the compute nodes. Default resources allocated when not specified: Default Memory: 2000MB (2GB) Cores: 1 Walltime: 36 hours","title":"Single core/thread sbatch job"},{"location":"midwayr/docs/system/","text":"System Overview MidwayR is comprised of two login nodes and four compute nodes. The total installed storage on MidwayR is 441TB. It uses Slurm as its workload manger and the software environment module to manage installed software. Login Nodes: MidwayR hosts two login nodes: sde-login1 and sde-login2 with the following specifications: CPUs: 2x Intel Xeon Gold 6130 2.1GHz Total cores per node: 16 cores Threads per core: 2 Memory: 96GB of RAM Compute Nodes: As of writing this document, MidwayR hosts four compute nodes with the following specifications: CPUs: 2x Intel Xeon Gold 6148 2.4GHz Total cores per node: 40 cores Thread per core: 1 Memory: 96GB of RAM Network: Intel Ethernet Controller 10Gbps Adapter Mellanox EDR Infiniband up to 100Gbps bandwidth and a sub-microsecond latency File Systems: MidwayR mounts two GPFS filesystems that are shared across all nodes: /home and /project . Total storage for /home is 21TB and for /project is 420TB. /home has a strict quota of 30GB. Large data should be placed on /project . MidwayR does not have a scratch filesystem. Using MidwayR: MidwayR nodes run CentOS 7. Its job scheduler is the Slurm . Slurm commands enable you to submit, manage, monitor, and control your jobs. Software: Organized in modules Use module avail , to see what is available To load a particular available package, for example, gcc version 8.2.0, do module load gcc/8.2.0 If you do not specify a version of the package, the default one is loaded To see what environmental variables are modified when gcc/8.2.0 is loaded, do module show gcc/8.2.0 To unload gcc/8.2.0 , do module unload gcc/8.2.0 MidwayR Compute Nodes Summary Table: Item Description Host midwayr.rcc.uchicago.edu Organization RCC Descriptive Name RCC Midway Restricted (MidwayR) Manufacturer Lenovo Platform ThinkSystem SD530 Linux Cluster CPU Type Intel Xeon Gold 6148 Operating System Linux (CentOS 7.6) Total Processor Cores 160 Nodes 4 Total Memory 382GB DDR4-2660 GHz (24GB DIMMs) Peak Performance 3.906 TFlops Total Storage 441TB Interconnect 100 Gbps Mellanox EDR Infiniband Workload Manager Slurm File System GPFS Memory Per CPU 2.4GB CPU Speed 2.4GHz CPU Cores Per Node 40 Memory Per Node 96 GB","title":"System Overview"},{"location":"midwayr/docs/system/#system-overview","text":"MidwayR is comprised of two login nodes and four compute nodes. The total installed storage on MidwayR is 441TB. It uses Slurm as its workload manger and the software environment module to manage installed software. Login Nodes: MidwayR hosts two login nodes: sde-login1 and sde-login2 with the following specifications: CPUs: 2x Intel Xeon Gold 6130 2.1GHz Total cores per node: 16 cores Threads per core: 2 Memory: 96GB of RAM Compute Nodes: As of writing this document, MidwayR hosts four compute nodes with the following specifications: CPUs: 2x Intel Xeon Gold 6148 2.4GHz Total cores per node: 40 cores Thread per core: 1 Memory: 96GB of RAM Network: Intel Ethernet Controller 10Gbps Adapter Mellanox EDR Infiniband up to 100Gbps bandwidth and a sub-microsecond latency File Systems: MidwayR mounts two GPFS filesystems that are shared across all nodes: /home and /project . Total storage for /home is 21TB and for /project is 420TB. /home has a strict quota of 30GB. Large data should be placed on /project . MidwayR does not have a scratch filesystem. Using MidwayR: MidwayR nodes run CentOS 7. Its job scheduler is the Slurm . Slurm commands enable you to submit, manage, monitor, and control your jobs. Software: Organized in modules Use module avail , to see what is available To load a particular available package, for example, gcc version 8.2.0, do module load gcc/8.2.0 If you do not specify a version of the package, the default one is loaded To see what environmental variables are modified when gcc/8.2.0 is loaded, do module show gcc/8.2.0 To unload gcc/8.2.0 , do module unload gcc/8.2.0 MidwayR Compute Nodes Summary Table: Item Description Host midwayr.rcc.uchicago.edu Organization RCC Descriptive Name RCC Midway Restricted (MidwayR) Manufacturer Lenovo Platform ThinkSystem SD530 Linux Cluster CPU Type Intel Xeon Gold 6148 Operating System Linux (CentOS 7.6) Total Processor Cores 160 Nodes 4 Total Memory 382GB DDR4-2660 GHz (24GB DIMMs) Peak Performance 3.906 TFlops Total Storage 441TB Interconnect 100 Gbps Mellanox EDR Infiniband Workload Manager Slurm File System GPFS Memory Per CPU 2.4GB CPU Speed 2.4GHz CPU Cores Per Node 40 Memory Per Node 96 GB","title":"System Overview"},{"location":"midwayssd/","text":"GM4 User Guide The user guide is built witht he following: MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs. Instructions Once you have installed MkDocs, render the webpages and view them as follows: cd midwayr/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"GM4 User Guide"},{"location":"midwayssd/#gm4-user-guide","text":"The user guide is built witht he following: MkDocs . The mkdocs package is available on the python/3.7.0 module on Midway2. Please see the Getting Started section on the MkDocs website on how you can render the files locally. You may also use this page to learn about different features of MkDocs.","title":"GM4 User Guide"},{"location":"midwayssd/#instructions","text":"Once you have installed MkDocs, render the webpages and view them as follows: cd midwayr/userguide mkdocs serve Once the MkDocs website has launched, it should provide you with an IP address you can enter into your favourite browser.","title":"Instructions"},{"location":"midwayssd/midwayssd_overview/","text":"MidwaySSD","title":"MidwaySSD"},{"location":"midwayssd/midwayssd_overview/#midwayssd","text":"","title":"MidwaySSD"},{"location":"midwayssd/docs/","text":"The MidwaySSD Cluster User Guide MidwaySSD is a high performance computing cluster within Midway ecosystem dedicated to computationally intensive Social Science research and educational excellence. This user guide provides an overview of the cluster setup and information on how users can access and utilize this resource. The following pages provide information on commonly referenced topics for using MidwaySSD. For anything not addressed in this User Guide, please direct your questions to help@rcc.uchicago.edu","title":"The MidwaySSD Cluster User Guide"},{"location":"midwayssd/docs/#the-midwayssd-cluster-user-guide","text":"MidwaySSD is a high performance computing cluster within Midway ecosystem dedicated to computationally intensive Social Science research and educational excellence. This user guide provides an overview of the cluster setup and information on how users can access and utilize this resource. The following pages provide information on commonly referenced topics for using MidwaySSD. For anything not addressed in this User Guide, please direct your questions to help@rcc.uchicago.edu","title":"The MidwaySSD Cluster User Guide"},{"location":"midwayssd/docs/cluster/","text":"MidwaySSD Cluster Overview This section of the documentation provides an overview of how the MidwaySSD cluster is organized. Partitions The MidwaySSD cluster consists of 21 Intel Caslake based nodes. The nodes are accessible thourgh the partition ssd . This partition includes all 21 nodes and is accessible to PI groups affiliated with the Division of Social Sciences (SSD). Three login nodes are designated for interactive sessions. The remaining 18 nodes are compute nodes. Slurm Quality of Service (QOS) The fair-share use of the MidwaySSD resources is managed through the Slurm scheduler Quality of Service (QOS) settings. The quality of service defines the type of resources that a job can request and whether the job is a production or debug run. The resource settings for each QOS are defined as follows: QOS Name: MidwaySSD (default if no QOS specified) Per User Settings Max Wall Time QOS Priority Max Running Jobs Max Jobs Submit Max CPUs 1-36:00:00 10000 28 28 320","title":"Index"},{"location":"midwayssd/docs/cluster/#midwayssd-cluster-overview","text":"This section of the documentation provides an overview of how the MidwaySSD cluster is organized.","title":"MidwaySSD Cluster Overview"},{"location":"midwayssd/docs/cluster/#partitions","text":"The MidwaySSD cluster consists of 21 Intel Caslake based nodes. The nodes are accessible thourgh the partition ssd . This partition includes all 21 nodes and is accessible to PI groups affiliated with the Division of Social Sciences (SSD). Three login nodes are designated for interactive sessions. The remaining 18 nodes are compute nodes.","title":"Partitions"},{"location":"midwayssd/docs/cluster/#slurm-quality-of-service-qos","text":"The fair-share use of the MidwaySSD resources is managed through the Slurm scheduler Quality of Service (QOS) settings. The quality of service defines the type of resources that a job can request and whether the job is a production or debug run. The resource settings for each QOS are defined as follows:","title":"Slurm Quality of Service (QOS)"},{"location":"midwayssd/docs/connecting/","text":"Accessing MidwaySSD To use MidwaySSD resources, you will need to have a Midway user account. If you do not have a Midway user account, please see the Getting Started section on how to apply. Note that you will either register for a PI or general user account. Please note that you must have enabled Two Factor Authentication for your CNetID before connecting to MidwaySSD. External collaborators will require a CNetID and should apply as a general user under their designated PI. Connecting to MidwaySSD There are two alternative ways to connect to MidwaySSD. You may use a graphical user interface known as ThinLinc. This is accessed via your web browser and will give you a familar desktop-like environment for accessing your data and software. The second option uses an SSH connection in your terminal for command line interaction with MidwaySSD. Connecting with ThinLinc ThinLinc is a remote desktop server application. We recommend using ThinLinc when you run software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). For your convenience, the ThinLinc interface has been modified to give you a comfortable, familar experience comparable to your local machine, while you interact with the cluster. To use ThinLinc to connect to MidwaySSD, please take the following steps: First connect to the UChicago VPN. You may use the Cisco AnyConnect client. Connect to cvpn.uchicago.edu . You will need to follow the two factor authentication prompts. Once connected to the VPN, open a browser (Chrome or Firefox) and enter https://ssd.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. This is the ThinLinc GUI, indicating that you are now working on MidwaySSD: In order to access the software applications or a terminal window via ThinLinc, please select the Activities menu in the upper left corner or hover over and click on the icon of nine dots in a square along the left side as indicated below: The resulting view from opening the Applications window should look like the picture below. From here, you can click on any of the software applications and run as you would on a local machine. Please refer to the subsequent section on running applications in ThinLinc for more details: To disconnect and exit ThinLinc, close your selected application and click on the power button symbol in the upper right corner of the desktop as indicated below. Navigate to your name and select log out. Please note that closing the window tab will not end your interactive session on the login node. It is essential to disconnect when you have completed your work on MidwaySSD prior to closing the browser window. When successful, you should see the following confirmation of your disconnected session. Connecting with an SSH client If you are more comfortable accessing MidwaySSD via SSH in your terminal, you can explore your file structure, submit jobs and move files without the ThinLinc graphical user interface. In this case, please open your terminal window on your local machine. Enter ssh cnetid@ssd.rcc.uchicago.edu an hit return: Next, enter your CnetID and password. (There should be an additional two-factor authentication at this step.) At this point, if you successfully passed the password authentication step, you should be connected to one of the MidwaySSD login nodes, either ssd001 , ssd002 or ssd003 . To disconnect and close your MidwaySSD connection in the terminal, type exit and hit return.","title":"Index"},{"location":"midwayssd/docs/connecting/#accessing-midwayssd","text":"To use MidwaySSD resources, you will need to have a Midway user account. If you do not have a Midway user account, please see the Getting Started section on how to apply. Note that you will either register for a PI or general user account. Please note that you must have enabled Two Factor Authentication for your CNetID before connecting to MidwaySSD. External collaborators will require a CNetID and should apply as a general user under their designated PI.","title":"Accessing MidwaySSD"},{"location":"midwayssd/docs/connecting/#connecting-to-midwayssd","text":"There are two alternative ways to connect to MidwaySSD. You may use a graphical user interface known as ThinLinc. This is accessed via your web browser and will give you a familar desktop-like environment for accessing your data and software. The second option uses an SSH connection in your terminal for command line interaction with MidwaySSD.","title":"Connecting to MidwaySSD"},{"location":"midwayssd/docs/connecting/#connecting-with-thinlinc","text":"ThinLinc is a remote desktop server application. We recommend using ThinLinc when you run software that requires a graphical user interface, or \"GUI\" (e.g., Stata, MATLAB). For your convenience, the ThinLinc interface has been modified to give you a comfortable, familar experience comparable to your local machine, while you interact with the cluster. To use ThinLinc to connect to MidwaySSD, please take the following steps: First connect to the UChicago VPN. You may use the Cisco AnyConnect client. Connect to cvpn.uchicago.edu . You will need to follow the two factor authentication prompts. Once connected to the VPN, open a browser (Chrome or Firefox) and enter https://ssd.rcc.uchicago.edu in the address bar. Enter your CNetID and password on the ThinLinc login page: Follow the two factor authentication prompts: If the login process is successful, you will see a Linux desktop environment. This is the ThinLinc GUI, indicating that you are now working on MidwaySSD: In order to access the software applications or a terminal window via ThinLinc, please select the Activities menu in the upper left corner or hover over and click on the icon of nine dots in a square along the left side as indicated below: The resulting view from opening the Applications window should look like the picture below. From here, you can click on any of the software applications and run as you would on a local machine. Please refer to the subsequent section on running applications in ThinLinc for more details: To disconnect and exit ThinLinc, close your selected application and click on the power button symbol in the upper right corner of the desktop as indicated below. Navigate to your name and select log out. Please note that closing the window tab will not end your interactive session on the login node. It is essential to disconnect when you have completed your work on MidwaySSD prior to closing the browser window. When successful, you should see the following confirmation of your disconnected session.","title":"Connecting with ThinLinc"},{"location":"midwayssd/docs/connecting/#connecting-with-an-ssh-client","text":"If you are more comfortable accessing MidwaySSD via SSH in your terminal, you can explore your file structure, submit jobs and move files without the ThinLinc graphical user interface. In this case, please open your terminal window on your local machine. Enter ssh cnetid@ssd.rcc.uchicago.edu an hit return: Next, enter your CnetID and password. (There should be an additional two-factor authentication at this step.) At this point, if you successfully passed the password authentication step, you should be connected to one of the MidwaySSD login nodes, either ssd001 , ssd002 or ssd003 . To disconnect and close your MidwaySSD connection in the terminal, type exit and hit return.","title":"Connecting with an SSH client"},{"location":"midwayssd/docs/datatransfer/","text":"MidwaySSD Data Transfer This section of the documentation provides an overview of data transfer mechanisms for MidwaySSD. Please remember that MidwaySSD is a partition on the Midway3 cluster. The SSD has purchased storage for SSD faculty and student researchers; however, your storage is allocated on the Midway3 cluster. Therefore, the data transfer process described below is identical to the description for the Midway3 Intel and AMD partitions. Your storage is mounted to the greater cluster, allowing you to specify which partition you would like to use for your compute needs. RCC provides a number of methods for accessing and transferring data in/out of our systems. We recommend the SCP protocol for transferring files to/from RCC systems. RCC hosts a managed Globus Online endpoint that can be used for moving very large amounts of data. SSH (SCP or SFTP) Secure copy or SCP is a means of securely transferring computer files between a local host and a remote host. It is based on the Secure Shell (SSH) and Secure File Transfer (SFTP) protocols. To transfer files or directories from your local computer to your home directory on Midway3, open a terminal window and issue the command: $ scp <some file> <CNetID>@midway3.rcc.uchicago.edu: Or for directories: $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu: Or to connect to a directory on Midway3 (/project, for example): $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu:/project SAMBA SAMBA allows uses to connect to (or \u201cmount\u201d) their home and project directories on their local computer so that the file system on Midway3 appears as if it were directly connected to the local machine. This method of accessing your RCC home and project space is only available from within the UChicago campus network. From off-campus you will need to first connect through the UChicago VPN. Your SAMBA account credentials are your CNetID and password: Username: ADLOCAL\\CNetID Password: CNet password Hostname: midway3smb.rcc.uchicago.edu Note: Make sure to prefix your username with ADLOCAL\\ Connecting from Windows On a Windows computer, use the \u201cMap Network Drive\u201d option: * Enter one of the following UNC paths depending on which location on Midway you wish to connect to: home: \\\\midway3smb.rcc.uchicago.edu\\homes project: \\\\midway3smb.rcc.uchicago.edu\\project scratch: \\\\midway3smb.rcc.uchicago.edu\\midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password. Connecting from Mac OS X To connect on a Mac OS X computer follow these steps: Open the Connect to Server utility in Finder Enter one of the following URLs in the input box for Server Address depending on which location on Midway you wish to connect to: home: smb://midway3smb.rcc.uchicago.edu/homes project: smb://midway3smb.rcc.uchicago.edu/project scratch/midway: smb://midway3smb.rcc.uchicago.edu/midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password. Connecting from Ubuntu From Ubuntu, follow the steps here: https://wiki.ubuntu.com/MountWindowsSharesPermanently. Specifically, Install cifs sudo apt-get install cifs-utils . Create a folder to contain the mounted filesystem, e.g. mkdir /media/midway Put your RCC credentials in a text file. Create a file .smbcredentials in your home directory containing: username=[midway username] password=[midway password] Change permissions so only you can read the file chmod 600 ~/.smbcredentials . Edit the file /etc/fstab as root and add the following on one line at the end (Note: You can change \u201cproject\u201d in the below line to \u201chomes\u201d, \u201cproject2\u201d, or \u201cmidway3-scratch\u201d depending on which location you wish to mount): //midway3smb.rcc.uchicago.edu/project2 /media/midway cifs credentials=/home/[username]/.smbcredentials,domain=ADLOCAL,iocharset=utf8,sec=ntlm,vers=2.0 0 0 Remount everything in /etc/fstab with the command sudo mount -a . Your Midway folders should now be accessible at /media/midway . HTTP (web access) RCC provides web access to data on their storage system via public_html directories in users\u2019 home directories. Be sure your home directories and public_html have the execute bit set, and that public_html has read permissions if you would like to allow indexing (directory listings and automatic selection of index.html). For example, these are the commands for setting up Web access to your home directory, where $HOME is the environment variable specifying the location of your home directory: chmod o+x $HOME mkdir -p $HOME/public_html chmod o+x $HOME/public_html chmod o+r $HOME/public_html The last line is optional and only needed if you would like to allow directory listing. Files in public_html must also be readable by the web user (other), but should not be made executable, e.g., chmod o+r $HOME/public_html/research.dat Note: Use of these directories must conform with the RCC usage policy ( https://rcc.uchicago.edu/about-rcc/rcc-user-policy ). Please notify RCC if you expect a large number of people to access data hosted here. Globus Online Globus Online is a robust tool for transferring large data files to and from Midway3. The RCC has a customized Globus Online login site at https://globus.rcc.uchicago.edu and uses the Single Sign On capabilities of CILogon. Once you have signed up, here is the connection information for Midway3: URL: https://globus.rcc.uchicago.edu End Point: ucrcc#midway3 For full instructions, please see Globus Online Data Transfer .","title":"MidwaySSD Data Transfer"},{"location":"midwayssd/docs/datatransfer/#midwayssd-data-transfer","text":"This section of the documentation provides an overview of data transfer mechanisms for MidwaySSD. Please remember that MidwaySSD is a partition on the Midway3 cluster. The SSD has purchased storage for SSD faculty and student researchers; however, your storage is allocated on the Midway3 cluster. Therefore, the data transfer process described below is identical to the description for the Midway3 Intel and AMD partitions. Your storage is mounted to the greater cluster, allowing you to specify which partition you would like to use for your compute needs. RCC provides a number of methods for accessing and transferring data in/out of our systems. We recommend the SCP protocol for transferring files to/from RCC systems. RCC hosts a managed Globus Online endpoint that can be used for moving very large amounts of data.","title":"MidwaySSD Data Transfer"},{"location":"midwayssd/docs/datatransfer/#ssh-scp-or-sftp","text":"Secure copy or SCP is a means of securely transferring computer files between a local host and a remote host. It is based on the Secure Shell (SSH) and Secure File Transfer (SFTP) protocols. To transfer files or directories from your local computer to your home directory on Midway3, open a terminal window and issue the command: $ scp <some file> <CNetID>@midway3.rcc.uchicago.edu: Or for directories: $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu: Or to connect to a directory on Midway3 (/project, for example): $ scp -r <some dir> <CNetID>@midway3.rcc.uchicago.edu:/project","title":"SSH (SCP or SFTP)"},{"location":"midwayssd/docs/datatransfer/#samba","text":"SAMBA allows uses to connect to (or \u201cmount\u201d) their home and project directories on their local computer so that the file system on Midway3 appears as if it were directly connected to the local machine. This method of accessing your RCC home and project space is only available from within the UChicago campus network. From off-campus you will need to first connect through the UChicago VPN. Your SAMBA account credentials are your CNetID and password: Username: ADLOCAL\\CNetID Password: CNet password Hostname: midway3smb.rcc.uchicago.edu Note: Make sure to prefix your username with ADLOCAL\\","title":"SAMBA"},{"location":"midwayssd/docs/datatransfer/#connecting-from-windows","text":"On a Windows computer, use the \u201cMap Network Drive\u201d option: * Enter one of the following UNC paths depending on which location on Midway you wish to connect to: home: \\\\midway3smb.rcc.uchicago.edu\\homes project: \\\\midway3smb.rcc.uchicago.edu\\project scratch: \\\\midway3smb.rcc.uchicago.edu\\midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password.","title":"Connecting from Windows"},{"location":"midwayssd/docs/datatransfer/#connecting-from-mac-os-x","text":"To connect on a Mac OS X computer follow these steps: Open the Connect to Server utility in Finder Enter one of the following URLs in the input box for Server Address depending on which location on Midway you wish to connect to: home: smb://midway3smb.rcc.uchicago.edu/homes project: smb://midway3smb.rcc.uchicago.edu/project scratch/midway: smb://midway3smb.rcc.uchicago.edu/midway3-scratch * When prompted for a username and password, select Registered User . * Enter ADLOCAL\\CNetID for the username and enter your CNet password.","title":"Connecting from Mac OS X"},{"location":"midwayssd/docs/datatransfer/#connecting-from-ubuntu","text":"From Ubuntu, follow the steps here: https://wiki.ubuntu.com/MountWindowsSharesPermanently. Specifically, Install cifs sudo apt-get install cifs-utils . Create a folder to contain the mounted filesystem, e.g. mkdir /media/midway Put your RCC credentials in a text file. Create a file .smbcredentials in your home directory containing: username=[midway username] password=[midway password] Change permissions so only you can read the file chmod 600 ~/.smbcredentials . Edit the file /etc/fstab as root and add the following on one line at the end (Note: You can change \u201cproject\u201d in the below line to \u201chomes\u201d, \u201cproject2\u201d, or \u201cmidway3-scratch\u201d depending on which location you wish to mount): //midway3smb.rcc.uchicago.edu/project2 /media/midway cifs credentials=/home/[username]/.smbcredentials,domain=ADLOCAL,iocharset=utf8,sec=ntlm,vers=2.0 0 0 Remount everything in /etc/fstab with the command sudo mount -a . Your Midway folders should now be accessible at /media/midway .","title":"Connecting from Ubuntu"},{"location":"midwayssd/docs/datatransfer/#http-web-access","text":"RCC provides web access to data on their storage system via public_html directories in users\u2019 home directories. Be sure your home directories and public_html have the execute bit set, and that public_html has read permissions if you would like to allow indexing (directory listings and automatic selection of index.html). For example, these are the commands for setting up Web access to your home directory, where $HOME is the environment variable specifying the location of your home directory: chmod o+x $HOME mkdir -p $HOME/public_html chmod o+x $HOME/public_html chmod o+r $HOME/public_html The last line is optional and only needed if you would like to allow directory listing. Files in public_html must also be readable by the web user (other), but should not be made executable, e.g., chmod o+r $HOME/public_html/research.dat Note: Use of these directories must conform with the RCC usage policy ( https://rcc.uchicago.edu/about-rcc/rcc-user-policy ). Please notify RCC if you expect a large number of people to access data hosted here.","title":"HTTP (web access)"},{"location":"midwayssd/docs/datatransfer/#globus-online","text":"Globus Online is a robust tool for transferring large data files to and from Midway3. The RCC has a customized Globus Online login site at https://globus.rcc.uchicago.edu and uses the Single Sign On capabilities of CILogon. Once you have signed up, here is the connection information for Midway3: URL: https://globus.rcc.uchicago.edu End Point: ucrcc#midway3 For full instructions, please see Globus Online Data Transfer .","title":"Globus Online"},{"location":"midwayssd/docs/interactive-jobs/","text":"Running Jobs Interactively on MidwaySSD This section of the documentation describes how to use the MidwaySSD cluster to run an interactive session with either ThinLinc or an ssh connection. Remember that you must connect to MidwaySSD first and can refer to the previous section in this User Guide for more information. An interactive session is run directly on one of the login nodes. This can be helpful when you are debugging code, unit testing to determine your resource allocation needs and completing tasks such as module installation or web scraping that require internet connectivity. The alternative to an interactive session is submission of a batch job to one or more of the compute nodes. This is done using a short batch script and will be explained in the next section of the User Guide. An Interactive Session in ThinLinc You open an interactive session as soon as you have logged into MidwaySSD via ThinLinc. You can click on the software that you require and run directly on that login node. If you close the ThinLinc tab or your browser window, your interactive session on MidwaySSD will continue to run while you use your local machine. You can access your results, or return to your session by logging in again with your CNetID. Computations run on the login node are not counted in terms of your service hour usage. It should be noted that there are only three login nodes. Jobs that computationally intensive or long running should be completed on a compute node and may be killed by the system monitor. You must remember to disconnect from ThinLinc and end your interactive session when your computation is complete. Failing to do so can result in future login failures. Example: Launching Stata Interactively in ThinLinc Once you have logged into MidwaySSD and are connected via ThinLinc, you can select the Stata icon from the desktop. Your screen should look like the image below. You are ready to begin your interactive session in Stata. When you finish your calculations, please exit as you usually would, by selecting exit under the file menu and then closing the window. You will still need to disconnect from the ThinLinc session.","title":"Running Jobs Interactively on MidwaySSD"},{"location":"midwayssd/docs/interactive-jobs/#running-jobs-interactively-on-midwayssd","text":"This section of the documentation describes how to use the MidwaySSD cluster to run an interactive session with either ThinLinc or an ssh connection. Remember that you must connect to MidwaySSD first and can refer to the previous section in this User Guide for more information. An interactive session is run directly on one of the login nodes. This can be helpful when you are debugging code, unit testing to determine your resource allocation needs and completing tasks such as module installation or web scraping that require internet connectivity. The alternative to an interactive session is submission of a batch job to one or more of the compute nodes. This is done using a short batch script and will be explained in the next section of the User Guide.","title":"Running Jobs Interactively on MidwaySSD"},{"location":"midwayssd/docs/interactive-jobs/#an-interactive-session-in-thinlinc","text":"You open an interactive session as soon as you have logged into MidwaySSD via ThinLinc. You can click on the software that you require and run directly on that login node. If you close the ThinLinc tab or your browser window, your interactive session on MidwaySSD will continue to run while you use your local machine. You can access your results, or return to your session by logging in again with your CNetID. Computations run on the login node are not counted in terms of your service hour usage. It should be noted that there are only three login nodes. Jobs that computationally intensive or long running should be completed on a compute node and may be killed by the system monitor. You must remember to disconnect from ThinLinc and end your interactive session when your computation is complete. Failing to do so can result in future login failures.","title":"An Interactive Session in ThinLinc"},{"location":"midwayssd/docs/interactive-jobs/#example-launching-stata-interactively-in-thinlinc","text":"Once you have logged into MidwaySSD and are connected via ThinLinc, you can select the Stata icon from the desktop. Your screen should look like the image below. You are ready to begin your interactive session in Stata. When you finish your calculations, please exit as you usually would, by selecting exit under the file menu and then closing the window. You will still need to disconnect from the ThinLinc session.","title":"Example: Launching Stata Interactively in ThinLinc"},{"location":"midwayssd/docs/storage/","text":"Storage Allocations for MidwaySSD A Principal Investigator (PI) account for PI eligible faculty members and staff will be assigned a standard storage allocation of 500GB. This allocation can be increased to 2GB upon request at no added cost to the faculty member. Additional storage can be purchased by contacting help@rcc.uchicago.edu . General user accounts are assigned to PI groups and will have access to share storage with the PI and other users within the PI group. Folder read and write access must be set by the PI. Student accounts for select degree programs will be alloted minimal storage for the entire period of active degree enrollment.","title":"Index"},{"location":"midwayssd/docs/storage/#storage-allocations-for-midwayssd","text":"A Principal Investigator (PI) account for PI eligible faculty members and staff will be assigned a standard storage allocation of 500GB. This allocation can be increased to 2GB upon request at no added cost to the faculty member. Additional storage can be purchased by contacting help@rcc.uchicago.edu . General user accounts are assigned to PI groups and will have access to share storage with the PI and other users within the PI group. Folder read and write access must be set by the PI. Student accounts for select degree programs will be alloted minimal storage for the entire period of active degree enrollment.","title":"Storage Allocations for MidwaySSD"},{"location":"midwayssd/docs/submit-jobs/","text":"Submitting Jobs on MidwaySSD using the Slurm Batch Scheduler A batch job is a compute task that you would like to schedule to run on MidwaySSD without your active participation in the session. Often these are tasks designed to utilize multiple cores in a parallel fashion to help enhance the compute efficiency of your job. Running a scheduled job on the compute nodes means that you request resources on MidwaySSD and the Slurm job scheduler allocates your requested resources while balancing the job requests of other authorized users. This ensures the efficient and fair utilization for all MidwaySSD users. Please remember that you will not be able to run jobs that require internet connectivity on the compute nodes. Running Jobs on a Compute Node You may request resources to run your job on a compute node through either ThinLinc or SSH. You will select the resources that you require and be notified upon job completion by an automated systems message to your email. If you choose to submit a job via ThinLinc, you will do this by selecting the Slurm Icon and following the prompts in the dialog box to choose the resources that you require and account under which you are currently working. If you prefer to submit a batch script via SSH, this is done from the terminal prompt on your local machine. Please connect first via ssh and follow the recommendations for writing this script as indicated below. Requesting Resources for Your Job If no wall time is specified in the resource request it will default to the 36 hour max wall time limit. An example sbatch resource request script for a 2 node, 80 core, mpi parallel job that uses the MidwaySSD partition is shown below. #!/bin/bash #SBATCH --time=1-12:00:00 #SBATCH --partition=ssd #SBATCH --account=ssd # You must specify the account as ssd #SBATCH --nodes=2 # Number of nodes #SBATCH --ntasks-per-node=40 # Number of tasks #SBATCH --cpus-per-task=1 # Number of threads per task #SBATCH --qos=midwaySSD # QOS # # SET NUM TASKS NTASKS=$(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) # # SET NUMBER OF THREADS export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK #LOAD MODULES (e.g. intelmpi) module load intelmpi/2018.2.199+intel-18.0 # EXECUTE JOB mpirun -np $NTASKS myjob.x # # EOF","title":"Submitting Jobs on MidwaySSD using the Slurm Batch Scheduler"},{"location":"midwayssd/docs/submit-jobs/#submitting-jobs-on-midwayssd-using-the-slurm-batch-scheduler","text":"A batch job is a compute task that you would like to schedule to run on MidwaySSD without your active participation in the session. Often these are tasks designed to utilize multiple cores in a parallel fashion to help enhance the compute efficiency of your job. Running a scheduled job on the compute nodes means that you request resources on MidwaySSD and the Slurm job scheduler allocates your requested resources while balancing the job requests of other authorized users. This ensures the efficient and fair utilization for all MidwaySSD users. Please remember that you will not be able to run jobs that require internet connectivity on the compute nodes.","title":"Submitting Jobs on MidwaySSD using the Slurm Batch Scheduler"},{"location":"midwayssd/docs/submit-jobs/#running-jobs-on-a-compute-node","text":"You may request resources to run your job on a compute node through either ThinLinc or SSH. You will select the resources that you require and be notified upon job completion by an automated systems message to your email. If you choose to submit a job via ThinLinc, you will do this by selecting the Slurm Icon and following the prompts in the dialog box to choose the resources that you require and account under which you are currently working. If you prefer to submit a batch script via SSH, this is done from the terminal prompt on your local machine. Please connect first via ssh and follow the recommendations for writing this script as indicated below.","title":"Running Jobs on a Compute Node"},{"location":"midwayssd/docs/submit-jobs/#requesting-resources-for-your-job","text":"If no wall time is specified in the resource request it will default to the 36 hour max wall time limit. An example sbatch resource request script for a 2 node, 80 core, mpi parallel job that uses the MidwaySSD partition is shown below. #!/bin/bash #SBATCH --time=1-12:00:00 #SBATCH --partition=ssd #SBATCH --account=ssd # You must specify the account as ssd #SBATCH --nodes=2 # Number of nodes #SBATCH --ntasks-per-node=40 # Number of tasks #SBATCH --cpus-per-task=1 # Number of threads per task #SBATCH --qos=midwaySSD # QOS # # SET NUM TASKS NTASKS=$(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES)) # # SET NUMBER OF THREADS export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK #LOAD MODULES (e.g. intelmpi) module load intelmpi/2018.2.199+intel-18.0 # EXECUTE JOB mpirun -np $NTASKS myjob.x # # EOF","title":"Requesting Resources for Your Job"},{"location":"midwayssd/docs/system/","text":"System Overview MidwaySSD is composed of 21 decicated compute nodes within the Midway high-performance ecosystem. MidwaySSD features Intel Cascade Lake nodes, HDR InfiniBand interconnect, and SSD storage controllers for more performant small file I/O. The total installed storage on MidwaySSD is 400TB, whereby 100TB is reserved for the secure data enclave, MidwayR. The cluster uses Slurm as its workload manger and the software environment module to manage installed software. Intel Hardware Key Features: MidwaySSD hosts three login nodes: midwayssd-login1 , midwayssd-login2 and midwayssd-login3 as well as 18 compute nodes CPUs: 21 nodes total All nodes have HDR InfiniBand (100 Gbps) network cards Each node has 960 GB SSD local disk Operating System throughout cluster is CentOS 8 File Systems: There is a total of 300TB of raw parallel file storage that is part of the MidwaySSD cluster. The new storage system is designed such that any file smaller than 4KB in size is bundled with the file metadata and stored on the pool of dedicated metadata SSDs that are part of the parallel storage system. This can lead to considerable improvement in performance for small file size (<4KB) I/O. This feature was not available with Midway2, where the metadata and data were stored on slower mechanical hard drives. Using MidwaySSD: MidwaySSD nodes run CentOS 8. Its job scheduler is the Slurm . Slurm commands enable you to submit, manage, monitor, and control your jobs. Software: If using ThinLinc, available software is visible as icons on the desktop. Organized in modules Use module avail , to see what is available To load a particular available package, for example, gcc version 8.2.0, do module load gcc/8.2.0 If you do not specify a version of the package, the default one is loaded To see what environmental variables are modified when gcc/8.2.0 is loaded, do module show gcc/8.2.0 To unload gcc/8.2.0 , do module unload gcc/8.2.0 Definition of Terms: When reading the User Guide, please keep in mind the following: A core is the basic computational unit of a multiprocessor CPU. A node is a single machine with memory, cores, operating system and network connection. A partition is a collection of nodes with similar technical specifications (memory, cores, etc.) The cluster is the complete collection of nodes with networking and file storage facilities.","title":"System Overview"},{"location":"midwayssd/docs/system/#system-overview","text":"MidwaySSD is composed of 21 decicated compute nodes within the Midway high-performance ecosystem. MidwaySSD features Intel Cascade Lake nodes, HDR InfiniBand interconnect, and SSD storage controllers for more performant small file I/O. The total installed storage on MidwaySSD is 400TB, whereby 100TB is reserved for the secure data enclave, MidwayR. The cluster uses Slurm as its workload manger and the software environment module to manage installed software. Intel Hardware Key Features: MidwaySSD hosts three login nodes: midwayssd-login1 , midwayssd-login2 and midwayssd-login3 as well as 18 compute nodes CPUs: 21 nodes total All nodes have HDR InfiniBand (100 Gbps) network cards Each node has 960 GB SSD local disk Operating System throughout cluster is CentOS 8 File Systems: There is a total of 300TB of raw parallel file storage that is part of the MidwaySSD cluster. The new storage system is designed such that any file smaller than 4KB in size is bundled with the file metadata and stored on the pool of dedicated metadata SSDs that are part of the parallel storage system. This can lead to considerable improvement in performance for small file size (<4KB) I/O. This feature was not available with Midway2, where the metadata and data were stored on slower mechanical hard drives. Using MidwaySSD: MidwaySSD nodes run CentOS 8. Its job scheduler is the Slurm . Slurm commands enable you to submit, manage, monitor, and control your jobs. Software: If using ThinLinc, available software is visible as icons on the desktop. Organized in modules Use module avail , to see what is available To load a particular available package, for example, gcc version 8.2.0, do module load gcc/8.2.0 If you do not specify a version of the package, the default one is loaded To see what environmental variables are modified when gcc/8.2.0 is loaded, do module show gcc/8.2.0 To unload gcc/8.2.0 , do module unload gcc/8.2.0 Definition of Terms: When reading the User Guide, please keep in mind the following: A core is the basic computational unit of a multiprocessor CPU. A node is a single machine with memory, cores, operating system and network connection. A partition is a collection of nodes with similar technical specifications (memory, cores, etc.) The cluster is the complete collection of nodes with networking and file storage facilities.","title":"System Overview"},{"location":"skyway/skyway_overview/","text":"Skyway","title":"Skyway"},{"location":"skyway/skyway_overview/#skyway","text":"","title":"Skyway"}]}